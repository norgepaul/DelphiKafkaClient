unit Kafka.Lib;
{ This unit is automatically generated by Chet:
  https://github.com/neslib/Chet }

{$MINENUMSIZE 4}

interface

uses
  System.Types;

const
  {$IF Defined(WIN32)}
  Lib = 'librdkafka.dll';
  _PU = '';
  {$ELSEIF Defined(WIN64)}
  Lib = 'librdkafka.dll';
  _PU = '';
  {$ELSEIF Defined(LINUX)}
  Lib = 'librdkafka.so';
  _PU = '';
  {$ELSE}
    {$MESSAGE Error 'Unsupported platform'}
  {$ENDIF}

const
  { TODO : Macro refers to system symbol "__inline": }
  (* RD_INLINE __inline *)
  { TODO : Macro refers to system symbol "__declspec": }
  (* RD_DEPRECATED __declspec ( deprecated ) *)
  { TODO : Macro refers to system symbol "__declspec": }
  (* RD_EXPORT __declspec ( dllimport ) *)
  LIBRDKAFKA_TYPECHECKS = 0;
  { TODO : Unable to convert function-like macro: }
  (* _LRK_TYPECHECK ( RET , TYPE , ARG ) ( RET ) *)
  { TODO : Unable to convert function-like macro: }
  (* _LRK_TYPECHECK2 ( RET , TYPE , ARG , TYPE2 , ARG2 ) ( RET ) *)
  { TODO : Unable to convert function-like macro: }
  (* _LRK_TYPECHECK3 ( RET , TYPE , ARG , TYPE2 , ARG2 , TYPE3 , ARG3 ) ( RET ) *)
  RD_KAFKA_VERSION_ = $010500ff;
  RD_KAFKA_DEBUG_CONTEXTS = 'all,generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos,mock';
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_TOPIC ( topic ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_TOPIC , const char * , topic ) , ( const char * ) topic *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_RKT ( rkt ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_RKT , rd_kafka_topic_t * , rkt ) , ( rd_kafka_topic_t * ) rkt *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_PARTITION ( partition ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_PARTITION , int32_t , partition ) , ( int32_t ) partition *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_VALUE ( VALUE , LEN ) _LRK_TYPECHECK2 ( RD_KAFKA_VTYPE_VALUE , void * , VALUE , size_t , LEN ) , ( void * ) VALUE , ( size_t ) LEN *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_KEY ( KEY , LEN ) _LRK_TYPECHECK2 ( RD_KAFKA_VTYPE_KEY , const void * , KEY , size_t , LEN ) , ( void * ) KEY , ( size_t ) LEN *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_OPAQUE ( msg_opaque ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_OPAQUE , void * , msg_opaque ) , ( void * ) msg_opaque *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_MSGFLAGS ( msgflags ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_MSGFLAGS , int , msgflags ) , ( int ) msgflags *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_TIMESTAMP ( timestamp ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_TIMESTAMP , int64_t , timestamp ) , ( int64_t ) timestamp *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_HEADER ( NAME , VALUE , LEN ) _LRK_TYPECHECK3 ( RD_KAFKA_VTYPE_HEADER , const char * , NAME , const void * , VALUE , ssize_t , LEN ) , ( const char * ) NAME , ( const void * ) VALUE , ( ssize_t ) LEN *)
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_V_HEADERS ( HDRS ) _LRK_TYPECHECK ( RD_KAFKA_VTYPE_HEADERS , rd_kafka_headers_t * , HDRS ) , ( rd_kafka_headers_t * ) HDRS *)
  RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE = $8;
  { TODO : Macro probably use invalid symbol "int32_t": }
  RD_KAFKA_PARTITION_UA = Integer(-1);
  RD_KAFKA_OFFSET_BEGINNING = -2;
  RD_KAFKA_OFFSET_END = -1;
  RD_KAFKA_OFFSET_STORED = -1000;
  RD_KAFKA_OFFSET_INVALID = -1001;
  RD_KAFKA_OFFSET_TAIL_BASE = -2000;
  { TODO : Unable to convert function-like macro: }
  (* RD_KAFKA_OFFSET_TAIL ( CNT ) ( RD_KAFKA_OFFSET_TAIL_BASE - ( CNT ) ) *)
  RD_KAFKA_MSG_F_FREE = $1;
  RD_KAFKA_MSG_F_COPY = $2;
  RD_KAFKA_MSG_F_BLOCK = $4;
  RD_KAFKA_MSG_F_PARTITION = $8;
  RD_KAFKA_PURGE_F_QUEUE = $1;
  RD_KAFKA_PURGE_F_INFLIGHT = $2;
  RD_KAFKA_PURGE_F_NON_BLOCKING = $4;
  RD_KAFKA_EVENT_NONE = $0;
  RD_KAFKA_EVENT_DR = $1;
  RD_KAFKA_EVENT_FETCH = $2;
  RD_KAFKA_EVENT_LOG = $4;
  RD_KAFKA_EVENT_ERROR_ = $8;
  RD_KAFKA_EVENT_REBALANCE = $10;
  RD_KAFKA_EVENT_OFFSET_COMMIT = $20;
  RD_KAFKA_EVENT_STATS = $40;
  RD_KAFKA_EVENT_CREATETOPICS_RESULT = 100;
  RD_KAFKA_EVENT_DELETETOPICS_RESULT = 101;
  RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT = 102;
  RD_KAFKA_EVENT_ALTERCONFIGS_RESULT = 103;
  RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT = 104;
  RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH = $100;

type
  // Forward declarations
  PPUTF8Char = ^PUTF8Char;
  PNativeUInt = ^NativeUInt;
  PInt64 = ^Int64;
  PPointer = ^Pointer;
  PInt32 = ^Int32;
  Prd_kafka_s = Pointer;
  PPrd_kafka_s = ^Prd_kafka_s;
  Prd_kafka_topic_s = Pointer;
  PPrd_kafka_topic_s = ^Prd_kafka_topic_s;
  Prd_kafka_conf_s = Pointer;
  PPrd_kafka_conf_s = ^Prd_kafka_conf_s;
  Prd_kafka_topic_conf_s = Pointer;
  PPrd_kafka_topic_conf_s = ^Prd_kafka_topic_conf_s;
  Prd_kafka_queue_s = Pointer;
  PPrd_kafka_queue_s = ^Prd_kafka_queue_s;
  Prd_kafka_op_s = Pointer;
  PPrd_kafka_op_s = ^Prd_kafka_op_s;
  Prd_kafka_topic_result_s = Pointer;
  PPrd_kafka_topic_result_s = ^Prd_kafka_topic_result_s;
  Prd_kafka_consumer_group_metadata_s = Pointer;
  PPrd_kafka_consumer_group_metadata_s = ^Prd_kafka_consumer_group_metadata_s;
  Prd_kafka_error_s = Pointer;
  PPrd_kafka_error_s = ^Prd_kafka_error_s;
  Prd_kafka_headers_s = Pointer;
  PPrd_kafka_headers_s = ^Prd_kafka_headers_s;
  Prd_kafka_AdminOptions_s = Pointer;
  PPrd_kafka_AdminOptions_s = ^Prd_kafka_AdminOptions_s;
  Prd_kafka_NewTopic_s = Pointer;
  PPrd_kafka_NewTopic_s = ^Prd_kafka_NewTopic_s;
  Prd_kafka_DeleteTopic_s = Pointer;
  PPrd_kafka_DeleteTopic_s = ^Prd_kafka_DeleteTopic_s;
  Prd_kafka_NewPartitions_s = Pointer;
  PPrd_kafka_NewPartitions_s = ^Prd_kafka_NewPartitions_s;
  Prd_kafka_ConfigEntry_s = Pointer;
  PPrd_kafka_ConfigEntry_s = ^Prd_kafka_ConfigEntry_s;
  Prd_kafka_ConfigResource_s = Pointer;
  PPrd_kafka_ConfigResource_s = ^Prd_kafka_ConfigResource_s;
  Prd_kafka_err_desc = ^rd_kafka_err_desc;
  PPrd_kafka_err_desc = ^Prd_kafka_err_desc;
  Prd_kafka_topic_partition_s = ^rd_kafka_topic_partition_s;
  Prd_kafka_topic_partition_list_s = ^rd_kafka_topic_partition_list_s;

  ssize_t = NativeInt;

  (**
   * @enum rd_kafka_type_t
   *
   * @brief rd_kafka_t handle type.
   *
   * @sa rd_kafka_new()
   *)
  rd_kafka_type_t = (
    (** Producer client *)
    RD_KAFKA_PRODUCER = 0,
    (** Consumer client *)
    RD_KAFKA_CONSUMER = 1);
  Prd_kafka_type_t = ^rd_kafka_type_t;

  (*!
   * Timestamp types
   *
   * @sa rd_kafka_message_timestamp()
   *)
  rd_kafka_timestamp_type_t = (
    (** Timestamp not available *)
    RD_KAFKA_TIMESTAMP_NOT_AVAILABLE = 0,
    (** Message creation time *)
    RD_KAFKA_TIMESTAMP_CREATE_TIME = 1,
    (** Log append time *)
    RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME = 2);
  Prd_kafka_timestamp_type_t = ^rd_kafka_timestamp_type_t;
  Prd_kafka_t = Pointer;
  PPrd_kafka_t = ^Prd_kafka_t;
  Prd_kafka_topic_t = Pointer;
  PPrd_kafka_topic_t = ^Prd_kafka_topic_t;
  Prd_kafka_conf_t = Pointer;
  PPrd_kafka_conf_t = ^Prd_kafka_conf_t;
  Prd_kafka_topic_conf_t = Pointer;
  PPrd_kafka_topic_conf_t = ^Prd_kafka_topic_conf_t;
  Prd_kafka_queue_t = Pointer;
  PPrd_kafka_queue_t = ^Prd_kafka_queue_t;
  Prd_kafka_event_t = Pointer;
  PPrd_kafka_event_t = ^Prd_kafka_event_t;
  Prd_kafka_topic_result_t = Pointer;
  PPrd_kafka_topic_result_t = ^Prd_kafka_topic_result_t;
  Prd_kafka_consumer_group_metadata_t = Pointer;
  PPrd_kafka_consumer_group_metadata_t = ^Prd_kafka_consumer_group_metadata_t;
  Prd_kafka_error_t = Pointer;
  PPrd_kafka_error_t = ^Prd_kafka_error_t;
  Prd_kafka_headers_t = Pointer;
  PPrd_kafka_headers_t = ^Prd_kafka_headers_t;

  (**
   * @enum rd_kafka_resp_err_t
   * @brief Error codes.
   *
   * The negative error codes delimited by two underscores
   * (\c RD_KAFKA_RESP_ERR__..) denotes errors internal to librdkafka and are
   * displayed as \c \"Local: \<error string..\>\", while the error codes
   * delimited by a single underscore (\c RD_KAFKA_RESP_ERR_..) denote broker
   * errors and are displayed as \c \"Broker: \<error string..\>\".
   *
   * @sa Use rd_kafka_err2str() to translate an error code a human readable string
   *)
  rd_kafka_resp_err_t = (
    (** Begin internal error codes *)
    RD_KAFKA_RESP_ERR__BEGIN = -200,
    (** Received message is incorrect *)
    RD_KAFKA_RESP_ERR__BAD_MSG = -199,
    (** Bad/unknown compression *)
    RD_KAFKA_RESP_ERR__BAD_COMPRESSION = -198,
    (** Broker is going away *)
    RD_KAFKA_RESP_ERR__DESTROY = -197,
    (** Generic failure *)
    RD_KAFKA_RESP_ERR__FAIL = -196,
    (** Broker transport failure *)
    RD_KAFKA_RESP_ERR__TRANSPORT = -195,
    (** Critical system resource *)
    RD_KAFKA_RESP_ERR__CRIT_SYS_RESOURCE = -194,
    (** Failed to resolve broker *)
    RD_KAFKA_RESP_ERR__RESOLVE = -193,
    (** Produced message timed out*)
    RD_KAFKA_RESP_ERR__MSG_TIMED_OUT = -192,
    (** Reached the end of the topic+partition queue on
     * the broker. Not really an error.
     * This event is disabled by default,
     * see the `enable.partition.eof` configuration property. *)
    RD_KAFKA_RESP_ERR__PARTITION_EOF = -191,
    (** Permanent: Partition does not exist in cluster. *)
    RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION = -190,
    (** File or filesystem error *)
    RD_KAFKA_RESP_ERR__FS = -189,
    (** Permanent: Topic does not exist in cluster. *)
    RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC = -188,
    (** All broker connections are down. *)
    RD_KAFKA_RESP_ERR__ALL_BROKERS_DOWN = -187,
    (** Invalid argument, or invalid configuration *)
    RD_KAFKA_RESP_ERR__INVALID_ARG = -186,
    (** Operation timed out *)
    RD_KAFKA_RESP_ERR__TIMED_OUT = -185,
    (** Queue is full *)
    RD_KAFKA_RESP_ERR__QUEUE_FULL = -184,
    (** ISR count < required.acks *)
    RD_KAFKA_RESP_ERR__ISR_INSUFF = -183,
    (** Broker node update *)
    RD_KAFKA_RESP_ERR__NODE_UPDATE = -182,
    (** SSL error *)
    RD_KAFKA_RESP_ERR__SSL = -181,
    (** Waiting for coordinator to become available. *)
    RD_KAFKA_RESP_ERR__WAIT_COORD = -180,
    (** Unknown client group *)
    RD_KAFKA_RESP_ERR__UNKNOWN_GROUP = -179,
    (** Operation in progress *)
    RD_KAFKA_RESP_ERR__IN_PROGRESS = -178,
    (** Previous operation in progress, wait for it to finish. *)
    RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS = -177,
    (** This operation would interfere with an existing subscription *)
    RD_KAFKA_RESP_ERR__EXISTING_SUBSCRIPTION = -176,
    (** Assigned partitions (rebalance_cb) *)
    RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS = -175,
    (** Revoked partitions (rebalance_cb) *)
    RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS = -174,
    (** Conflicting use *)
    RD_KAFKA_RESP_ERR__CONFLICT = -173,
    (** Wrong state *)
    RD_KAFKA_RESP_ERR__STATE = -172,
    (** Unknown protocol *)
    RD_KAFKA_RESP_ERR__UNKNOWN_PROTOCOL = -171,
    (** Not implemented *)
    RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED = -170,
    (** Authentication failure*)
    RD_KAFKA_RESP_ERR__AUTHENTICATION = -169,
    (** No stored offset *)
    RD_KAFKA_RESP_ERR__NO_OFFSET = -168,
    (** Outdated *)
    RD_KAFKA_RESP_ERR__OUTDATED = -167,
    (** Timed out in queue *)
    RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE = -166,
    (** Feature not supported by broker *)
    RD_KAFKA_RESP_ERR__UNSUPPORTED_FEATURE = -165,
    (** Awaiting cache update *)
    RD_KAFKA_RESP_ERR__WAIT_CACHE = -164,
    (** Operation interrupted (e.g., due to yield)) *)
    RD_KAFKA_RESP_ERR__INTR = -163,
    (** Key serialization error *)
    RD_KAFKA_RESP_ERR__KEY_SERIALIZATION = -162,
    (** Value serialization error *)
    RD_KAFKA_RESP_ERR__VALUE_SERIALIZATION = -161,
    (** Key deserialization error *)
    RD_KAFKA_RESP_ERR__KEY_DESERIALIZATION = -160,
    (** Value deserialization error *)
    RD_KAFKA_RESP_ERR__VALUE_DESERIALIZATION = -159,
    (** Partial response *)
    RD_KAFKA_RESP_ERR__PARTIAL = -158,
    (** Modification attempted on read-only object *)
    RD_KAFKA_RESP_ERR__READ_ONLY = -157,
    (** No such entry / item not found *)
    RD_KAFKA_RESP_ERR__NOENT = -156,
    (** Read underflow *)
    RD_KAFKA_RESP_ERR__UNDERFLOW = -155,
    (** Invalid type *)
    RD_KAFKA_RESP_ERR__INVALID_TYPE = -154,
    (** Retry operation *)
    RD_KAFKA_RESP_ERR__RETRY = -153,
    (** Purged in queue *)
    RD_KAFKA_RESP_ERR__PURGE_QUEUE = -152,
    (** Purged in flight *)
    RD_KAFKA_RESP_ERR__PURGE_INFLIGHT = -151,
    (** Fatal error: see rd_kafka_fatal_error() *)
    RD_KAFKA_RESP_ERR__FATAL = -150,
    (** Inconsistent state *)
    RD_KAFKA_RESP_ERR__INCONSISTENT = -149,
    (** Gap-less ordering would not be guaranteed if proceeding *)
    RD_KAFKA_RESP_ERR__GAPLESS_GUARANTEE = -148,
    (** Maximum poll interval exceeded *)
    RD_KAFKA_RESP_ERR__MAX_POLL_EXCEEDED = -147,
    (** Unknown broker *)
    RD_KAFKA_RESP_ERR__UNKNOWN_BROKER = -146,
    (** Functionality not configured *)
    RD_KAFKA_RESP_ERR__NOT_CONFIGURED = -145,
    (** Instance has been fenced *)
    RD_KAFKA_RESP_ERR__FENCED = -144,
    (** Application generated error *)
    RD_KAFKA_RESP_ERR__APPLICATION = -143,
    (** End internal error codes *)
    RD_KAFKA_RESP_ERR__END = -100,
    (** Unknown broker error *)
    RD_KAFKA_RESP_ERR_UNKNOWN = -1,
    (** Success *)
    RD_KAFKA_RESP_ERR_NO_ERROR = 0,
    (** Offset out of range *)
    RD_KAFKA_RESP_ERR_OFFSET_OUT_OF_RANGE = 1,
    (** Invalid message *)
    RD_KAFKA_RESP_ERR_INVALID_MSG = 2,
    (** Unknown topic or partition *)
    RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART = 3,
    (** Invalid message size *)
    RD_KAFKA_RESP_ERR_INVALID_MSG_SIZE = 4,
    (** Leader not available *)
    RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE = 5,
    (** Not leader for partition *)
    RD_KAFKA_RESP_ERR_NOT_LEADER_FOR_PARTITION = 6,
    (** Request timed out *)
    RD_KAFKA_RESP_ERR_REQUEST_TIMED_OUT = 7,
    (** Broker not available *)
    RD_KAFKA_RESP_ERR_BROKER_NOT_AVAILABLE = 8,
    (** Replica not available *)
    RD_KAFKA_RESP_ERR_REPLICA_NOT_AVAILABLE = 9,
    (** Message size too large *)
    RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE = 10,
    (** StaleControllerEpochCode *)
    RD_KAFKA_RESP_ERR_STALE_CTRL_EPOCH = 11,
    (** Offset metadata string too large *)
    RD_KAFKA_RESP_ERR_OFFSET_METADATA_TOO_LARGE = 12,
    (** Broker disconnected before response received *)
    RD_KAFKA_RESP_ERR_NETWORK_EXCEPTION = 13,
    (** Coordinator load in progress *)
    RD_KAFKA_RESP_ERR_COORDINATOR_LOAD_IN_PROGRESS = 14,
    (** Coordinator not available *)
    RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE = 15,
    (** Not coordinator *)
    RD_KAFKA_RESP_ERR_NOT_COORDINATOR = 16,
    (** Invalid topic *)
    RD_KAFKA_RESP_ERR_TOPIC_EXCEPTION = 17,
    (** Message batch larger than configured server segment size *)
    RD_KAFKA_RESP_ERR_RECORD_LIST_TOO_LARGE = 18,
    (** Not enough in-sync replicas *)
    RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS = 19,
    (** Message(s) written to insufficient number of in-sync replicas *)
    RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS_AFTER_APPEND = 20,
    (** Invalid required acks value *)
    RD_KAFKA_RESP_ERR_INVALID_REQUIRED_ACKS = 21,
    (** Specified group generation id is not valid *)
    RD_KAFKA_RESP_ERR_ILLEGAL_GENERATION = 22,
    (** Inconsistent group protocol *)
    RD_KAFKA_RESP_ERR_INCONSISTENT_GROUP_PROTOCOL = 23,
    (** Invalid group.id *)
    RD_KAFKA_RESP_ERR_INVALID_GROUP_ID = 24,
    (** Unknown member *)
    RD_KAFKA_RESP_ERR_UNKNOWN_MEMBER_ID = 25,
    (** Invalid session timeout *)
    RD_KAFKA_RESP_ERR_INVALID_SESSION_TIMEOUT = 26,
    (** Group rebalance in progress *)
    RD_KAFKA_RESP_ERR_REBALANCE_IN_PROGRESS = 27,
    (** Commit offset data size is not valid *)
    RD_KAFKA_RESP_ERR_INVALID_COMMIT_OFFSET_SIZE = 28,
    (** Topic authorization failed *)
    RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED = 29,
    (** Group authorization failed *)
    RD_KAFKA_RESP_ERR_GROUP_AUTHORIZATION_FAILED = 30,
    (** Cluster authorization failed *)
    RD_KAFKA_RESP_ERR_CLUSTER_AUTHORIZATION_FAILED = 31,
    (** Invalid timestamp *)
    RD_KAFKA_RESP_ERR_INVALID_TIMESTAMP = 32,
    (** Unsupported SASL mechanism *)
    RD_KAFKA_RESP_ERR_UNSUPPORTED_SASL_MECHANISM = 33,
    (** Illegal SASL state *)
    RD_KAFKA_RESP_ERR_ILLEGAL_SASL_STATE = 34,
    (** Unuspported version *)
    RD_KAFKA_RESP_ERR_UNSUPPORTED_VERSION = 35,
    (** Topic already exists *)
    RD_KAFKA_RESP_ERR_TOPIC_ALREADY_EXISTS = 36,
    (** Invalid number of partitions *)
    RD_KAFKA_RESP_ERR_INVALID_PARTITIONS = 37,
    (** Invalid replication factor *)
    RD_KAFKA_RESP_ERR_INVALID_REPLICATION_FACTOR = 38,
    (** Invalid replica assignment *)
    RD_KAFKA_RESP_ERR_INVALID_REPLICA_ASSIGNMENT = 39,
    (** Invalid config *)
    RD_KAFKA_RESP_ERR_INVALID_CONFIG = 40,
    (** Not controller for cluster *)
    RD_KAFKA_RESP_ERR_NOT_CONTROLLER = 41,
    (** Invalid request *)
    RD_KAFKA_RESP_ERR_INVALID_REQUEST = 42,
    (** Message format on broker does not support request *)
    RD_KAFKA_RESP_ERR_UNSUPPORTED_FOR_MESSAGE_FORMAT = 43,
    (** Policy violation *)
    RD_KAFKA_RESP_ERR_POLICY_VIOLATION = 44,
    (** Broker received an out of order sequence number *)
    RD_KAFKA_RESP_ERR_OUT_OF_ORDER_SEQUENCE_NUMBER = 45,
    (** Broker received a duplicate sequence number *)
    RD_KAFKA_RESP_ERR_DUPLICATE_SEQUENCE_NUMBER = 46,
    (** Producer attempted an operation with an old epoch *)
    RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH = 47,
    (** Producer attempted a transactional operation in an invalid state *)
    RD_KAFKA_RESP_ERR_INVALID_TXN_STATE = 48,
    (** Producer attempted to use a producer id which is not
     *  currently assigned to its transactional id *)
    RD_KAFKA_RESP_ERR_INVALID_PRODUCER_ID_MAPPING = 49,
    (** Transaction timeout is larger than the maximum
     *  value allowed by the broker's max.transaction.timeout.ms *)
    RD_KAFKA_RESP_ERR_INVALID_TRANSACTION_TIMEOUT = 50,
    (** Producer attempted to update a transaction while another
     *  concurrent operation on the same transaction was ongoing *)
    RD_KAFKA_RESP_ERR_CONCURRENT_TRANSACTIONS = 51,
    (** Indicates that the transaction coordinator sending a
     *  WriteTxnMarker is no longer the current coordinator for a
     *  given producer *)
    RD_KAFKA_RESP_ERR_TRANSACTION_COORDINATOR_FENCED = 52,
    (** Transactional Id authorization failed *)
    RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED = 53,
    (** Security features are disabled *)
    RD_KAFKA_RESP_ERR_SECURITY_DISABLED = 54,
    (** Operation not attempted *)
    RD_KAFKA_RESP_ERR_OPERATION_NOT_ATTEMPTED = 55,
    (** Disk error when trying to access log file on the disk *)
    RD_KAFKA_RESP_ERR_KAFKA_STORAGE_ERROR = 56,
    (** The user-specified log directory is not found in the broker config *)
    RD_KAFKA_RESP_ERR_LOG_DIR_NOT_FOUND = 57,
    (** SASL Authentication failed *)
    RD_KAFKA_RESP_ERR_SASL_AUTHENTICATION_FAILED = 58,
    (** Unknown Producer Id *)
    RD_KAFKA_RESP_ERR_UNKNOWN_PRODUCER_ID = 59,
    (** Partition reassignment is in progress *)
    RD_KAFKA_RESP_ERR_REASSIGNMENT_IN_PROGRESS = 60,
    (** Delegation Token feature is not enabled *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTH_DISABLED = 61,
    (** Delegation Token is not found on server *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_NOT_FOUND = 62,
    (** Specified Principal is not valid Owner/Renewer *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_OWNER_MISMATCH = 63,
    (** Delegation Token requests are not allowed on this connection *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_REQUEST_NOT_ALLOWED = 64,
    (** Delegation Token authorization failed *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTHORIZATION_FAILED = 65,
    (** Delegation Token is expired *)
    RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_EXPIRED = 66,
    (** Supplied principalType is not supported *)
    RD_KAFKA_RESP_ERR_INVALID_PRINCIPAL_TYPE = 67,
    (** The group is not empty *)
    RD_KAFKA_RESP_ERR_NON_EMPTY_GROUP = 68,
    (** The group id does not exist *)
    RD_KAFKA_RESP_ERR_GROUP_ID_NOT_FOUND = 69,
    (** The fetch session ID was not found *)
    RD_KAFKA_RESP_ERR_FETCH_SESSION_ID_NOT_FOUND = 70,
    (** The fetch session epoch is invalid *)
    RD_KAFKA_RESP_ERR_INVALID_FETCH_SESSION_EPOCH = 71,
    (** No matching listener *)
    RD_KAFKA_RESP_ERR_LISTENER_NOT_FOUND = 72,
    (** Topic deletion is disabled *)
    RD_KAFKA_RESP_ERR_TOPIC_DELETION_DISABLED = 73,
    (** Leader epoch is older than broker epoch *)
    RD_KAFKA_RESP_ERR_FENCED_LEADER_EPOCH = 74,
    (** Leader epoch is newer than broker epoch *)
    RD_KAFKA_RESP_ERR_UNKNOWN_LEADER_EPOCH = 75,
    (** Unsupported compression type *)
    RD_KAFKA_RESP_ERR_UNSUPPORTED_COMPRESSION_TYPE = 76,
    (** Broker epoch has changed *)
    RD_KAFKA_RESP_ERR_STALE_BROKER_EPOCH = 77,
    (** Leader high watermark is not caught up *)
    RD_KAFKA_RESP_ERR_OFFSET_NOT_AVAILABLE = 78,
    (** Group member needs a valid member ID *)
    RD_KAFKA_RESP_ERR_MEMBER_ID_REQUIRED = 79,
    (** Preferred leader was not available *)
    RD_KAFKA_RESP_ERR_PREFERRED_LEADER_NOT_AVAILABLE = 80,
    (** Consumer group has reached maximum size *)
    RD_KAFKA_RESP_ERR_GROUP_MAX_SIZE_REACHED = 81,
    (** Static consumer fenced by other consumer with same
     *  group.instance.id. *)
    RD_KAFKA_RESP_ERR_FENCED_INSTANCE_ID = 82,
    (** Static consumer fenced by other consumer with same
     *  group.instance.id. *)
    RD_KAFKA_RESP_ERR_END_ALL = 83);

  Prd_kafka_resp_err_t = ^rd_kafka_resp_err_t;

  (**
   * @brief Error code value, name and description.
   *        Typically for use with language bindings to automatically expose
   *        the full set of librdkafka error codes.
   *)
  rd_kafka_err_desc = record
    (** Error code *)
    code: rd_kafka_resp_err_t;
    (** Error name, same as code enum sans prefix *)
    name: PUTF8Char;
    (** Human readable error description. *)
    desc: PUTF8Char;
  end;

  (**
   * @brief Generic place holder for a specific Topic+Partition.
   *
   * @sa rd_kafka_topic_partition_list_new()
   *)
  rd_kafka_topic_partition_s = record
    (** Topic name *)
    topic: PUTF8Char;
    (** Partition *)
    partition: Int32;
    (** Offset *)
    offset: Int64;
    (** Metadata *)
    metadata: Pointer;
    (** Metadata size *)
    metadata_size: NativeUInt;
    (** Opaque value for application use *)
    opaque: Pointer;
    (** Error code, depending on use. *)
    err: rd_kafka_resp_err_t;
    (** INTERNAL USE ONLY,
     *   INITIALIZE TO ZERO, DO NOT TOUCH *)
    _private: Pointer;
  end;

  rd_kafka_topic_partition_t = rd_kafka_topic_partition_s;
  Prd_kafka_topic_partition_t = ^rd_kafka_topic_partition_t;

  (**
   * @brief A growable list of Topic+Partitions.
   *
   *)
  rd_kafka_topic_partition_list_s = record
    (** Current number of elements *)
    cnt: Integer;
    (** Current allocated size *)
    size: Integer;
    (** Element array[] *)
    elems: Prd_kafka_topic_partition_t;
  end;

  SunB = packed record
    s_b1, s_b2, s_b3, s_b4: AnsiChar;
  end;

  SunW = packed record
    s_w1, s_w2: Word;
  end;

  TInAddr = packed record
    case integer of
      0: (S_un_b: SunB);
      1: (S_un_w: SunW);
      2: (S_addr: System.Types.DWORD);
  end;
  PInAddr = ^TInAddr;

  TSockAddrIn = packed record
    case Integer of
      0: (sin_family : Word;
          sin_port   : Word;
          sin_addr   : TInAddr;
          sin_zero   : array[0..7] of Char);
      1: (sa_family  : Word;
          sa_data    : array[0..13] of Char)
  end;
  PSockAddrIn = ^TSockAddrIn;
  TSockAddr   = TSockAddrIn;
  PSockAddr   = ^TSockAddr;
  SOCKADDR    = TSockAddr;
  SOCKADDR_IN = TSockAddrIn;

  rd_kafka_topic_partition_list_t = rd_kafka_topic_partition_list_s;
  Prd_kafka_topic_partition_list_t = ^rd_kafka_topic_partition_list_t;
  PPrd_kafka_topic_partition_list_t = ^Prd_kafka_topic_partition_list_t;

  (**
   * @enum rd_kafka_vtype_t
   *
   * @brief Var-arg tag types
   *
   * @sa rd_kafka_producev()
   *)
  rd_kafka_vtype_t = (
    (** va-arg sentinel *)
    RD_KAFKA_VTYPE_END = 0,
    (** (const char * ) Topic name *)
    RD_KAFKA_VTYPE_TOPIC = 1,
    (** (rd_kafka_topic_t * ) Topic handle *)
    RD_KAFKA_VTYPE_RKT = 2,
    (** (int32_t) Partition *)
    RD_KAFKA_VTYPE_PARTITION = 3,
    (** (void *, size_t) Message value (payload)*)
    RD_KAFKA_VTYPE_VALUE = 4,
    (** (void *, size_t) Message key *)
    RD_KAFKA_VTYPE_KEY = 5,
    (** (void * ) Per-message application opaque
     *            value. This is the same as
     *            the _private field in
     *            rd_kafka_message_t, also known
     *            as the msg_opaque. *)
    RD_KAFKA_VTYPE_OPAQUE = 6,
    (** (int) RD_KAFKA_MSG_F_.. flags *)
    RD_KAFKA_VTYPE_MSGFLAGS = 7,
    (** (int64_t) Milliseconds since epoch UTC *)
    RD_KAFKA_VTYPE_TIMESTAMP = 8,
    (** (const char *, const void *, ssize_t)
     *   Message Header *)
    RD_KAFKA_VTYPE_HEADER = 9,
    (** (rd_kafka_headers_t * ) Headers list *)
    RD_KAFKA_VTYPE_HEADERS = 10);
  Prd_kafka_vtype_t = ^rd_kafka_vtype_t;

const
  RD_KAFKA_RESP_ERR_GROUP_LOAD_IN_PROGRESS = RD_KAFKA_RESP_ERR_COORDINATOR_LOAD_IN_PROGRESS;
  RD_KAFKA_RESP_ERR_GROUP_COORDINATOR_NOT_AVAILABLE = RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE;
  RD_KAFKA_RESP_ERR_NOT_COORDINATOR_FOR_GROUP = RD_KAFKA_RESP_ERR_NOT_COORDINATOR;
  RD_KAFKA_V_END = RD_KAFKA_VTYPE_END;

type

  _anonymous_type_1 = record
    ptr: Pointer;
    size: NativeUInt;
  end;
  P_anonymous_type_1 = ^_anonymous_type_1;

  _anonymous_type_2 = record
    name: PUTF8Char;
    val: Pointer;
    size: ssize_t;
  end;
  P_anonymous_type_2 = ^_anonymous_type_2;

  (** Value union, see RD_KAFKA_V_.. macros for which field to use. *)
  _anonymous_type_3 = record
    case Integer of
      0: (cstr: PUTF8Char);
      1: (rkt: Prd_kafka_topic_t);
      2: (i: Integer);
      3: (i32: Int32);
      4: (i64: Int64);
      5: (mem: _anonymous_type_1);
      6: (header: _anonymous_type_2);
      7: (headers: Prd_kafka_headers_t);
      8: (ptr: Pointer);
    (** Padding size for future-proofness *)
      9: (_pad: array [0..63] of UTF8Char);
  end;
  P_anonymous_type_3 = ^_anonymous_type_3;

  (**
   * @brief VTYPE + argument container for use with rd_kafka_produce_va()
   *
   * See RD_KAFKA_V_..() macros below for which union field corresponds
   * to which RD_KAFKA_VTYPE_...
   *)
  rd_kafka_vu_s = record
    (** RD_KAFKA_VTYPE_.. *)
    vtype: rd_kafka_vtype_t;
    u: _anonymous_type_3;
  end;

  rd_kafka_vu_t = rd_kafka_vu_s;
  Prd_kafka_vu_t = ^rd_kafka_vu_t;

  (**
   * @brief A Kafka message as returned by the \c rd_kafka_consume*() family
   *        of functions as well as provided to the Producer \c dr_msg_cb().
   *
   * For the consumer this object has two purposes:
   *  - provide the application with a consumed message. (\c err == 0)
   *  - report per-topic+partition consumer errors (\c err != 0)
   *
   * The application must check \c err to decide what action to take.
   *
   * When the application is finished with a message it must call
   * rd_kafka_message_destroy() unless otherwise noted.
   *)
  rd_kafka_message_s = record
    (** Non-zero for error signaling. *)
    err: rd_kafka_resp_err_t;
    (** Topic *)
    rkt: Prd_kafka_topic_t;
    (** Partition *)
    partition: Int32;
    (** Producer: original message payload.
     * Consumer: Depends on the value of \c err :
     * - \c err==0: Message payload.
     * - \c err!=0: Error string *)
    payload: Pointer;
    (** Depends on the value of \c err :
     * - \c err==0: Message payload length
     * - \c err!=0: Error string length *)
    len: NativeUInt;
    (** Depends on the value of \c err :
     * - \c err==0: Optional message key *)
    key: Pointer;
    (** Depends on the value of \c err :
     * - \c err==0: Optional message key length*)
    key_len: NativeUInt;
    (** Consumer:
     * - Message offset (or offset for error
     *   if \c err!=0 if applicable).
     *   Producer, dr_msg_cb:
     *   Message offset assigned by broker.
     *   May be RD_KAFKA_OFFSET_INVALID
     *   for retried messages when
     *   idempotence is enabled. *)
    offset: Int64;
    (** Consumer:
     *  - rdkafka private pointer: DO NOT MODIFY
     *  Producer:
     *  - dr_msg_cb:
     *    msg_opaque from produce() call or
     *    RD_KAFKA_V_OPAQUE from producev(). *)
    _private: Pointer;
  end;

  rd_kafka_message_t = rd_kafka_message_s;
  Prd_kafka_message_t = ^rd_kafka_message_t;
  PPrd_kafka_message_t = ^Prd_kafka_message_t;

  (**
   * @enum rd_kafka_msg_status_t
   * @brief Message persistence status can be used by the application to
   *        find out if a produced message was persisted in the topic log.
   *)
  rd_kafka_msg_status_t = (
    (** Message was never transmitted to the broker, or failed with
     *  an error indicating it was not written to the log.
     *  Application retry risks ordering, but not duplication. *)
    RD_KAFKA_MSG_STATUS_NOT_PERSISTED = 0,
    (** Message was transmitted to broker, but no acknowledgement was
     *  received.
     *  Application retry risks ordering and duplication. *)
    RD_KAFKA_MSG_STATUS_POSSIBLY_PERSISTED = 1,
    (** Message was written to the log and acknowledged by the broker.
     *  No reason for application to retry.
     *  Note: this value should only be trusted with \c acks=all. *)
    RD_KAFKA_MSG_STATUS_PERSISTED = 2);
  Prd_kafka_msg_status_t = ^rd_kafka_msg_status_t;

  (**
   * @enum rd_kafka_conf_res_t
   * @brief Configuration result type
   *)
  rd_kafka_conf_res_t = (
    (** Unknown configuration name. *)
    RD_KAFKA_CONF_UNKNOWN = -2,
    (** Invalid configuration value. *)
    RD_KAFKA_CONF_INVALID = -1,
    (** Configuration okay *)
    RD_KAFKA_CONF_OK = 0);
  Prd_kafka_conf_res_t = ^rd_kafka_conf_res_t;

  (**
   * @enum rd_kafka_cert_type_t
   *
   * @brief SSL certificate type
   *
   * @sa rd_kafka_conf_set_ssl_cert
   *)
  rd_kafka_cert_type_t = (
    (** Client's public key *)
    RD_KAFKA_CERT_PUBLIC_KEY = 0,
    (** Client's private key *)
    RD_KAFKA_CERT_PRIVATE_KEY = 1,
    (** CA certificate *)
    RD_KAFKA_CERT_CA = 2,
    RD_KAFKA_CERT__CNT = 3);
  Prd_kafka_cert_type_t = ^rd_kafka_cert_type_t;

  (**
   * @enum rd_kafka_cert_enc_t
   *
   * @brief SSL certificate encoding
   *
   * @sa rd_kafka_conf_set_ssl_cert
   *)
  rd_kafka_cert_enc_t = (
    (** PKCS#12 *)
    RD_KAFKA_CERT_ENC_PKCS12 = 0,
    (** DER / binary X.509 ASN1 *)
    RD_KAFKA_CERT_ENC_DER = 1,
    (** PEM *)
    RD_KAFKA_CERT_ENC_PEM = 2,
    RD_KAFKA_CERT_ENC__CNT = 3);
  Prd_kafka_cert_enc_t = ^rd_kafka_cert_enc_t;

  (**
   * @brief Broker information
   *)
  rd_kafka_metadata_broker = record
    (** Broker Id *)
    id: Int32;
    (** Broker hostname *)
    host: PUTF8Char;
    (** Broker listening port *)
    port: Integer;
  end;

  rd_kafka_metadata_broker_t = rd_kafka_metadata_broker;

  (**
   * @brief Partition information
   *)
  rd_kafka_metadata_partition = record
    (** Partition Id *)
    id: Int32;
    (** Partition error reported by broker *)
    err: rd_kafka_resp_err_t;
    (** Leader broker *)
    leader: Int32;
    (** Number of brokers in \p replicas *)
    replica_cnt: Integer;
    (** Replica brokers *)
    replicas: PInt32;
    (** Number of ISR brokers in \p isrs *)
    isr_cnt: Integer;
    (** In-Sync-Replica brokers *)
    isrs: PInt32;
  end;

  rd_kafka_metadata_partition_t = rd_kafka_metadata_partition;

  Prd_kafka_vu_s = ^rd_kafka_vu_s;
  Prd_kafka_message_s = ^rd_kafka_message_s;
  Prd_kafka_metadata_broker = ^rd_kafka_metadata_broker;
  Prd_kafka_metadata_partition = ^rd_kafka_metadata_partition;
  Prd_kafka_metadata_topic = ^rd_kafka_metadata_topic;
  Prd_kafka_metadata = ^rd_kafka_metadata;
  PPrd_kafka_metadata = ^Prd_kafka_metadata;
  Prd_kafka_group_member_info = ^rd_kafka_group_member_info;
  Prd_kafka_group_info = ^rd_kafka_group_info;
  Prd_kafka_group_list = ^rd_kafka_group_list;
  PPrd_kafka_group_list = ^Prd_kafka_group_list;

  (**
   * @brief Topic information
   *)
  rd_kafka_metadata_topic = record
    (** Topic name *)
    topic: PUTF8Char;
    (** Number of partitions in \p partitions*)
    partition_cnt: Integer;
    (** Partitions *)
    partitions: Prd_kafka_metadata_partition;
    (** Topic error reported by broker *)
    err: rd_kafka_resp_err_t;
  end;

  rd_kafka_metadata_topic_t = rd_kafka_metadata_topic;

  (**
   * @brief Metadata container
   *)
  rd_kafka_metadata = record
    (** Number of brokers in \p brokers *)
    broker_cnt: Integer;
    (** Brokers *)
    brokers: Prd_kafka_metadata_broker;
    (** Number of topics in \p topics *)
    topic_cnt: Integer;
    (** Topics *)
    topics: Prd_kafka_metadata_topic;
    (** Broker originating this metadata *)
    orig_broker_id: Int32;
    (** Name of originating broker *)
    orig_broker_name: PUTF8Char;
  end;

  rd_kafka_metadata_t = rd_kafka_metadata;

  (**
   * @brief Group member information
   *
   * For more information on \p member_metadata format, see
   * https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-GroupMembershipAPI
   *
   *)
  rd_kafka_group_member_info = record
    (** Member id (generated by broker) *)
    member_id: PUTF8Char;
    (** Client's \p client.id *)
    client_id: PUTF8Char;
    (** Client's hostname *)
    client_host: PUTF8Char;
    (** Member metadata (binary),
     *   format depends on \p protocol_type. *)
    member_metadata: Pointer;
    (** Member metadata size in bytes *)
    member_metadata_size: Integer;
    (** Member assignment (binary),
     *    format depends on \p protocol_type. *)
    member_assignment: Pointer;
    (** Member assignment size in bytes *)
    member_assignment_size: Integer;
  end;

  (**
   * @brief Group information
   *)
  rd_kafka_group_info = record
    (** Originating broker info *)
    broker: rd_kafka_metadata_broker;
    (** Group name *)
    group: PUTF8Char;
    (** Broker-originated error *)
    err: rd_kafka_resp_err_t;
    (** Group state *)
    state: PUTF8Char;
    (** Group protocol type *)
    protocol_type: PUTF8Char;
    (** Group protocol *)
    protocol: PUTF8Char;
    (** Group members *)
    members: Prd_kafka_group_member_info;
    (** Group member count *)
    member_cnt: Integer;
  end;

  (**
   * @brief List of groups
   *
   * @sa rd_kafka_group_list_destroy() to release list memory.
   *)
  rd_kafka_group_list = record
    (** Groups *)
    groups: Prd_kafka_group_info;
    (** Group count *)
    group_cnt: Integer;
  end;

  (**
   * @enum rd_kafka_thread_type_t
   *
   * @brief librdkafka internal thread type.
   *
   * @sa rd_kafka_interceptor_add_on_thread_start()
   *)
  rd_kafka_thread_type_t = (
    (** librdkafka's internal main thread *)
    RD_KAFKA_THREAD_MAIN = 0,
    (** Background thread (if enabled) *)
    RD_KAFKA_THREAD_BACKGROUND = 1,
    (** Per-broker thread *)
    RD_KAFKA_THREAD_BROKER = 2);
  Prd_kafka_thread_type_t = ^rd_kafka_thread_type_t;
  (**
   * @brief Event types
   *)
  rd_kafka_event_type_t = Integer;
  Prd_kafka_CreateTopics_result_t = Pointer;
  PPrd_kafka_CreateTopics_result_t = ^Prd_kafka_CreateTopics_result_t;
  Prd_kafka_DeleteTopics_result_t = Pointer;
  PPrd_kafka_DeleteTopics_result_t = ^Prd_kafka_DeleteTopics_result_t;
  Prd_kafka_CreatePartitions_result_t = Pointer;
  PPrd_kafka_CreatePartitions_result_t = ^Prd_kafka_CreatePartitions_result_t;
  Prd_kafka_AlterConfigs_result_t = Pointer;
  PPrd_kafka_AlterConfigs_result_t = ^Prd_kafka_AlterConfigs_result_t;
  Prd_kafka_DescribeConfigs_result_t = Pointer;
  PPrd_kafka_DescribeConfigs_result_t = ^Prd_kafka_DescribeConfigs_result_t;

  (**
   * @brief Plugin's configuration initializer method called each time the
   *        library is referenced from configuration (even if previously loaded by
   *        another client instance).
   *
   * @remark This method MUST be implemented by plugins and have the symbol name
   *         \c conf_init
   *
   * @param conf Configuration set up to this point.
   * @param plug_opaquep Plugin can set this pointer to a per-configuration
   *                     opaque pointer.
   * @param errstr String buffer of size \p errstr_size where plugin must write
   *               a human readable error string in the case the initializer
   *               fails (returns non-zero).
   *
   * @remark A plugin may add an on_conf_destroy() interceptor to clean up
   *         plugin-specific resources created in the plugin's conf_init() method.
   *
   * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
   *)
  Prd_kafka_plugin_f_conf_init_t = function(conf: Prd_kafka_conf_t; plug_opaquep: PPointer; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_conf_set() is called from rd_kafka_*_conf_set() in the order
   *        the interceptors were added.
   *
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   * @param name The configuration property to set.
   * @param val The configuration value to set, or NULL for reverting to default
   *            in which case the previous value should be freed.
   * @param errstr A human readable error string in case the interceptor fails.
   * @param errstr_size Maximum space (including \0) in \p errstr.
   *
   * @returns RD_KAFKA_CONF_RES_OK if the property was known and successfully
   *          handled by the interceptor, RD_KAFKA_CONF_RES_INVALID if the
   *          property was handled by the interceptor but the value was invalid,
   *          or RD_KAFKA_CONF_RES_UNKNOWN if the interceptor did not handle
   *          this property, in which case the property is passed on on the
   *          interceptor in the chain, finally ending up at the built-in
   *          configuration handler.
   *)
  Prd_kafka_interceptor_f_on_conf_set_t = function(conf: Prd_kafka_conf_t; const name: PUTF8Char; const val: PUTF8Char; errstr: PUTF8Char; errstr_size: NativeUInt; ic_opaque: Pointer): rd_kafka_conf_res_t; cdecl;

  (**
   * @brief on_conf_dup() is called from rd_kafka_conf_dup() in the
   *        order the interceptors were added and is used to let
   *        an interceptor re-register its conf interecptors with a new
   *        opaque value.
   *        The on_conf_dup() method is called prior to the configuration from
   *        \p old_conf being copied to \p new_conf.
   *
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code
   *          on failure (which is logged but otherwise ignored).
   *
   * @remark No on_conf_* interceptors are copied to the new configuration
   *         object on rd_kafka_conf_dup().
   *)
  Prd_kafka_interceptor_f_on_conf_dup_t = function(new_conf: Prd_kafka_conf_t; const old_conf: Prd_kafka_conf_t; filter_cnt: NativeUInt; filter: PPUTF8Char; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_conf_destroy() is called from rd_kafka_*_conf_destroy() in the
   *        order the interceptors were added.
   *
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *)
  Prd_kafka_interceptor_f_on_conf_destroy_t = function(ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_new() is called from rd_kafka_new() prior toreturning
   *        the newly created client instance to the application.
   *
   * @param rk The client instance.
   * @param conf The client instance's final configuration.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   * @param errstr A human readable error string in case the interceptor fails.
   * @param errstr_size Maximum space (including \0) in \p errstr.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *
   * @warning The \p rk client instance will not be fully set up when this
   *          interceptor is called and the interceptor MUST NOT call any
   *          other rk-specific APIs than rd_kafka_interceptor_add..().
   *
   *)
  Prd_kafka_interceptor_f_on_new_t = function(rk: Prd_kafka_t; const conf: Prd_kafka_conf_t; ic_opaque: Pointer; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_destroy() is called from rd_kafka_destroy() or (rd_kafka_new()
   *        if rd_kafka_new() fails during initialization).
   *
   * @param rk The client instance.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *)
  Prd_kafka_interceptor_f_on_destroy_t = function(rk: Prd_kafka_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_send() is called from rd_kafka_produce*() (et.al) prior to
   *        the partitioner being called.
   *
   * @param rk The client instance.
   * @param rkmessage The message being produced. Immutable.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @remark This interceptor is only used by producer instances.
   *
   * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
   *         by the interceptor.
   *
   * @remark If the partitioner fails or an unknown partition was specified,
   *         the on_acknowledgement() interceptor chain will be called from
   *         within the rd_kafka_produce*() call to maintain send-acknowledgement
   *         symmetry.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_send_t = function(rk: Prd_kafka_t; rkmessage: Prd_kafka_message_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_acknowledgement() is called to inform interceptors that a message
   *        was succesfully delivered or permanently failed delivery.
   *        The interceptor chain is called from internal librdkafka background
   *        threads, or rd_kafka_produce*() if the partitioner failed.
   *
   * @param rk The client instance.
   * @param rkmessage The message being produced. Immutable.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @remark This interceptor is only used by producer instances.
   *
   * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
   *         by the interceptor.
   *
   * @warning The on_acknowledgement() method may be called from internal
   *         librdkafka threads. An on_acknowledgement() interceptor MUST NOT
   *         call any librdkafka API's associated with the \p rk, or perform
   *         any blocking or prolonged work.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_acknowledgement_t = function(rk: Prd_kafka_t; rkmessage: Prd_kafka_message_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_consume() is called just prior to passing the message to the
   *        application in rd_kafka_consumer_poll(), rd_kafka_consume*(),
   *        the event interface, etc.
   *
   * @param rk The client instance.
   * @param rkmessage The message being consumed. Immutable.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @remark This interceptor is only used by consumer instances.
   *
   * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
   *         by the interceptor.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_consume_t = function(rk: Prd_kafka_t; rkmessage: Prd_kafka_message_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_commit() is called on completed or failed offset commit.
   *        It is called from internal librdkafka threads.
   *
   * @param rk The client instance.
   * @param offsets List of topic+partition+offset+error that were committed.
   *                The error message of each partition should be checked for
   *                error.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @remark This interceptor is only used by consumer instances.
   *
   * @warning The on_commit() interceptor is called from internal
   *          librdkafka threads. An on_commit() interceptor MUST NOT
   *          call any librdkafka API's associated with the \p rk, or perform
   *          any blocking or prolonged work.
   *
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_commit_t = function(rk: Prd_kafka_t; const offsets: Prd_kafka_topic_partition_list_t; err: rd_kafka_resp_err_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_request_sent() is called when a request has been fully written
   *        to a broker TCP connections socket.
   *
   * @param rk The client instance.
   * @param sockfd Socket file descriptor.
   * @param brokername Broker request is being sent to.
   * @param brokerid Broker request is being sent to.
   * @param ApiKey Kafka protocol request type.
   * @param ApiVersion Kafka protocol request type version.
   * @param Corrid Kafka protocol request correlation id.
   * @param size Size of request.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @warning The on_request_sent() interceptor is called from internal
   *          librdkafka broker threads. An on_request_sent() interceptor MUST NOT
   *          call any librdkafka API's associated with the \p rk, or perform
   *          any blocking or prolonged work.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_request_sent_t = function(rk: Prd_kafka_t; sockfd: Integer; const brokername: PUTF8Char; brokerid: Int32; ApiKey: Int16; ApiVersion: Int16; CorrId: Int32; size: NativeUInt; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_thread_start() is called from a newly created librdkafka-managed
   *        thread.

   * @param rk The client instance.
   * @param thread_type Thread type.
   * @param thread_name Human-readable thread name, may not be unique.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @warning The on_thread_start() interceptor is called from internal
   *          librdkafka threads. An on_thread_start() interceptor MUST NOT
   *          call any librdkafka API's associated with the \p rk, or perform
   *          any blocking or prolonged work.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_thread_start_t = function(rk: Prd_kafka_t; thread_type: rd_kafka_thread_type_t; const thread_name: PUTF8Char; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @brief on_thread_exit() is called just prior to a librdkafka-managed
   *        thread exiting from the exiting thread itself.
   *
   * @param rk The client instance.
   * @param thread_type Thread type.n
   * @param thread_name Human-readable thread name, may not be unique.
   * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
   *
   * @remark Depending on the thread type, librdkafka may execute additional
   *         code on the thread after on_thread_exit() returns.
   *
   * @warning The on_thread_exit() interceptor is called from internal
   *          librdkafka threads. An on_thread_exit() interceptor MUST NOT
   *          call any librdkafka API's associated with the \p rk, or perform
   *          any blocking or prolonged work.
   *
   * @returns an error code on failure, the error is logged but otherwise ignored.
   *)
  Prd_kafka_interceptor_f_on_thread_exit_t = function(rk: Prd_kafka_t; thread_type: rd_kafka_thread_type_t; const thread_name: PUTF8Char; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;

  (**
   * @enum rd_kafka_admin_op_t
   *
   * @brief Admin operation enum name for use with rd_kafka_AdminOptions_new()
   *
   * @sa rd_kafka_AdminOptions_new()
   *)
  rd_kafka_admin_op_t = (
    (** Default value *)
    RD_KAFKA_ADMIN_OP_ANY = 0,
    (** CreateTopics *)
    RD_KAFKA_ADMIN_OP_CREATETOPICS = 1,
    (** DeleteTopics *)
    RD_KAFKA_ADMIN_OP_DELETETOPICS = 2,
    (** CreatePartitions *)
    RD_KAFKA_ADMIN_OP_CREATEPARTITIONS = 3,
    (** AlterConfigs *)
    RD_KAFKA_ADMIN_OP_ALTERCONFIGS = 4,
    (** DescribeConfigs *)
    RD_KAFKA_ADMIN_OP_DESCRIBECONFIGS = 5,
    (** Number of ops defined *)
    RD_KAFKA_ADMIN_OP__CNT = 6);
  Prd_kafka_admin_op_t = ^rd_kafka_admin_op_t;
  Prd_kafka_AdminOptions_t = Pointer;
  PPrd_kafka_AdminOptions_t = ^Prd_kafka_AdminOptions_t;
  Prd_kafka_NewTopic_t = Pointer;
  PPrd_kafka_NewTopic_t = ^Prd_kafka_NewTopic_t;
  Prd_kafka_DeleteTopic_t = Pointer;
  PPrd_kafka_DeleteTopic_t = ^Prd_kafka_DeleteTopic_t;
  Prd_kafka_NewPartitions_t = Pointer;
  PPrd_kafka_NewPartitions_t = ^Prd_kafka_NewPartitions_t;

  (**
   * @enum rd_kafka_ConfigSource_t
   *
   * @brief Apache Kafka config sources.
   *
   * @remark These entities relate to the cluster, not the local client.
   *
   * @sa rd_kafka_conf_set(), et.al. for local client configuration.
   *)
  rd_kafka_ConfigSource_t = (
    (** Source unknown, e.g., in the ConfigEntry used for alter requests
     *  where source is not set *)
    RD_KAFKA_CONFIG_SOURCE_UNKNOWN_CONFIG = 0,
    (** Dynamic topic config that is configured for a specific topic *)
    RD_KAFKA_CONFIG_SOURCE_DYNAMIC_TOPIC_CONFIG = 1,
    (** Dynamic broker config that is configured for a specific broker *)
    RD_KAFKA_CONFIG_SOURCE_DYNAMIC_BROKER_CONFIG = 2,
    (** Dynamic broker config that is configured as default for all
     *  brokers in the cluster *)
    RD_KAFKA_CONFIG_SOURCE_DYNAMIC_DEFAULT_BROKER_CONFIG = 3,
    (** Static broker config provided as broker properties at startup
     *  (e.g. from server.properties file) *)
    RD_KAFKA_CONFIG_SOURCE_STATIC_BROKER_CONFIG = 4,
    (** Built-in default configuration for configs that have a
     *  default value *)
    RD_KAFKA_CONFIG_SOURCE_DEFAULT_CONFIG = 5,
    (** Number of source types defined *)
    RD_KAFKA_CONFIG_SOURCE__CNT = 6);
  Prd_kafka_ConfigSource_t = ^rd_kafka_ConfigSource_t;
  Prd_kafka_ConfigEntry_t = Pointer;
  PPrd_kafka_ConfigEntry_t = ^Prd_kafka_ConfigEntry_t;

  (*! Apache Kafka resource types *)
  rd_kafka_ResourceType_t = (
    (** Unknown *)
    RD_KAFKA_RESOURCE_UNKNOWN = 0,
    (** Any (used for lookups) *)
    RD_KAFKA_RESOURCE_ANY = 1,
    (** Topic *)
    RD_KAFKA_RESOURCE_TOPIC = 2,
    (** Group *)
    RD_KAFKA_RESOURCE_GROUP = 3,
    (** Broker *)
    RD_KAFKA_RESOURCE_BROKER = 4,
    (** Number of resource types defined *)
    RD_KAFKA_RESOURCE__CNT = 5);
  Prd_kafka_ResourceType_t = ^rd_kafka_ResourceType_t;
  Prd_kafka_ConfigResource_t = Pointer;
  PPrd_kafka_ConfigResource_t = ^Prd_kafka_ConfigResource_t;

(**
 * @brief Returns the librdkafka version as integer.
 *
 * @returns Version integer.
 *
 * @sa See RD_KAFKA_VERSION_ for how to parse the integer format.
 * @sa Use rd_kafka_version_str() to retreive the version as a string.
 *)
function rd_kafka_version(): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_version';

(**
 * @brief Returns the librdkafka version as string.
 *
 * @returns Version string
 *)
function rd_kafka_version_str(): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_version_str';

(**
 * @brief Retrieve supported debug contexts for use with the \c \"debug\"
 *        configuration property. (runtime)
 *
 * @returns Comma-separated list of available debugging contexts.
 *)
function rd_kafka_get_debug_contexts(): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_get_debug_contexts';

(**
 * @brief Returns the full list of error codes.
 *)
procedure rd_kafka_get_err_descs(errdescs: PPrd_kafka_err_desc; cntp: PNativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_get_err_descs';

(**
 * @brief Returns a human readable representation of a kafka error.
 *
 * @param err Error code to translate
 *)
function rd_kafka_err2str(err: rd_kafka_resp_err_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_err2str';

(**
 * @brief Returns the error code name (enum name).
 *
 * @param err Error code to translate
 *)
function rd_kafka_err2name(err: rd_kafka_resp_err_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_err2name';

(**
 * @brief Returns the last error code generated by a legacy API call
 *        in the current thread.
 *
 * The legacy APIs are the ones using errno to propagate error value, namely:
 *  - rd_kafka_topic_new()
 *  - rd_kafka_consume_start()
 *  - rd_kafka_consume_stop()
 *  - rd_kafka_consume()
 *  - rd_kafka_consume_batch()
 *  - rd_kafka_consume_callback()
 *  - rd_kafka_consume_queue()
 *  - rd_kafka_produce()
 *
 * The main use for this function is to avoid converting system \p errno
 * values to rd_kafka_resp_err_t codes for legacy APIs.
 *
 * @remark The last error is stored per-thread, if multiple rd_kafka_t handles
 *         are used in the same application thread the developer needs to
 *         make sure rd_kafka_last_error() is called immediately after
 *         a failed API call.
 *
 * @remark errno propagation from librdkafka is not safe on Windows
 *         and should not be used, use rd_kafka_last_error() instead.
 *)
function rd_kafka_last_error(): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_last_error';

(**
 * @brief Converts the system errno value \p errnox to a rd_kafka_resp_err_t
 *        error code upon failure from the following functions:
 *  - rd_kafka_topic_new()
 *  - rd_kafka_consume_start()
 *  - rd_kafka_consume_stop()
 *  - rd_kafka_consume()
 *  - rd_kafka_consume_batch()
 *  - rd_kafka_consume_callback()
 *  - rd_kafka_consume_queue()
 *  - rd_kafka_produce()
 *
 * @param errnox  System errno value to convert
 *
 * @returns Appropriate error code for \p errnox
 *
 * @remark A better alternative is to call rd_kafka_last_error() immediately
 *         after any of the above functions return -1 or NULL.
 *
 * @deprecated Use rd_kafka_last_error() to retrieve the last error code
 *             set by the legacy librdkafka APIs.
 *
 * @sa rd_kafka_last_error()
 *)
function rd_kafka_errno2err(errnox: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_errno2err';

(**
 * @brief Returns the thread-local system errno
 *
 * On most platforms this is the same as \p errno but in case of different
 * runtimes between library and application (e.g., Windows static DLLs)
 * this provides a means for exposing the errno librdkafka uses.
 *
 * @remark The value is local to the current calling thread.
 *
 * @deprecated Use rd_kafka_last_error() to retrieve the last error code
 *             set by the legacy librdkafka APIs.
 *)
function rd_kafka_errno(): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_errno';

(**
 * @brief Returns the first fatal error set on this client instance,
 *        or RD_KAFKA_RESP_ERR_NO_ERROR if no fatal error has occurred.
 *
 * This function is to be used with the Idempotent Producer and \c error_cb
 * to detect fatal errors.
 *
 * Generally all errors raised by \c error_cb are to be considered
 * informational and temporary, the client will try to recover from all
 * errors in a graceful fashion (by retrying, etc).
 *
 * However, some errors should logically be considered fatal to retain
 * consistency; in particular a set of errors that may occur when using the
 * Idempotent Producer and the in-order or exactly-once producer guarantees
 * can't be satisfied.
 *
 * @param rk Client instance.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written to if there is a fatal error.
 * @param errstr_size Writable size in \p errstr.
 *
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if no fatal error has been raised, else
 *          any other error code.
 *)
function rd_kafka_fatal_error(rk: Prd_kafka_t; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_fatal_error';

(**
 * @brief Trigger a fatal error for testing purposes.
 *
 * Since there is no practical way to trigger real fatal errors in the
 * idempotent producer, this method allows an application to trigger
 * fabricated fatal errors in tests to check its error handling code.
 *
 * @param rk Client instance.
 * @param err The underlying error code.
 * @param reason A human readable error reason.
 *               Will be prefixed with "test_fatal_error: " to differentiate
 *               from real fatal errors.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if a fatal error was triggered, or
 *          RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS if a previous fatal error
 *          has already been triggered.
 *)
function rd_kafka_test_fatal_error(rk: Prd_kafka_t; err: rd_kafka_resp_err_t; const reason: PUTF8Char): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_test_fatal_error';

(**
 * @returns the error code for \p error.
 *)
function rd_kafka_error_code(const error: Prd_kafka_error_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_error_code';

(**
 * @returns the error code name for \p error, e.g, "ERR_UNKNOWN_MEMBER_ID".
 *
 * @remark The lifetime of the returned pointer is the same as the error object.
 *
 * @sa rd_kafka_err2name()
 *)
function rd_kafka_error_name(const error: Prd_kafka_error_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_error_name';

(**
 * @returns a human readable error string for \p error.
 *
 * @remark The lifetime of the returned pointer is the same as the error object.
 *)
function rd_kafka_error_string(const error: Prd_kafka_error_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_error_string';

(**
 * @returns 1 if the error is a fatal error, indicating that the client
 *          instance is no longer usable, else 0.
 *)
function rd_kafka_error_is_fatal(const error: Prd_kafka_error_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_error_is_fatal';

(**
 * @returns 1 if the operation may be retried, else 0.
 *)
function rd_kafka_error_is_retriable(const error: Prd_kafka_error_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_error_is_retriable';

(**
 * @returns 1 if the error is an abortable transaction error in which case
 *          the application must call rd_kafka_abort_transaction() and
 *          start a new transaction with rd_kafka_begin_transaction() if it
 *          wishes to proceed with transactions.
 *          Else returns 0.
 *
 * @remark The return value of this method is only valid for errors returned
 *         by the transactional API.
 *)
function rd_kafka_error_txn_requires_abort(const error: Prd_kafka_error_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_error_txn_requires_abort';

(**
 * @brief Free and destroy an error object.
 *)
procedure rd_kafka_error_destroy(error: Prd_kafka_error_t); cdecl;
  external Lib name _PU + 'rd_kafka_error_destroy';

(**
 * @brief Create a new error object with error \p code and optional
 *        human readable error string in \p fmt.
 *
 * This method is mainly to be used for mocking errors in application test code.
 *
 * The returned object must be destroyed with rd_kafka_error_destroy().
 *)
function rd_kafka_error_new(code: rd_kafka_resp_err_t; const fmt: PUTF8Char): Prd_kafka_error_t varargs; cdecl;
  external Lib name _PU + 'rd_kafka_error_new';

(**
 * @brief Destroy a rd_kafka_topic_partition_t.
 * @remark This must not be called for elements in a topic partition list.
 *)
procedure rd_kafka_topic_partition_destroy(rktpar: Prd_kafka_topic_partition_t); cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_destroy';

(**
 * @brief Create a new list/vector Topic+Partition container.
 *
 * @param size  Initial allocated size used when the expected number of
 *              elements is known or can be estimated.
 *              Avoids reallocation and possibly relocation of the
 *              elems array.
 *
 * @returns A newly allocated Topic+Partition list.
 *
 * @remark Use rd_kafka_topic_partition_list_destroy() to free all resources
 *         in use by a list and the list itself.
 * @sa     rd_kafka_topic_partition_list_add()
 *)
function rd_kafka_topic_partition_list_new(size: Integer): Prd_kafka_topic_partition_list_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_new';

(**
 * @brief Free all resources used by the list and the list itself.
 *)
procedure rd_kafka_topic_partition_list_destroy(rkparlist: Prd_kafka_topic_partition_list_t); cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_destroy';

(**
 * @brief Add topic+partition to list
 *
 * @param rktparlist List to extend
 * @param topic      Topic name (copied)
 * @param partition  Partition id
 *
 * @returns The object which can be used to fill in additionals fields.
 *)
function rd_kafka_topic_partition_list_add(rktparlist: Prd_kafka_topic_partition_list_t; const topic: PUTF8Char; partition: Int32): Prd_kafka_topic_partition_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_add';

(**
 * @brief Add range of partitions from \p start to \p stop inclusive.
 *
 * @param rktparlist List to extend
 * @param topic      Topic name (copied)
 * @param start      Start partition of range
 * @param stop       Last partition of range (inclusive)
 *)
procedure rd_kafka_topic_partition_list_add_range(rktparlist: Prd_kafka_topic_partition_list_t; const topic: PUTF8Char; start: Int32; stop: Int32); cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_add_range';

(**
 * @brief Delete partition from list.
 *
 * @param rktparlist List to modify
 * @param topic      Topic name to match
 * @param partition  Partition to match
 *
 * @returns 1 if partition was found (and removed), else 0.
 *
 * @remark Any held indices to elems[] are unusable after this call returns 1.
 *)
function rd_kafka_topic_partition_list_del(rktparlist: Prd_kafka_topic_partition_list_t; const topic: PUTF8Char; partition: Int32): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_del';

(**
 * @brief Delete partition from list by elems[] index.
 *
 * @returns 1 if partition was found (and removed), else 0.
 *
 * @sa rd_kafka_topic_partition_list_del()
 *)
function rd_kafka_topic_partition_list_del_by_idx(rktparlist: Prd_kafka_topic_partition_list_t; idx: Integer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_del_by_idx';

(**
 * @brief Make a copy of an existing list.
 *
 * @param src   The existing list to copy.
 *
 * @returns A new list fully populated to be identical to \p src
 *)
function rd_kafka_topic_partition_list_copy(const src: Prd_kafka_topic_partition_list_t): Prd_kafka_topic_partition_list_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_copy';

(**
 * @brief Set offset to \p offset for \p topic and \p partition
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if \p partition was not found
 *          in the list.
 *)
function rd_kafka_topic_partition_list_set_offset(rktparlist: Prd_kafka_topic_partition_list_t; const topic: PUTF8Char; partition: Int32; offset: Int64): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_set_offset';

(**
 * @brief Find element by \p topic and \p partition.
 *
 * @returns a pointer to the first matching element, or NULL if not found.
 *)
function rd_kafka_topic_partition_list_find(rktparlist: Prd_kafka_topic_partition_list_t; const topic: PUTF8Char; partition: Int32): Prd_kafka_topic_partition_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_find';

type
  rd_kafka_topic_partition_list_sort_cmp = function(const a: Pointer; const b: Pointer; cmp_opaque: Pointer): Integer; cdecl;

(**
 * @brief Sort list using comparator \p cmp.
 *
 * If \p cmp is NULL the default comparator will be used that
 * sorts by ascending topic name and partition.
 *
 * \p cmp_opaque is provided as the \p cmp_opaque argument to \p cmp.
 *
 *)
procedure rd_kafka_topic_partition_list_sort(rktparlist: Prd_kafka_topic_partition_list_t; cmp: rd_kafka_topic_partition_list_sort_cmp; cmp_opaque: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_list_sort';

(**
 * @brief Create a new headers list.
 *
 * @param initial_count Preallocate space for this number of headers.
 *                      Any number of headers may be added, updated and
 *                      removed regardless of the initial count.
 *)
function rd_kafka_headers_new(initial_count: NativeUInt): Prd_kafka_headers_t; cdecl;
  external Lib name _PU + 'rd_kafka_headers_new';

(**
 * @brief Destroy the headers list. The object and any returned value pointers
 *        are not usable after this call.
 *)
procedure rd_kafka_headers_destroy(hdrs: Prd_kafka_headers_t); cdecl;
  external Lib name _PU + 'rd_kafka_headers_destroy';

(**
 * @brief Make a copy of headers list \p src.
 *)
function rd_kafka_headers_copy(const src: Prd_kafka_headers_t): Prd_kafka_headers_t; cdecl;
  external Lib name _PU + 'rd_kafka_headers_copy';

(**
 * @brief Add header with name \p name and value \p val (copied) of size
 *        \p size (not including null-terminator).
 *
 * @param hdrs       Headers list.
 * @param name       Header name.
 * @param name_size  Header name size (not including the null-terminator).
 *                   If -1 the \p name length is automatically acquired using
 *                   strlen().
 * @param value      Pointer to header value, or NULL (set size to 0 or -1).
 * @param value_size Size of header value. If -1 the \p value is assumed to be a
 *                   null-terminated string and the length is automatically
 *                   acquired using strlen().
 *
 * @returns RD_KAFKA_RESP_ERR__READ_ONLY if the headers are read-only,
 *          else RD_KAFKA_RESP_ERR_NO_ERROR.
 *)
function rd_kafka_header_add(hdrs: Prd_kafka_headers_t; const name: PUTF8Char; name_size: ssize_t; const value: Pointer; value_size: ssize_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_header_add';

(**
 * @brief Remove all headers for the given key (if any).
 *
 * @returns RD_KAFKA_RESP_ERR__READ_ONLY if the headers are read-only,
 *          RD_KAFKA_RESP_ERR__NOENT if no matching headers were found,
 *          else RD_KAFKA_RESP_ERR_NO_ERROR if headers were removed.
 *)
function rd_kafka_header_remove(hdrs: Prd_kafka_headers_t; const name: PUTF8Char): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_header_remove';

(**
 * @brief Find last header in list \p hdrs matching \p name.
 *
 * @param hdrs   Headers list.
 * @param name   Header to find (last match).
 * @param valuep (out) Set to a (null-terminated) const pointer to the value
 *               (may be NULL).
 * @param sizep  (out) Set to the value's size (not including null-terminator).
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if an entry was found, else
 *          RD_KAFKA_RESP_ERR__NOENT.
 *
 * @remark The returned pointer in \p valuep includes a trailing null-terminator
 *         that is not accounted for in \p sizep.
 * @remark The returned pointer is only valid as long as the headers list and
 *         the header item is valid.
 *)
function rd_kafka_header_get_last(const hdrs: Prd_kafka_headers_t; const name: PUTF8Char; valuep: PPointer; sizep: PNativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_header_get_last';

(**
 * @brief Iterator for headers matching \p name.
 *
 *        Same semantics as rd_kafka_header_get_last()
 *
 * @param hdrs   Headers to iterate.
 * @param idx    Iterator index, start at 0 and increment by one for each call
 *               as long as RD_KAFKA_RESP_ERR_NO_ERROR is returned.
 * @param name   Header name to match.
 * @param valuep (out) Set to a (null-terminated) const pointer to the value
 *               (may be NULL).
 * @param sizep  (out) Set to the value's size (not including null-terminator).
 *)
function rd_kafka_header_get(const hdrs: Prd_kafka_headers_t; idx: NativeUInt; const name: PUTF8Char; valuep: PPointer; sizep: PNativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_header_get';

(**
 * @brief Iterator for all headers.
 *
 *        Same semantics as rd_kafka_header_get()
 *
 * @sa rd_kafka_header_get()
 *)
function rd_kafka_header_get_all(const hdrs: Prd_kafka_headers_t; idx: NativeUInt; namep: PPUTF8Char; valuep: PPointer; sizep: PNativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_header_get_all';

(**
 * @brief Frees resources for \p rkmessage and hands ownership back to rdkafka.
 *)
procedure rd_kafka_message_destroy(rkmessage: Prd_kafka_message_t); cdecl;
  external Lib name _PU + 'rd_kafka_message_destroy';

(**
 * @brief Returns the error string for an errored rd_kafka_message_t or NULL if
 *        there was no error.
 *
 * @remark This function MUST NOT be used with the producer.
 *)
function rd_kafka_message_errstr(const rkmessage: Prd_kafka_message_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_message_errstr';

(**
 * @brief Returns the message timestamp for a consumed message.
 *
 * The timestamp is the number of milliseconds since the epoch (UTC).
 *
 * \p tstype (if not NULL) is updated to indicate the type of timestamp.
 *
 * @returns message timestamp, or -1 if not available.
 *
 * @remark Message timestamps require broker version 0.10.0 or later.
 *)
function rd_kafka_message_timestamp(const rkmessage: Prd_kafka_message_t; tstype: Prd_kafka_timestamp_type_t): Int64; cdecl;
  external Lib name _PU + 'rd_kafka_message_timestamp';

(**
 * @brief Returns the latency for a produced message measured from
 *        the produce() call.
 *
 * @returns the latency in microseconds, or -1 if not available.
 *)
function rd_kafka_message_latency(const rkmessage: Prd_kafka_message_t): Int64; cdecl;
  external Lib name _PU + 'rd_kafka_message_latency';

(**
 * @brief Returns the broker id of the broker the message was produced to
 *        or fetched from.
 *
 * @returns a broker id if known, else -1.
 *)
function rd_kafka_message_broker_id(const rkmessage: Prd_kafka_message_t): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_message_broker_id';

(**
 * @brief Get the message header list.
 *
 * The returned pointer in \p *hdrsp is associated with the \p rkmessage and
 * must not be used after destruction of the message object or the header
 * list is replaced with rd_kafka_message_set_headers().
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if headers were returned,
 *          RD_KAFKA_RESP_ERR__NOENT if the message has no headers,
 *          or another error code if the headers could not be parsed.
 *
 * @remark Headers require broker version 0.11.0.0 or later.
 *
 * @remark As an optimization the raw protocol headers are parsed on
 *         the first call to this function.
 *)
function rd_kafka_message_headers(const rkmessage: Prd_kafka_message_t; hdrsp: PPrd_kafka_headers_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_message_headers';

(**
 * @brief Get the message header list and detach the list from the message
 *        making the application the owner of the headers.
 *        The application must eventually destroy the headers using
 *        rd_kafka_headers_destroy().
 *        The message's headers will be set to NULL.
 *
 *        Otherwise same semantics as rd_kafka_message_headers()
 *
 * @sa rd_kafka_message_headers
 *)
function rd_kafka_message_detach_headers(rkmessage: Prd_kafka_message_t; hdrsp: PPrd_kafka_headers_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_message_detach_headers';

(**
 * @brief Replace the message's current headers with a new list.
 *
 * @param rkmessage The message to set headers.
 * @param hdrs New header list. The message object assumes ownership of
 *             the list, the list will be destroyed automatically with
 *             the message object.
 *             The new headers list may be updated until the message object
 *             is passed or returned to librdkafka.
 *
 * @remark The existing headers object, if any, will be destroyed.
 *)
procedure rd_kafka_message_set_headers(rkmessage: Prd_kafka_message_t; hdrs: Prd_kafka_headers_t); cdecl;
  external Lib name _PU + 'rd_kafka_message_set_headers';

(**
 * @brief Returns the number of header key/value pairs
 *
 * @param hdrs   Headers to count
 *)
function rd_kafka_header_cnt(const hdrs: Prd_kafka_headers_t): NativeUInt; cdecl;
  external Lib name _PU + 'rd_kafka_header_cnt';

(**
 * @brief Returns the message's persistence status in the topic log.
 *
 * @remark The message status is not available in on_acknowledgement
 *         interceptors.
 *)
function rd_kafka_message_status(const rkmessage: Prd_kafka_message_t): rd_kafka_msg_status_t; cdecl;
  external Lib name _PU + 'rd_kafka_message_status';

(**
 * @brief Create configuration object.
 *
 * When providing your own configuration to the \c rd_kafka_*_new_*() calls
 * the rd_kafka_conf_t objects needs to be created with this function
 * which will set up the defaults.
 * I.e.:
 * @code
 *   rd_kafka_conf_t *myconf;
 *   rd_kafka_conf_res_t res;
 *
 *   myconf = rd_kafka_conf_new();
 *   res = rd_kafka_conf_set(myconf, "socket.timeout.ms", "600",
 *                           errstr, sizeof(errstr));
 *   if (res != RD_KAFKA_CONF_OK)
 *      die("%s\n", errstr);
 *
 *   rk = rd_kafka_new(..., myconf);
 * @endcode
 *
 * Please see CONFIGURATION.md for the default settings or use
 * rd_kafka_conf_properties_show() to provide the information at runtime.
 *
 * The properties are identical to the Apache Kafka configuration properties
 * whenever possible.
 *
 * @remark A successful call to rd_kafka_new() will assume ownership of
 * the conf object and rd_kafka_conf_destroy() must not be called.
 *
 * @returns A new rd_kafka_conf_t object with defaults set.
 *
 * @sa rd_kafka_new(), rd_kafka_conf_set(), rd_kafka_conf_destroy()
 *)
function rd_kafka_conf_new(): Prd_kafka_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_new';

(**
 * @brief Destroys a conf object.
 *)
procedure rd_kafka_conf_destroy(conf: Prd_kafka_conf_t); cdecl;
  external Lib name _PU + 'rd_kafka_conf_destroy';

(**
 * @brief Creates a copy/duplicate of configuration object \p conf
 *
 * @remark Interceptors are NOT copied to the new configuration object.
 * @sa rd_kafka_interceptor_f_on_conf_dup
 *)
function rd_kafka_conf_dup(const conf: Prd_kafka_conf_t): Prd_kafka_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_dup';

(**
 * @brief Same as rd_kafka_conf_dup() but with an array of property name
 *        prefixes to filter out (ignore) when copying.
 *)
function rd_kafka_conf_dup_filter(const conf: Prd_kafka_conf_t; filter_cnt: NativeUInt; filter: PPUTF8Char): Prd_kafka_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_dup_filter';

(**
 * @returns the configuration object used by an rd_kafka_t instance.
 *          For use with rd_kafka_conf_get(), et.al., to extract configuration
 *          properties from a running client.
 *
 * @remark the returned object is read-only and its lifetime is the same
 *         as the rd_kafka_t object.
 *)
function rd_kafka_conf(rk: Prd_kafka_t): Prd_kafka_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf';

(**
 * @brief Sets a configuration property.
 *
 * \p conf must have been previously created with rd_kafka_conf_new().
 *
 * Fallthrough:
 * Topic-level configuration properties may be set using this interface
 * in which case they are applied on the \c default_topic_conf.
 * If no \c default_topic_conf has been set one will be created.
 * Any sub-sequent rd_kafka_conf_set_default_topic_conf() calls will
 * replace the current default topic configuration.
 *
 * @returns \c rd_kafka_conf_res_t to indicate success or failure.
 * In case of failure \p errstr is updated to contain a human readable
 * error string.
 *)
function rd_kafka_conf_set(conf: Prd_kafka_conf_t; const name: PUTF8Char; const value: PUTF8Char; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_set';

(**
 * @brief Enable event sourcing.
 * \p events is a bitmask of \c RD_KAFKA_EVENT_* of events to enable
 * for consumption by `rd_kafka_queue_poll()`.
 *)
procedure rd_kafka_conf_set_events(conf: Prd_kafka_conf_t; events: Integer); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_events';

type
  rd_kafka_conf_set_background_event_cb_event_cb = procedure(rk: Prd_kafka_t; rkev: Prd_kafka_event_t; opaque: Pointer); cdecl;

(**
 * @brief Generic event callback to be used with the event API to trigger
 *        callbacks for \c rd_kafka_event_t objects from a background
 *        thread serving the background queue.
 *
 * How to use:
 *  1. First set the event callback on the configuration object with this
 *     function, followed by creating an rd_kafka_t instance
 *     with rd_kafka_new().
 *  2. Get the instance's background queue with rd_kafka_queue_get_background()
 *     and pass it as the reply/response queue to an API that takes an
 *     event queue, such as rd_kafka_CreateTopics().
 *  3. As the response event is ready and enqueued on the background queue the
 *     event callback will be triggered from the background thread.
 *  4. Prior to destroying the client instance, loose your reference to the
 *     background queue by calling rd_kafka_queue_destroy().
 *
 * The application must destroy the \c rkev passed to \p event cb using
 * rd_kafka_event_destroy().
 *
 * The \p event_cb \c opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark This callback is a specialized alternative to the poll-based
 *         event API described in the Event interface section.
 *
 * @remark The \p event_cb will be called spontaneously from a background
 *         thread completely managed by librdkafka.
 *         Take care to perform proper locking of application objects.
 *
 * @warning The application MUST NOT call rd_kafka_destroy() from the
 *          event callback.
 *
 * @sa rd_kafka_queue_get_background
 *)
procedure rd_kafka_conf_set_background_event_cb(conf: Prd_kafka_conf_t; event_cb: rd_kafka_conf_set_background_event_cb_event_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_background_event_cb';

type
  rd_kafka_conf_set_dr_cb_dr_cb = procedure(rk: Prd_kafka_t; payload: Pointer; len: NativeUInt; err: rd_kafka_resp_err_t; opaque: Pointer; msg_opaque: Pointer); cdecl;

(**
 * @deprecated See rd_kafka_conf_set_dr_msg_cb()
 *)
procedure rd_kafka_conf_set_dr_cb(conf: Prd_kafka_conf_t; dr_cb: rd_kafka_conf_set_dr_cb_dr_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_dr_cb';

type
  rd_kafka_conf_set_dr_msg_cb_dr_msg_cb = procedure(rk: Prd_kafka_t; const rkmessage: Prd_kafka_message_t; opaque: Pointer); cdecl;

(**
 * @brief \b Producer: Set delivery report callback in provided \p conf object.
 *
 * The delivery report callback will be called once for each message
 * accepted by rd_kafka_produce() (et.al) with \p err set to indicate
 * the result of the produce request.
 *
 * The callback is called when a message is succesfully produced or
 * if librdkafka encountered a permanent failure.
 * Delivery errors occur when the retry count is exceeded, when the
 * message.timeout.ms timeout is exceeded or there is a permanent error
 * like RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART.
 *
 * An application must call rd_kafka_poll() at regular intervals to
 * serve queued delivery report callbacks.
 *
 * The broker-assigned offset can be retrieved with \c rkmessage->offset
 * and the timestamp can be retrieved using rd_kafka_message_timestamp().
 *
 * The \p dr_msg_cb \c opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 * The per-message msg_opaque value is available in
 * \c rd_kafka_message_t._private.
 *
 * @remark The Idempotent Producer may return invalid timestamp
 *         (RD_KAFKA_TIMESTAMP_NOT_AVAILABLE), and
 *         and offset (RD_KAFKA_OFFSET_INVALID) for retried messages
 *         that were previously successfully delivered but not properly
 *         acknowledged.
 *)
procedure rd_kafka_conf_set_dr_msg_cb(conf: Prd_kafka_conf_t; dr_msg_cb: rd_kafka_conf_set_dr_msg_cb_dr_msg_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_dr_msg_cb';

type
  rd_kafka_conf_set_consume_cb_consume_cb = procedure(rkmessage: Prd_kafka_message_t; opaque: Pointer); cdecl;

(**
 * @brief \b Consumer: Set consume callback for use with
 *        rd_kafka_consumer_poll()
 *
 * The \p consume_cb \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *)
procedure rd_kafka_conf_set_consume_cb(conf: Prd_kafka_conf_t; consume_cb: rd_kafka_conf_set_consume_cb_consume_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_consume_cb';

type
  rd_kafka_conf_set_rebalance_cb_rebalance_cb = procedure(rk: Prd_kafka_t; err: rd_kafka_resp_err_t; partitions: Prd_kafka_topic_partition_list_t; opaque: Pointer); cdecl;

(**
 * @brief \b Consumer: Set rebalance callback for use with
 *                     coordinated consumer group balancing.
 *
 * The \p err field is set to either RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
 * or RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS and 'partitions'
 * contains the full partition set that was either assigned or revoked.
 *
 * Registering a \p rebalance_cb turns off librdkafka's automatic
 * partition assignment/revocation and instead delegates that responsibility
 * to the application's \p rebalance_cb.
 *
 * The rebalance callback is responsible for updating librdkafka's
 * assignment set based on the two events: RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
 * and RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS but should also be able to handle
 * arbitrary rebalancing failures where \p err is neither of those.
 * @remark In this latter case (arbitrary error), the application must
 *         call rd_kafka_assign(rk, NULL) to synchronize state.
 *
 * Without a rebalance callback this is done automatically by librdkafka
 * but registering a rebalance callback gives the application flexibility
 * in performing other operations along with the assigning/revocation,
 * such as fetching offsets from an alternate location (on assign)
 * or manually committing offsets (on revoke).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The \p partitions list is destroyed by librdkafka on return
 *         return from the rebalance_cb and must not be freed or
 *         saved by the application.
 *
 * @remark Be careful when modifying the \p partitions list.
 *         Changing this list should only be done to change the initial
 *         offsets for each partition.
 *         But a function like `rd_kafka_position()` might have unexpected
 *         effects for instance when a consumer gets assigned a partition
 *         it used to consume at an earlier rebalance. In this case, the
 *         list of partitions will be updated with the old offset for that
 *         partition. In this case, it is generally better to pass a copy
 *         of the list (see `rd_kafka_topic_partition_list_copy()`).
 *         The result of `rd_kafka_position()` is typically outdated in
 *         RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS.
 *
 * The following example shows the application's responsibilities:
 * @code
 *    static void rebalance_cb (rd_kafka_t *rk, rd_kafka_resp_err_t err,
 *                              rd_kafka_topic_partition_list_t *partitions,
 *                              void *opaque) {
 *
 *        switch (err)
 *        {
 *          case RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS:
 *             // application may load offets from arbitrary external
 *             // storage here and update \p partitions
 *
 *             rd_kafka_assign(rk, partitions);
 *             break;
 *
 *          case RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS:
 *             if (manual_commits) // Optional explicit manual commit
 *                 rd_kafka_commit(rk, partitions, 0); // sync commit
 *
 *             rd_kafka_assign(rk, NULL);
 *             break;
 *
 *          default:
 *             handle_unlikely_error(err);
 *             rd_kafka_assign(rk, NULL); // sync state
 *             break;
 *         }
 *    }
 * @endcode
 *)
procedure rd_kafka_conf_set_rebalance_cb(conf: Prd_kafka_conf_t; rebalance_cb: rd_kafka_conf_set_rebalance_cb_rebalance_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_rebalance_cb';

type
  rd_kafka_conf_set_offset_commit_cb_offset_commit_cb = procedure(rk: Prd_kafka_t; err: rd_kafka_resp_err_t; offsets: Prd_kafka_topic_partition_list_t; opaque: Pointer); cdecl;

(**
 * @brief \b Consumer: Set offset commit callback for use with consumer groups.
 *
 * The results of automatic or manual offset commits will be scheduled
 * for this callback and is served by rd_kafka_consumer_poll().
 *
 * If no partitions had valid offsets to commit this callback will be called
 * with \p err == RD_KAFKA_RESP_ERR__NO_OFFSET which is not to be considered
 * an error.
 *
 * The \p offsets list contains per-partition information:
 *   - \c offset: committed offset (attempted)
 *   - \c err:    commit error
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *)
procedure rd_kafka_conf_set_offset_commit_cb(conf: Prd_kafka_conf_t; offset_commit_cb: rd_kafka_conf_set_offset_commit_cb_offset_commit_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_offset_commit_cb';

type
  rd_kafka_conf_set_error_cb_error_cb = procedure(rk: Prd_kafka_t; err: Integer; const reason: PUTF8Char; opaque: Pointer); cdecl;

(**
 * @brief Set error callback in provided conf object.
 *
 * The error callback is used by librdkafka to signal warnings and errors
 * back to the application.
 *
 * These errors should generally be considered informational and non-permanent,
 * the client will try to recover automatically from all type of errors.
 * Given that the client and cluster configuration is correct the
 * application should treat these as temporary errors.
 *
 * \p error_cb will be triggered with \c err set to RD_KAFKA_RESP_ERR__FATAL
 * if a fatal error has been raised; in this case use rd_kafka_fatal_error() to
 * retrieve the fatal error code and error string, and then begin terminating
 * the client instance.
 *
 * If no \p error_cb is registered, or RD_KAFKA_EVENT_ERROR_ has not been set
 * with rd_kafka_conf_set_events, then the errors will be logged instead.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *)
procedure rd_kafka_conf_set_error_cb(conf: Prd_kafka_conf_t; error_cb: rd_kafka_conf_set_error_cb_error_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_error_cb';

type
  rd_kafka_conf_set_throttle_cb_throttle_cb = procedure(rk: Prd_kafka_t; const broker_name: PUTF8Char; broker_id: Int32; throttle_time_ms: Integer; opaque: Pointer); cdecl;

(**
 * @brief Set throttle callback.
 *
 * The throttle callback is used to forward broker throttle times to the
 * application for Produce and Fetch (consume) requests.
 *
 * Callbacks are triggered whenever a non-zero throttle time is returned by
 * the broker, or when the throttle time drops back to zero.
 *
 * An application must call rd_kafka_poll() or rd_kafka_consumer_poll() at
 * regular intervals to serve queued callbacks.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark Requires broker version 0.9.0 or later.
 *)
procedure rd_kafka_conf_set_throttle_cb(conf: Prd_kafka_conf_t; throttle_cb: rd_kafka_conf_set_throttle_cb_throttle_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_throttle_cb';

type
  rd_kafka_conf_set_log_cb_log_cb = procedure(const rk: Prd_kafka_t; level: Integer; const fac: PUTF8Char; const buf: PUTF8Char); cdecl;

(**
 * @brief Set logger callback.
 *
 * The default is to print to stderr, but a syslog logger is also available,
 * see rd_kafka_log_print and rd_kafka_log_syslog for the builtin alternatives.
 * Alternatively the application may provide its own logger callback.
 * Or pass \p func as NULL to disable logging.
 *
 * This is the configuration alternative to the deprecated rd_kafka_set_logger()
 *
 * @remark The log_cb will be called spontaneously from librdkafka's internal
 *         threads unless logs have been forwarded to a poll queue through
 *         \c rd_kafka_set_log_queue().
 *         An application MUST NOT call any librdkafka APIs or do any prolonged
 *         work in a non-forwarded \c log_cb.
 *)
procedure rd_kafka_conf_set_log_cb(conf: Prd_kafka_conf_t; log_cb: rd_kafka_conf_set_log_cb_log_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_log_cb';

type
  rd_kafka_conf_set_stats_cb_stats_cb = function(rk: Prd_kafka_t; json: PUTF8Char; json_len: NativeUInt; opaque: Pointer): Integer; cdecl;

(**
 * @brief Set statistics callback in provided conf object.
 *
 * The statistics callback is triggered from rd_kafka_poll() every
 * \c statistics.interval.ms (needs to be configured separately).
 * Function arguments:
 *   - \p rk - Kafka handle
 *   - \p json - String containing the statistics data in JSON format
 *   - \p json_len - Length of \p json string.
 *   - \p opaque - Application-provided opaque as set by
 *                 rd_kafka_conf_set_opaque().
 *
 * For more information on the format of \p json, see
 * https://github.com/edenhill/librdkafka/wiki/Statistics
 *
 * If the application wishes to hold on to the \p json pointer and free
 * it at a later time it must return 1 from the \p stats_cb.
 * If the application returns 0 from the \p stats_cb then librdkafka
 * will immediately free the \p json pointer.
 *
 * See STATISTICS.md for a full definition of the JSON object.
 *)
procedure rd_kafka_conf_set_stats_cb(conf: Prd_kafka_conf_t; stats_cb: rd_kafka_conf_set_stats_cb_stats_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_stats_cb';

type
  rd_kafka_conf_set_oauthbearer_token_refresh_cb_oauthbearer_token_refresh_cb = procedure(rk: Prd_kafka_t; const oauthbearer_config: PUTF8Char; opaque: Pointer); cdecl;

(**
 * @brief Set SASL/OAUTHBEARER token refresh callback in provided conf object.
 *
 * @param conf the configuration to mutate.
 * @param oauthbearer_token_refresh_cb the callback to set; callback function
 *  arguments:<br>
 *   \p rk - Kafka handle<br>
 *   \p oauthbearer_config - Value of configuration property
 *                           sasl.oauthbearer.config.
 *   \p opaque - Application-provided opaque set via
 *               rd_kafka_conf_set_opaque()
 *
 * The SASL/OAUTHBEARER token refresh callback is triggered via rd_kafka_poll()
 * whenever OAUTHBEARER is the SASL mechanism and a token needs to be retrieved,
 * typically based on the configuration defined in \c sasl.oauthbearer.config.
 *
 * The callback should invoke rd_kafka_oauthbearer_set_token()
 * or rd_kafka_oauthbearer_set_token_failure() to indicate success
 * or failure, respectively.
 *
 * The refresh operation is eventable and may be received via
 * rd_kafka_queue_poll() with an event type of
 * \c RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH.
 *
 * Note that before any SASL/OAUTHBEARER broker connection can succeed the
 * application must call rd_kafka_oauthbearer_set_token() once -- either
 * directly or, more typically, by invoking either rd_kafka_poll() or
 * rd_kafka_queue_poll() -- in order to cause retrieval of an initial token to
 * occur.
 *
 * An unsecured JWT refresh handler is provided by librdkafka for development
 * and testing purposes, it is enabled by setting
 * the \c enable.sasl.oauthbearer.unsecure.jwt property to true and is
 * mutually exclusive to using a refresh callback.
 *)
procedure rd_kafka_conf_set_oauthbearer_token_refresh_cb(conf: Prd_kafka_conf_t; oauthbearer_token_refresh_cb: rd_kafka_conf_set_oauthbearer_token_refresh_cb_oauthbearer_token_refresh_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_oauthbearer_token_refresh_cb';

type
  rd_kafka_conf_set_socket_cb_socket_cb = function(domain: Integer; &type: Integer; protocol: Integer; opaque: Pointer): Integer; cdecl;

(**
 * @brief Set socket callback.
 *
 * The socket callback is responsible for opening a socket
 * according to the supplied \p domain, \p type and \p protocol.
 * The socket shall be created with \c CLOEXEC set in a racefree fashion, if
 * possible.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * Default:
 *  - on linux: racefree CLOEXEC
 *  - others  : non-racefree CLOEXEC
 *
 * @remark The callback will be called from an internal librdkafka thread.
 *)
procedure rd_kafka_conf_set_socket_cb(conf: Prd_kafka_conf_t; socket_cb: rd_kafka_conf_set_socket_cb_socket_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_socket_cb';

type
  rd_kafka_conf_set_connect_cb_connect_cb = function(sockfd: Integer; const addr: Psockaddr; addrlen: Integer; const id: PUTF8Char; opaque: Pointer): Integer; cdecl;

(**
 * @brief Set connect callback.
 *
 * The connect callback is responsible for connecting socket \p sockfd
 * to peer address \p addr.
 * The \p id field contains the broker identifier.
 *
 * \p connect_cb shall return 0 on success (socket connected) or an error
 * number (errno) on error.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The callback will be called from an internal librdkafka thread.
 *)
//procedure rd_kafka_conf_set_connect_cb(conf: Prd_kafka_conf_t; connect_cb: rd_kafka_conf_set_connect_cb_connect_cb); cdecl;
//  external Lib name _PU + 'rd_kafka_conf_set_connect_cb';

type
  rd_kafka_conf_set_closesocket_cb_closesocket_cb = function(sockfd: Integer; opaque: Pointer): Integer; cdecl;

(**
 * @brief Set close socket callback.
 *
 * Close a socket (optionally opened with socket_cb()).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The callback will be called from an internal librdkafka thread.
 *)
procedure rd_kafka_conf_set_closesocket_cb(conf: Prd_kafka_conf_t; closesocket_cb: rd_kafka_conf_set_closesocket_cb_closesocket_cb); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_closesocket_cb';

type
  rd_kafka_conf_set_ssl_cert_verify_cb_ssl_cert_verify_cb = function(rk: Prd_kafka_t; const broker_name: PUTF8Char; broker_id: Int32; x509_error: PInteger; depth: Integer; const buf: PUTF8Char; size: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt; opaque: Pointer): Integer; cdecl;

(**
 * @brief Sets the verification callback of the broker certificate
 *
 * The verification callback is triggered from internal librdkafka threads
 * upon connecting to a broker. On each connection attempt the callback
 * will be called for each certificate in the broker's certificate chain,
 * starting at the root certification, as long as the application callback
 * returns 1 (valid certificate).
 * \c broker_name and \c broker_id correspond to the broker the connection
 * is being made to.
 * The \c x509_error argument indicates if OpenSSL's verification of
 * the certificate succeed (0) or failed (an OpenSSL error code).
 * The application may set the SSL context error code by returning 0
 * from the verify callback and providing a non-zero SSL context error code
 * in \c x509_error.
 * If the verify callback sets \c x509_error to 0, returns 1, and the
 * original \c x509_error was non-zero, the error on the SSL context will
 * be cleared.
 * \c x509_error is always a valid pointer to an int.
 *
 * \c depth is the depth of the current certificate in the chain, starting
 * at the root certificate.
 *
 * The certificate itself is passed in binary DER format in \c buf of
 * size \c size.
 *
 * The callback must return 1 if verification succeeds, or
 * 0 if verification fails and then write a human-readable error message
 * to \c errstr (limited to \c errstr_size bytes, including nul-term).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @returns RD_KAFKA_CONF_OK if SSL is supported in this build, else
 *          RD_KAFKA_CONF_INVALID.
 *
 * @warning This callback will be called from internal librdkafka threads.
 *
 * @remark See <openssl/x509_vfy.h> in the OpenSSL source distribution
 *         for a list of \p x509_error codes.
 *)
function rd_kafka_conf_set_ssl_cert_verify_cb(conf: Prd_kafka_conf_t; ssl_cert_verify_cb: rd_kafka_conf_set_ssl_cert_verify_cb_ssl_cert_verify_cb): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_ssl_cert_verify_cb';

(**
 * @brief Set certificate/key \p cert_type from the \p cert_enc encoded
 *        memory at \p buffer of \p size bytes.
 *
 * @param conf Configuration object.
 * @param cert_type Certificate or key type to configure.
 * @param cert_enc  Buffer \p encoding type.
 * @param buffer Memory pointer to encoded certificate or key.
 *               The memory is not referenced after this function returns.
 * @param size Size of memory at \p buffer.
 * @param errstr Memory were a human-readable error string will be written
 *               on failure.
 * @param errstr_size Size of \p errstr, including space for nul-terminator.
 *
 * @returns RD_KAFKA_CONF_OK on success or RD_KAFKA_CONF_INVALID if the
 *          memory in \p buffer is of incorrect encoding, or if librdkafka
 *          was not built with SSL support.
 *
 * @remark Calling this method multiple times with the same \p cert_type
 *         will replace the previous value.
 *
 * @remark Calling this method with \p buffer set to NULL will clear the
 *         configuration for \p cert_type.
 *
 * @remark The private key may require a password, which must be specified
 *         with the `ssl.key.password` configuration property prior to
 *         calling this function.
 *
 * @remark Private and public keys in PEM format may also be set with the
 *         `ssl.key.pem` and `ssl.certificate.pem` configuration properties.
 *)
function rd_kafka_conf_set_ssl_cert(conf: Prd_kafka_conf_t; cert_type: rd_kafka_cert_type_t; cert_enc: rd_kafka_cert_enc_t; const buffer: Pointer; size: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_ssl_cert';

(**
 * @brief Sets the application's opaque pointer that will be passed to callbacks
 *
 * @sa rd_kafka_opaque()
 *)
procedure rd_kafka_conf_set_opaque(conf: Prd_kafka_conf_t; opaque: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_opaque';

(**
 * @brief Retrieves the opaque pointer previously set
 *        with rd_kafka_conf_set_opaque()
 *)
function rd_kafka_opaque(const rk: Prd_kafka_t): Pointer; cdecl;
  external Lib name _PU + 'rd_kafka_opaque';

(**
 * @brief Sets the default topic configuration to use for automatically
 *        subscribed topics (e.g., through pattern-matched topics).
 *        The topic config object is not usable after this call.
 *
 * @warning Any topic configuration settings that have been set on the
 *          global rd_kafka_conf_t object will be overwritten by this call
 *          since the implicitly created default topic config object is
 *          replaced by the user-supplied one.
 *
 * @deprecated Set default topic level configuration on the
 *             global rd_kafka_conf_t object instead.
 *)
procedure rd_kafka_conf_set_default_topic_conf(conf: Prd_kafka_conf_t; tconf: Prd_kafka_topic_conf_t); cdecl;
  external Lib name _PU + 'rd_kafka_conf_set_default_topic_conf';

(**
 * @brief Retrieve configuration value for property \p name.
 *
 * If \p dest is non-NULL the value will be written to \p dest with at
 * most \p dest_size.
 *
 * \p *dest_size is updated to the full length of the value, thus if
 * \p *dest_size initially is smaller than the full length the application
 * may reallocate \p dest to fit the returned \p *dest_size and try again.
 *
 * If \p dest is NULL only the full length of the value is returned.
 *
 * Fallthrough:
 * Topic-level configuration properties from the \c default_topic_conf
 * may be retrieved using this interface.
 *
 * @returns \p RD_KAFKA_CONF_OK if the property name matched, else
 * \p RD_KAFKA_CONF_UNKNOWN.
 *)
function rd_kafka_conf_get(const conf: Prd_kafka_conf_t; const name: PUTF8Char; dest: PUTF8Char; dest_size: PNativeUInt): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_get';

(**
 * @brief Retrieve topic configuration value for property \p name.
 *
 * @sa rd_kafka_conf_get()
 *)
function rd_kafka_topic_conf_get(const conf: Prd_kafka_topic_conf_t; const name: PUTF8Char; dest: PUTF8Char; dest_size: PNativeUInt): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_get';

(**
 * @brief Dump the configuration properties and values of \p conf to an array
 *        with \"key\", \"value\" pairs.
 *
 * The number of entries in the array is returned in \p *cntp.
 *
 * The dump must be freed with `rd_kafka_conf_dump_free()`.
 *)
function rd_kafka_conf_dump(conf: Prd_kafka_conf_t; cntp: PNativeUInt): PPUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_conf_dump';

(**
 * @brief Dump the topic configuration properties and values of \p conf
 *        to an array with \"key\", \"value\" pairs.
 *
 * The number of entries in the array is returned in \p *cntp.
 *
 * The dump must be freed with `rd_kafka_conf_dump_free()`.
 *)
function rd_kafka_topic_conf_dump(conf: Prd_kafka_topic_conf_t; cntp: PNativeUInt): PPUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_dump';

(**
 * @brief Frees a configuration dump returned from `rd_kafka_conf_dump()` or
 *        `rd_kafka_topic_conf_dump().
 *)
procedure rd_kafka_conf_dump_free(arr: PPUTF8Char; cnt: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_conf_dump_free';

(**
 * @brief Prints a table to \p fp of all supported configuration properties,
 *        their default values as well as a description.
 *)
procedure rd_kafka_conf_properties_show(fp: PPointer); cdecl;
  external Lib name _PU + 'rd_kafka_conf_properties_show';

(**
 * @brief Create topic configuration object
 *
 * @sa Same semantics as for rd_kafka_conf_new().
 *)
function rd_kafka_topic_conf_new(): Prd_kafka_topic_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_new';

(**
 * @brief Creates a copy/duplicate of topic configuration object \p conf.
 *)
function rd_kafka_topic_conf_dup(const conf: Prd_kafka_topic_conf_t): Prd_kafka_topic_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_dup';

(**
 * @brief Creates a copy/duplicate of \p rk 's default topic configuration
 *        object.
 *)
function rd_kafka_default_topic_conf_dup(rk: Prd_kafka_t): Prd_kafka_topic_conf_t; cdecl;
  external Lib name _PU + 'rd_kafka_default_topic_conf_dup';

(**
 * @brief Destroys a topic conf object.
 *)
procedure rd_kafka_topic_conf_destroy(topic_conf: Prd_kafka_topic_conf_t); cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_destroy';

(**
 * @brief Sets a single rd_kafka_topic_conf_t value by property name.
 *
 * \p topic_conf should have been previously set up
 * with `rd_kafka_topic_conf_new()`.
 *
 * @returns rd_kafka_conf_res_t to indicate success or failure.
 *)
function rd_kafka_topic_conf_set(conf: Prd_kafka_topic_conf_t; const name: PUTF8Char; const value: PUTF8Char; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_conf_res_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_set';

(**
 * @brief Sets the application's opaque pointer that will be passed to all topic
 * callbacks as the \c rkt_opaque argument.
 *
 * @sa rd_kafka_topic_opaque()
 *)
procedure rd_kafka_topic_conf_set_opaque(conf: Prd_kafka_topic_conf_t; rkt_opaque: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_set_opaque';

type
  rd_kafka_topic_conf_set_partitioner_cb_partitioner = function(const rkt: Prd_kafka_topic_t; const keydata: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;

(**
 * @brief \b Producer: Set partitioner callback in provided topic conf object.
 *
 * The partitioner may be called in any thread at any time,
 * it may be called multiple times for the same message/key.
 *
 * The callback's \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The callback's \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * Partitioner function constraints:
 *   - MUST NOT call any rd_kafka_*() functions except:
 *       rd_kafka_topic_partition_available()
 *   - MUST NOT block or execute for prolonged periods of time.
 *   - MUST return a value between 0 and partition_cnt-1, or the
 *     special \c RD_KAFKA_PARTITION_UA value if partitioning
 *     could not be performed.
 *)
procedure rd_kafka_topic_conf_set_partitioner_cb(topic_conf: Prd_kafka_topic_conf_t; partitioner: rd_kafka_topic_conf_set_partitioner_cb_partitioner); cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_set_partitioner_cb';

type
  rd_kafka_topic_conf_set_msg_order_cmp_msg_order_cmp = function(const a: Prd_kafka_message_t; const b: Prd_kafka_message_t): Integer; cdecl;

(**
 * @brief \b Producer: Set message queueing order comparator callback.
 *
 * The callback may be called in any thread at any time,
 * it may be called multiple times for the same message.
 *
 * Ordering comparator function constraints:
 *   - MUST be stable sort (same input gives same output).
 *   - MUST NOT call any rd_kafka_*() functions.
 *   - MUST NOT block or execute for prolonged periods of time.
 *
 * The comparator shall compare the two messages and return:
 *  - < 0 if message \p a should be inserted before message \p b.
 *  - >=0 if message \p a should be inserted after message \p b.
 *
 * @remark Insert sorting will be used to enqueue the message in the
 *         correct queue position, this comes at a cost of O(n).
 *
 * @remark If `queuing.strategy=fifo` new messages are enqueued to the
 *         tail of the queue regardless of msg_order_cmp, but retried messages
 *         are still affected by msg_order_cmp.
 *
 * @warning THIS IS AN EXPERIMENTAL API, SUBJECT TO CHANGE OR REMOVAL,
 *          DO NOT USE IN PRODUCTION.
 *)
procedure rd_kafka_topic_conf_set_msg_order_cmp(topic_conf: Prd_kafka_topic_conf_t; msg_order_cmp: rd_kafka_topic_conf_set_msg_order_cmp_msg_order_cmp); cdecl;
  external Lib name _PU + 'rd_kafka_topic_conf_set_msg_order_cmp';

(**
 * @brief Check if partition is available (has a leader broker).
 *
 * @returns 1 if the partition is available, else 0.
 *
 * @warning This function must only be called from inside a partitioner function
 *)
function rd_kafka_topic_partition_available(const rkt: Prd_kafka_topic_t; partition: Int32): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_topic_partition_available';

(**
 * @brief Random partitioner.
 *
 * Will try not to return unavailable partitions.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a random partition between 0 and \p partition_cnt - 1.
 *
 *)
function rd_kafka_msg_partitioner_random(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_random';

(**
 * @brief Consistent partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a \"random\" partition between 0 and \p partition_cnt - 1 based on
 *          the CRC value of the key
 *)
function rd_kafka_msg_partitioner_consistent(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_consistent';

(**
 * @brief Consistent-Random partitioner.
 *
 * This is the default partitioner.
 * Uses consistent hashing to map identical keys onto identical partitions, and
 * messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a \"random\" partition between 0 and \p partition_cnt - 1 based on
 *          the CRC value of the key (if provided)
 *)
function rd_kafka_msg_partitioner_consistent_random(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_consistent_random';

(**
 * @brief Murmur2 partitioner (Java compatible).
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using Java-compatible Murmur2 hashing.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 *)
function rd_kafka_msg_partitioner_murmur2(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_murmur2';

(**
 * @brief Consistent-Random Murmur2 partitioner (Java compatible).
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using Java-compatible Murmur2 hashing.
 * Messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 *)
function rd_kafka_msg_partitioner_murmur2_random(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_murmur2_random';

(**
 * @brief FNV-1a partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using FNV-1a hashing.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 *)
function rd_kafka_msg_partitioner_fnv1a(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_fnv1a';

(**
 * @brief Consistent-Random FNV-1a partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using FNV-1a hashing.
 * Messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 *)
function rd_kafka_msg_partitioner_fnv1a_random(const rkt: Prd_kafka_topic_t; const key: Pointer; keylen: NativeUInt; partition_cnt: Int32; rkt_opaque: Pointer; msg_opaque: Pointer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_msg_partitioner_fnv1a_random';

(**
 * @brief Creates a new Kafka handle and starts its operation according to the
 *        specified \p type (\p RD_KAFKA_CONSUMER or \p RD_KAFKA_PRODUCER).
 *
 * \p conf is an optional struct created with `rd_kafka_conf_new()` that will
 * be used instead of the default configuration.
 * The \p conf object is freed by this function on success and must not be used
 * or destroyed by the application sub-sequently.
 * See `rd_kafka_conf_set()` et.al for more information.
 *
 * \p errstr must be a pointer to memory of at least size \p errstr_size where
 * `rd_kafka_new()` may write a human readable error message in case the
 * creation of a new handle fails. In which case the function returns NULL.
 *
 * @remark \b RD_KAFKA_CONSUMER: When a new \p RD_KAFKA_CONSUMER
 *           rd_kafka_t handle is created it may either operate in the
 *           legacy simple consumer mode using the rd_kafka_consume_start()
 *           interface, or the High-level KafkaConsumer API.
 * @remark An application must only use one of these groups of APIs on a given
 *         rd_kafka_t RD_KAFKA_CONSUMER handle.

 *
 * @returns The Kafka handle on success or NULL on error (see \p errstr)
 *
 * @sa To destroy the Kafka handle, use rd_kafka_destroy().
 *)
function rd_kafka_new(&type: rd_kafka_type_t; conf: Prd_kafka_conf_t; errstr: PUTF8Char; errstr_size: NativeUInt): Prd_kafka_t; cdecl;
  external Lib name _PU + 'rd_kafka_new';

(**
 * @brief Destroy Kafka handle.
 *
 * @remark This is a blocking operation.
 * @remark rd_kafka_consumer_close() will be called from this function
 *         if the instance type is RD_KAFKA_CONSUMER, a \c group.id was
 *         configured, and the rd_kafka_consumer_close() was not
 *         explicitly called by the application. This in turn may
 *         trigger consumer callbacks, such as rebalance_cb.
 *         Use rd_kafka_destroy_flags() with
 *         RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE to avoid this behaviour.
 *
 * @sa rd_kafka_destroy_flags()
 *)
procedure rd_kafka_destroy(rk: Prd_kafka_t); cdecl;
  external Lib name _PU + 'rd_kafka_destroy';

(**
 * @brief Destroy Kafka handle according to specified destroy flags
 *
 *)
procedure rd_kafka_destroy_flags(rk: Prd_kafka_t; flags: Integer); cdecl;
  external Lib name _PU + 'rd_kafka_destroy_flags';

(**
 * @brief Returns Kafka handle name.
 *)
function rd_kafka_name(const rk: Prd_kafka_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_name';

(**
 * @brief Returns Kafka handle type.
 *)
function rd_kafka_type(const rk: Prd_kafka_t): rd_kafka_type_t; cdecl;
  external Lib name _PU + 'rd_kafka_type';

(**
 * @brief Returns this client's broker-assigned group member id.
 *
 * @remark This currently requires the high-level KafkaConsumer
 *
 * @returns An allocated string containing the current broker-assigned group
 *          member id, or NULL if not available.
 *          The application must free the string with \p free() or
 *          rd_kafka_mem_free()
 *)
function rd_kafka_memberid(const rk: Prd_kafka_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_memberid';

(**
 * @brief Returns the ClusterId as reported in broker metadata.
 *
 * @param rk         Client instance.
 * @param timeout_ms If there is no cached value from metadata retrieval
 *                   then this specifies the maximum amount of time
 *                   (in milliseconds) the call will block waiting
 *                   for metadata to be retrieved.
 *                   Use 0 for non-blocking calls.

 * @remark Requires broker version >=0.10.0 and api.version.request=true.
 *
 * @remark The application must free the returned pointer
 *         using rd_kafka_mem_free().
 *
 * @returns a newly allocated string containing the ClusterId, or NULL
 *          if no ClusterId could be retrieved in the allotted timespan.
 *)
function rd_kafka_clusterid(rk: Prd_kafka_t; timeout_ms: Integer): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_clusterid';

(**
 * @brief Returns the current ControllerId as reported in broker metadata.
 *
 * @param rk         Client instance.
 * @param timeout_ms If there is no cached value from metadata retrieval
 *                   then this specifies the maximum amount of time
 *                   (in milliseconds) the call will block waiting
 *                   for metadata to be retrieved.
 *                   Use 0 for non-blocking calls.

 * @remark Requires broker version >=0.10.0 and api.version.request=true.
 *
 * @returns the controller broker id (>= 0), or -1 if no ControllerId could be
 *          retrieved in the allotted timespan.
 *)
function rd_kafka_controllerid(rk: Prd_kafka_t; timeout_ms: Integer): Int32; cdecl;
  external Lib name _PU + 'rd_kafka_controllerid';

(**
 * @brief Creates a new topic handle for topic named \p topic.
 *
 * \p conf is an optional configuration for the topic created with
 * `rd_kafka_topic_conf_new()` that will be used instead of the default
 * topic configuration.
 * The \p conf object is freed by this function and must not be used or
 * destroyed by the application sub-sequently.
 * See `rd_kafka_topic_conf_set()` et.al for more information.
 *
 * Topic handles are refcounted internally and calling rd_kafka_topic_new()
 * again with the same topic name will return the previous topic handle
 * without updating the original handle's configuration.
 * Applications must eventually call rd_kafka_topic_destroy() for each
 * succesfull call to rd_kafka_topic_new() to clear up resources.
 *
 * @returns the new topic handle or NULL on error (use rd_kafka_errno2err()
 *          to convert system \p errno to an rd_kafka_resp_err_t error code.
 *
 * @sa rd_kafka_topic_destroy()
 *)
function rd_kafka_topic_new(rk: Prd_kafka_t; const topic: PUTF8Char; conf: Prd_kafka_topic_conf_t): Prd_kafka_topic_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_new';

(**
 * @brief Loose application's topic handle refcount as previously created
 *        with `rd_kafka_topic_new()`.
 *
 * @remark Since topic objects are refcounted (both internally and for the app)
 *         the topic object might not actually be destroyed by this call,
 *         but the application must consider the object destroyed.
 *)
procedure rd_kafka_topic_destroy(rkt: Prd_kafka_topic_t); cdecl;
  external Lib name _PU + 'rd_kafka_topic_destroy';

(**
 * @brief Returns the topic name.
 *)
function rd_kafka_topic_name(const rkt: Prd_kafka_topic_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_topic_name';

(**
 * @brief Get the \p rkt_opaque pointer that was set in the topic configuration
 *        with rd_kafka_topic_conf_set_opaque().
 *)
function rd_kafka_topic_opaque(const rkt: Prd_kafka_topic_t): Pointer; cdecl;
  external Lib name _PU + 'rd_kafka_topic_opaque';

(**
 * @brief Polls the provided kafka handle for events.
 *
 * Events will cause application provided callbacks to be called.
 *
 * The \p timeout_ms argument specifies the maximum amount of time
 * (in milliseconds) that the call will block waiting for events.
 * For non-blocking calls, provide 0 as \p timeout_ms.
 * To wait indefinately for an event, provide -1.
 *
 * @remark  An application should make sure to call poll() at regular
 *          intervals to serve any queued callbacks waiting to be called.
 * @remark  If your producer doesn't have any callback set (in particular
 *          via rd_kafka_conf_set_dr_msg_cb or rd_kafka_conf_set_error_cb)
 *          you might chose not to call poll(), though this is not
 *          recommended.
 *
 * Events:
 *   - delivery report callbacks  (if dr_cb/dr_msg_cb is configured) [producer]
 *   - error callbacks (rd_kafka_conf_set_error_cb()) [all]
 *   - stats callbacks (rd_kafka_conf_set_stats_cb()) [all]
 *   - throttle callbacks (rd_kafka_conf_set_throttle_cb()) [all]
 *   - OAUTHBEARER token refresh callbacks (rd_kafka_conf_set_oauthbearer_token_refresh_cb()) [all]
 *
 * @returns the number of events served.
 *)
function rd_kafka_poll(rk: Prd_kafka_t; timeout_ms: Integer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_poll';

(**
 * @brief Cancels the current callback dispatcher (rd_kafka_poll(),
 *        rd_kafka_consume_callback(), etc).
 *
 * A callback may use this to force an immediate return to the calling
 * code (caller of e.g. rd_kafka_poll()) without processing any further
 * events.
 *
 * @remark This function MUST ONLY be called from within a librdkafka callback.
 *)
procedure rd_kafka_yield(rk: Prd_kafka_t); cdecl;
  external Lib name _PU + 'rd_kafka_yield';

(**
 * @brief Pause producing or consumption for the provided list of partitions.
 *
 * Success or error is returned per-partition \p err in the \p partitions list.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 *)
function rd_kafka_pause_partitions(rk: Prd_kafka_t; partitions: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_pause_partitions';

(**
 * @brief Resume producing consumption for the provided list of partitions.
 *
 * Success or error is returned per-partition \p err in the \p partitions list.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 *)
function rd_kafka_resume_partitions(rk: Prd_kafka_t; partitions: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_resume_partitions';

(**
 * @brief Query broker for low (oldest/beginning) and high (newest/end) offsets
 *        for partition.
 *
 * Offsets are returned in \p *low and \p *high respectively.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on failure.
 *)
function rd_kafka_query_watermark_offsets(rk: Prd_kafka_t; const topic: PUTF8Char; partition: Int32; low: PInt64; high: PInt64; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_query_watermark_offsets';

(**
 * @brief Get last known low (oldest/beginning) and high (newest/end) offsets
 *        for partition.
 *
 * The low offset is updated periodically (if statistics.interval.ms is set)
 * while the high offset is updated on each fetched message set from the broker.
 *
 * If there is no cached offset (either low or high, or both) then
 * RD_KAFKA_OFFSET_INVALID will be returned for the respective offset.
 *
 * Offsets are returned in \p *low and \p *high respectively.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on failure.
 *
 * @remark Shall only be used with an active consumer instance.
 *)
function rd_kafka_get_watermark_offsets(rk: Prd_kafka_t; const topic: PUTF8Char; partition: Int32; low: PInt64; high: PInt64): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_get_watermark_offsets';

(**
 * @brief Look up the offsets for the given partitions by timestamp.
 *
 * The returned offset for each partition is the earliest offset whose
 * timestamp is greater than or equal to the given timestamp in the
 * corresponding partition.
 *
 * The timestamps to query are represented as \c offset in \p offsets
 * on input, and \c offset will contain the offset on output.
 *
 * The function will block for at most \p timeout_ms milliseconds.
 *
 * @remark Duplicate Topic+Partitions are not supported.
 * @remark Per-partition errors may be returned in \c rd_kafka_topic_partition_t.err
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if offsets were be queried (do note
 *          that per-partition errors might be set),
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if not all offsets could be fetched
 *          within \p timeout_ms,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if the \p offsets list is empty,
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if all partitions are unknown,
 *          RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE if unable to query leaders
 *          for the given partitions.
 *)
function rd_kafka_offsets_for_times(rk: Prd_kafka_t; offsets: Prd_kafka_topic_partition_list_t; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_offsets_for_times';

(**
 * @brief Free pointer returned by librdkafka
 *
 * This is typically an abstraction for the free(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * freeing pointers returned by librdkafka.
 *
 * In standard setups it is usually not necessary to use this interface
 * rather than the free(3) functione.
 *
 * \p rk must be set for memory returned by APIs that take an \c rk argument,
 * for other APIs pass NULL for \p rk.
 *
 * @remark rd_kafka_mem_free() must only be used for pointers returned by APIs
 *         that explicitly mention using this function for freeing.
 *)
procedure rd_kafka_mem_free(rk: Prd_kafka_t; ptr: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_mem_free';

(**
 * @brief Create a new message queue.
 *
 * See rd_kafka_consume_start_queue(), rd_kafka_consume_queue(), et.al.
 *)
function rd_kafka_queue_new(rk: Prd_kafka_t): Prd_kafka_queue_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_new';

(**
 * Destroy a queue, purging all of its enqueued messages.
 *)
procedure rd_kafka_queue_destroy(rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_queue_destroy';

(**
 * @returns a reference to the main librdkafka event queue.
 * This is the queue served by rd_kafka_poll().
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *)
function rd_kafka_queue_get_main(rk: Prd_kafka_t): Prd_kafka_queue_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_get_main';

(**
 * @returns a reference to the librdkafka consumer queue.
 * This is the queue served by rd_kafka_consumer_poll().
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @remark rd_kafka_queue_destroy() MUST be called on this queue
 *         prior to calling rd_kafka_consumer_close().
 *)
function rd_kafka_queue_get_consumer(rk: Prd_kafka_t): Prd_kafka_queue_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_get_consumer';

(**
 * @returns a reference to the partition's queue, or NULL if
 *          partition is invalid.
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @remark rd_kafka_queue_destroy() MUST be called on this queue
 *
 * @remark This function only works on consumers.
 *)
function rd_kafka_queue_get_partition(rk: Prd_kafka_t; const topic: PUTF8Char; partition: Int32): Prd_kafka_queue_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_get_partition';

(**
 * @returns a reference to the background thread queue, or NULL if the
 *          background queue is not enabled.
 *
 * To enable the background thread queue set a generic event handler callback
 * with rd_kafka_conf_set_background_event_cb() on the client instance
 * configuration object (rd_kafka_conf_t).
 *
 * The background queue is polled and served by librdkafka and MUST NOT be
 * polled, forwarded, or otherwise managed by the application, it may only
 * be used as the destination queue passed to queue-enabled APIs, such as
 * the Admin API.
 *
 * The background thread queue provides the application with an automatically
 * polled queue that triggers the event callback in a background thread,
 * this background thread is completely managed by librdkafka.
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @warning The background queue MUST NOT be read from (polled, consumed, etc),
 *          or forwarded from.
 *)
function rd_kafka_queue_get_background(rk: Prd_kafka_t): Prd_kafka_queue_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_get_background';

(**
 * @brief Forward/re-route queue \p src to \p dst.
 * If \p dst is \c NULL the forwarding is removed.
 *
 * The internal refcounts for both queues are increased.
 *
 * @remark Regardless of whether \p dst is NULL or not, after calling this
 *         function, \p src will not forward it's fetch queue to the consumer
 *         queue.
 *)
procedure rd_kafka_queue_forward(src: Prd_kafka_queue_t; dst: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_queue_forward';

(**
 * @brief Forward librdkafka logs (and debug) to the specified queue
 *        for serving with one of the ..poll() calls.
 *
 *        This allows an application to serve log callbacks (\c log_cb)
 *        in its thread of choice.
 *
 * @param rk   Client instance.
 * @param rkqu Queue to forward logs to. If the value is NULL the logs
 *        are forwarded to the main queue.
 *
 * @remark The configuration property \c log.queue MUST also be set to true.
 *
 * @remark librdkafka maintains its own reference to the provided queue.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
 *)
function rd_kafka_set_log_queue(rk: Prd_kafka_t; rkqu: Prd_kafka_queue_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_set_log_queue';

(**
 * @returns the current number of elements in queue.
 *)
function rd_kafka_queue_length(rkqu: Prd_kafka_queue_t): NativeUInt; cdecl;
  external Lib name _PU + 'rd_kafka_queue_length';

(**
 * @brief Enable IO event triggering for queue.
 *
 * To ease integration with IO based polling loops this API
 * allows an application to create a separate file-descriptor
 * that librdkafka will write \p payload (of size \p size) to
 * whenever a new element is enqueued on a previously empty queue.
 *
 * To remove event triggering call with \p fd = -1.
 *
 * librdkafka will maintain a copy of the \p payload.
 *
 * @remark IO and callback event triggering are mutually exclusive.
 * @remark When using forwarded queues the IO event must only be enabled
 *         on the final forwarded-to (destination) queue.
 *)
procedure rd_kafka_queue_io_event_enable(rkqu: Prd_kafka_queue_t; fd: Integer; const payload: Pointer; size: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_queue_io_event_enable';

type
  rd_kafka_queue_cb_event_enable_event_cb = procedure(rk: Prd_kafka_t; qev_opaque: Pointer); cdecl;

(**
 * @brief Enable callback event triggering for queue.
 *
 * The callback will be called from an internal librdkafka thread
 * when a new element is enqueued on a previously empty queue.
 *
 * To remove event triggering call with \p event_cb = NULL.
 *
 * The \p qev_opaque is passed to the callback's \p qev_opaque argument.
 *
 * @remark IO and callback event triggering are mutually exclusive.
 * @remark Since the callback may be triggered from internal librdkafka
 *         threads, the application must not perform any pro-longed work in
 *         the callback, or call any librdkafka APIs (for the same rd_kafka_t
 *         handle).
 *)
procedure rd_kafka_queue_cb_event_enable(rkqu: Prd_kafka_queue_t; event_cb: rd_kafka_queue_cb_event_enable_event_cb; qev_opaque: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_queue_cb_event_enable';

(**
 * @brief Start consuming messages for topic \p rkt and \p partition
 * at offset \p offset which may either be an absolute \c (0..N)
 * or one of the logical offsets:
 *  - RD_KAFKA_OFFSET_BEGINNING
 *  - RD_KAFKA_OFFSET_END
 *  - RD_KAFKA_OFFSET_STORED
 *  - RD_KAFKA_OFFSET_TAIL
 *
 * rdkafka will attempt to keep \c queued.min.messages (config property)
 * messages in the local queue by repeatedly fetching batches of messages
 * from the broker until the threshold is reached.
 *
 * The application shall use one of the `rd_kafka_consume*()` functions
 * to consume messages from the local queue, each kafka message being
 * represented as a `rd_kafka_message_t *` object.
 *
 * `rd_kafka_consume_start()` must not be called multiple times for the same
 * topic and partition without stopping consumption first with
 * `rd_kafka_consume_stop()`.
 *
 * @returns 0 on success or -1 on error in which case errno is set accordingly:
 *  - EBUSY    - Conflicts with an existing or previous subscription
 *               (RD_KAFKA_RESP_ERR__CONFLICT)
 *  - EINVAL   - Invalid offset, or incomplete configuration (lacking group.id)
 *               (RD_KAFKA_RESP_ERR__INVALID_ARG)
 *  - ESRCH    - requested \p partition is invalid.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
 *  - ENOENT   - topic is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
 *
 * Use `rd_kafka_errno2err()` to convert sytem \c errno to `rd_kafka_resp_err_t`
 *)
function rd_kafka_consume_start(rkt: Prd_kafka_topic_t; partition: Int32; offset: Int64): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_consume_start';

(**
 * @brief Same as rd_kafka_consume_start() but re-routes incoming messages to
 * the provided queue \p rkqu (which must have been previously allocated
 * with `rd_kafka_queue_new()`.
 *
 * The application must use one of the `rd_kafka_consume_*_queue()` functions
 * to receive fetched messages.
 *
 * `rd_kafka_consume_start_queue()` must not be called multiple times for the
 * same topic and partition without stopping consumption first with
 * `rd_kafka_consume_stop()`.
 * `rd_kafka_consume_start()` and `rd_kafka_consume_start_queue()` must not
 * be combined for the same topic and partition.
 *)
function rd_kafka_consume_start_queue(rkt: Prd_kafka_topic_t; partition: Int32; offset: Int64; rkqu: Prd_kafka_queue_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_consume_start_queue';

(**
 * @brief Stop consuming messages for topic \p rkt and \p partition, purging
 * all messages currently in the local queue.
 *
 * NOTE: To enforce synchronisation this call will block until the internal
 *       fetcher has terminated and offsets are committed to configured
 *       storage method.
 *
 * The application needs to be stop all consumers before calling
 * `rd_kafka_destroy()` on the main object handle.
 *
 * @returns 0 on success or -1 on error (see `errno`).
 *)
function rd_kafka_consume_stop(rkt: Prd_kafka_topic_t; partition: Int32): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_consume_stop';

(**
 * @brief Seek consumer for topic+partition to \p offset which is either an
 *        absolute or logical offset.
 *
 * If \p timeout_ms is not 0 the call will wait this long for the
 * seek to be performed. If the timeout is reached the internal state
 * will be unknown and this function returns `RD_KAFKA_RESP_ERR__TIMED_OUT`.
 * If \p timeout_ms is 0 it will initiate the seek but return
 * immediately without any error reporting (e.g., async).
 *
 * This call triggers a fetch queue barrier flush.
 *
 * @returns `RD_KAFKA_RESP_ERR__NO_ERROR` on success else an error code.
 *)
function rd_kafka_seek(rkt: Prd_kafka_topic_t; partition: Int32; offset: Int64; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_seek';

(**
 * @brief Consume a single message from topic \p rkt and \p partition
 *
 * \p timeout_ms is maximum amount of time to wait for a message to be received.
 * Consumer must have been previously started with `rd_kafka_consume_start()`.
 *
 * @returns a message object on success or \c NULL on error.
 * The message object must be destroyed with `rd_kafka_message_destroy()`
 * when the application is done with it.
 *
 * Errors (when returning NULL):
 *  - ETIMEDOUT - \p timeout_ms was reached with no new messages fetched.
 *  - ENOENT    - \p rkt + \p partition is unknown.
 *                 (no prior `rd_kafka_consume_start()` call)
 *
 * NOTE: The returned message's \c ..->err must be checked for errors.
 * NOTE: \c ..->err \c == \c RD_KAFKA_RESP_ERR__PARTITION_EOF signals that the
 *       end of the partition has been reached, which should typically not be
 *       considered an error. The application should handle this case
 *       (e.g., ignore).
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *)
function rd_kafka_consume(rkt: Prd_kafka_topic_t; partition: Int32; timeout_ms: Integer): Prd_kafka_message_t; cdecl;
  external Lib name _PU + 'rd_kafka_consume';

(**
 * @brief Consume up to \p rkmessages_size from topic \p rkt and \p partition
 *        putting a pointer to each message in the application provided
 *        array \p rkmessages (of size \p rkmessages_size entries).
 *
 * `rd_kafka_consume_batch()` provides higher throughput performance
 * than `rd_kafka_consume()`.
 *
 * \p timeout_ms is the maximum amount of time to wait for all of
 * \p rkmessages_size messages to be put into \p rkmessages.
 * If no messages were available within the timeout period this function
 * returns 0 and \p rkmessages remains untouched.
 * This differs somewhat from `rd_kafka_consume()`.
 *
 * The message objects must be destroyed with `rd_kafka_message_destroy()`
 * when the application is done with it.
 *
 * @returns the number of rkmessages added in \p rkmessages,
 * or -1 on error (same error codes as for `rd_kafka_consume()`.
 *
 * @sa rd_kafka_consume()
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *)
function rd_kafka_consume_batch(rkt: Prd_kafka_topic_t; partition: Int32; timeout_ms: Integer; rkmessages: PPrd_kafka_message_t; rkmessages_size: NativeUInt): ssize_t; cdecl;
  external Lib name _PU + 'rd_kafka_consume_batch';

type
  rd_kafka_consume_callback_consume_cb = procedure(rkmessage: Prd_kafka_message_t; commit_opaque: Pointer); cdecl;

(**
 * @brief Consumes messages from topic \p rkt and \p partition, calling
 * the provided callback for each consumed messsage.
 *
 * `rd_kafka_consume_callback()` provides higher throughput performance
 * than both `rd_kafka_consume()` and `rd_kafka_consume_batch()`.
 *
 * \p timeout_ms is the maximum amount of time to wait for one or more messages
 * to arrive.
 *
 * The provided \p consume_cb function is called for each message,
 * the application \b MUST \b NOT call `rd_kafka_message_destroy()` on the
 * provided \p rkmessage.
 *
 * The \p commit_opaque argument is passed to the \p consume_cb
 * as \p commit_opaque.
 *
 * @returns the number of messages processed or -1 on error.
 *
 * @sa rd_kafka_consume()
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *
 * @remark This function will return early if a transaction control message is
 *         received, these messages are not exposed to the application but
 *         still enqueued on the consumer queue to make sure their
 *         offsets are stored.
 *
 * @deprecated This API is deprecated and subject for future removal.
 *             There is no new callback-based consume interface, use the
 *             poll/queue based alternatives.
 *)
function rd_kafka_consume_callback(rkt: Prd_kafka_topic_t; partition: Int32; timeout_ms: Integer; consume_cb: rd_kafka_consume_callback_consume_cb; commit_opaque: Pointer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_consume_callback';

(**
 * @brief Consume from queue
 *
 * @sa rd_kafka_consume()
 *)
function rd_kafka_consume_queue(rkqu: Prd_kafka_queue_t; timeout_ms: Integer): Prd_kafka_message_t; cdecl;
  external Lib name _PU + 'rd_kafka_consume_queue';

(**
 * @brief Consume batch of messages from queue
 *
 * @sa rd_kafka_consume_batch()
 *)
function rd_kafka_consume_batch_queue(rkqu: Prd_kafka_queue_t; timeout_ms: Integer; rkmessages: PPrd_kafka_message_t; rkmessages_size: NativeUInt): ssize_t; cdecl;
  external Lib name _PU + 'rd_kafka_consume_batch_queue';

type
  rd_kafka_consume_callback_queue_consume_cb = procedure(rkmessage: Prd_kafka_message_t; commit_opaque: Pointer); cdecl;

(**
 * @brief Consume multiple messages from queue with callback
 *
 * @sa rd_kafka_consume_callback()
 *
 * @deprecated This API is deprecated and subject for future removal.
 *             There is no new callback-based consume interface, use the
 *             poll/queue based alternatives.
 *)
function rd_kafka_consume_callback_queue(rkqu: Prd_kafka_queue_t; timeout_ms: Integer; consume_cb: rd_kafka_consume_callback_queue_consume_cb; commit_opaque: Pointer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_consume_callback_queue';

(**
 * @brief Store offset \p offset + 1 for topic \p rkt partition \p partition.
 *
 * The \c offset + 1 will be committed (written) to broker (or file) according
 * to \c `auto.commit.interval.ms` or manual offset-less commit()
 *
 * @remark \c `enable.auto.offset.store` must be set to "false" when using
 *         this API.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
 *)
function rd_kafka_offset_store(rkt: Prd_kafka_topic_t; partition: Int32; offset: Int64): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_offset_store';

(**
 * @brief Store offsets for next auto-commit for one or more partitions.
 *
 * The offset will be committed (written) to the offset store according
 * to \c `auto.commit.interval.ms` or manual offset-less commit().
 *
 * Per-partition success/error status propagated through each partition's
 * \c .err field.
 *
 * @remark The \c .offset field is stored as is, it will NOT be + 1.
 *
 * @remark \c `enable.auto.offset.store` must be set to "false" when using
 *         this API.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if none of the
 *          offsets could be stored, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \c enable.auto.offset.store
 *          is true.
 *)
function rd_kafka_offsets_store(rk: Prd_kafka_t; offsets: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_offsets_store';

(**
 * @brief Subscribe to topic set using balanced consumer groups.
 *
 * Wildcard (regex) topics are supported:
 * any topic name in the \p topics list that is prefixed with \c \"^\" will
 * be regex-matched to the full list of topics in the cluster and matching
 * topics will be added to the subscription list.
 *
 * The full topic list is retrieved every \c topic.metadata.refresh.interval.ms
 * to pick up new or delete topics that match the subscription.
 * If there is any change to the matched topics the consumer will
 * immediately rejoin the group with the updated set of subscribed topics.
 *
 * Regex and full topic names can be mixed in \p topics.
 *
 * @remark Only the \c .topic field is used in the supplied \p topics list,
 *         all other fields are ignored.
 *
 * @remark subscribe() is an asynchronous method which returns immediately:
 *         background threads will (re)join the group, wait for group rebalance,
 *         issue any registered rebalance_cb, assign() the assigned partitions,
 *         and then start fetching messages. This cycle may take up to
 *         \c session.timeout.ms * 2 or more to complete.
 *
 * @remark A consumer error will be raised for each unavailable topic in the
 *         \p topics. The error will be RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART
 *         for non-existent topics, and
 *         RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED for unauthorized topics.
 *         The consumer error will be raised through rd_kafka_consumer_poll()
 *         (et.al.) with the \c rd_kafka_message_t.err field set to one of the
 *         error codes mentioned above.
 *         The subscribe function itself is asynchronous and will not return
 *         an error on unavailable topics.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if list is empty, contains invalid
 *          topics or regexes,
 *          RD_KAFKA_RESP_ERR__FATAL if the consumer has raised a fatal error.
 *)
function rd_kafka_subscribe(rk: Prd_kafka_t; const topics: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_subscribe';

(**
 * @brief Unsubscribe from the current subscription set.
 *)
function rd_kafka_unsubscribe(rk: Prd_kafka_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_unsubscribe';

(**
 * @brief Returns the current topic subscription
 *
 * @returns An error code on failure, otherwise \p topic is updated
 *          to point to a newly allocated topic list (possibly empty).
 *
 * @remark The application is responsible for calling
 *         rd_kafka_topic_partition_list_destroy on the returned list.
 *)
function rd_kafka_subscription(rk: Prd_kafka_t; topics: PPrd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_subscription';

(**
 * @brief Poll the consumer for messages or events.
 *
 * Will block for at most \p timeout_ms milliseconds.
 *
 * @remark  An application should make sure to call consumer_poll() at regular
 *          intervals, even if no messages are expected, to serve any
 *          queued callbacks waiting to be called. This is especially
 *          important when a rebalance_cb has been registered as it needs
 *          to be called and handled properly to synchronize internal
 *          consumer state.
 *
 * @returns A message object which is a proper message if \p ->err is
 *          RD_KAFKA_RESP_ERR_NO_ERROR, or an event or error for any other
 *          value.
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *
 * @remark When subscribing to topics the application must call poll at
 *         least every \c max.poll.interval.ms to remain a member of the
 *         consumer group.
 *
 * Noteworthy errors returned in \c ->err:
 * - RD_KAFKA_RESP_ERR__MAX_POLL_EXCEEDED - application failed to call
 *   poll within `max.poll.interval.ms`.
 *
 * @sa rd_kafka_message_t
 *)
function rd_kafka_consumer_poll(rk: Prd_kafka_t; timeout_ms: Integer): Prd_kafka_message_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_poll';

(**
 * @brief Close down the KafkaConsumer.
 *
 * @remark This call will block until the consumer has revoked its assignment,
 *         calling the \c rebalance_cb if it is configured, committed offsets
 *         to broker, and left the consumer group.
 *         The maximum blocking time is roughly limited to session.timeout.ms.
 *
 * @returns An error code indicating if the consumer close was succesful
 *          or not.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 *
 * @remark The application still needs to call rd_kafka_destroy() after
 *         this call finishes to clean up the underlying handle resources.
 *
 *)
function rd_kafka_consumer_close(rk: Prd_kafka_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_close';

(**
 * @brief Atomic assignment of partitions to consume.
 *
 * The new \p partitions will replace the existing assignment.
 *
 * When used from a rebalance callback the application shall pass the
 * partition list passed to the callback (or a copy of it) (even if the list
 * is empty) rather than NULL to maintain internal join state.

 * A zero-length \p partitions will treat the partitions as a valid,
 * albeit empty, assignment, and maintain internal state, while a \c NULL
 * value for \p partitions will reset and clear the internal state.
 *
 * @returns An error code indicating if the new assignment was applied or not.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 *)
function rd_kafka_assign(rk: Prd_kafka_t; const partitions: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_assign';

(**
 * @brief Returns the current partition assignment
 *
 * @returns An error code on failure, otherwise \p partitions is updated
 *          to point to a newly allocated partition list (possibly empty).
 *
 * @remark The application is responsible for calling
 *         rd_kafka_topic_partition_list_destroy on the returned list.
 *)
function rd_kafka_assignment(rk: Prd_kafka_t; partitions: PPrd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_assignment';

(**
 * @brief Commit offsets on broker for the provided list of partitions.
 *
 * \p offsets should contain \c topic, \c partition, \c offset and possibly
 * \c metadata. The \c offset should be the offset where consumption will
 * resume, i.e., the last processed offset + 1.
 * If \p offsets is NULL the current partition assignment will be used instead.
 *
 * If \p async is false this operation will block until the broker offset commit
 * is done, returning the resulting success or error code.
 *
 * If a rd_kafka_conf_set_offset_commit_cb() offset commit callback has been
 * configured the callback will be enqueued for a future call to
 * rd_kafka_poll(), rd_kafka_consumer_poll() or similar.
 *
 * @returns An error code indiciating if the commit was successful,
 *          or successfully scheduled if asynchronous, or failed.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 *)
function rd_kafka_commit(rk: Prd_kafka_t; const offsets: Prd_kafka_topic_partition_list_t; async: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_commit';

(**
 * @brief Commit message's offset on broker for the message's partition.
 *        The committed offset is the message's offset + 1.
 *
 * @sa rd_kafka_commit
 *)
function rd_kafka_commit_message(rk: Prd_kafka_t; const rkmessage: Prd_kafka_message_t; async: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_commit_message';

type
  rd_kafka_commit_queue_cb = procedure(rk: Prd_kafka_t; err: rd_kafka_resp_err_t; offsets: Prd_kafka_topic_partition_list_t; commit_opaque: Pointer); cdecl;

(**
 * @brief Commit offsets on broker for the provided list of partitions.
 *
 * See rd_kafka_commit for \p offsets semantics.
 *
 * The result of the offset commit will be posted on the provided \p rkqu queue.
 *
 * If the application uses one of the poll APIs (rd_kafka_poll(),
 * rd_kafka_consumer_poll(), rd_kafka_queue_poll(), ..) to serve the queue
 * the \p cb callback is required.
 *
 * The \p commit_opaque argument is passed to the callback as \p commit_opaque,
 * or if using the event API the callback is ignored and the offset commit
 * result will be returned as an RD_KAFKA_EVENT_COMMIT event and the
 * \p commit_opaque value will be available with rd_kafka_event_opaque().
 *
 * If \p rkqu is NULL a temporary queue will be created and the callback will
 * be served by this call.
 *
 * @sa rd_kafka_commit()
 * @sa rd_kafka_conf_set_offset_commit_cb()
 *)
function rd_kafka_commit_queue(rk: Prd_kafka_t; const offsets: Prd_kafka_topic_partition_list_t; rkqu: Prd_kafka_queue_t; cb: rd_kafka_commit_queue_cb; commit_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_commit_queue';

(**
 * @brief Retrieve committed offsets for topics+partitions.
 *
 * The \p offset field of each requested partition will either be set to
 * stored offset or to RD_KAFKA_OFFSET_INVALID in case there was no stored
 * offset for that partition.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success in which case the
 *          \p offset or \p err field of each \p partitions' element is filled
 *          in with the stored offset, or a partition specific error.
 *          Else returns an error code.
 *)
function rd_kafka_committed(rk: Prd_kafka_t; partitions: Prd_kafka_topic_partition_list_t; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_committed';

(**
 * @brief Retrieve current positions (offsets) for topics+partitions.
 *
 * The \p offset field of each requested partition will be set to the offset
 * of the last consumed message + 1, or RD_KAFKA_OFFSET_INVALID in case there was
 * no previous message.
 *
 * @remark  In this context the last consumed message is the offset consumed
 *          by the current librdkafka instance and, in case of rebalancing, not
 *          necessarily the last message fetched from the partition.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success in which case the
 *          \p offset or \p err field of each \p partitions' element is filled
 *          in with the stored offset, or a partition specific error.
 *          Else returns an error code.
 *)
function rd_kafka_position(rk: Prd_kafka_t; partitions: Prd_kafka_topic_partition_list_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_position';

(**
 * @returns the current consumer group metadata associated with this consumer,
 *          or NULL if \p rk is not a consumer configured with a \c group.id.
 *          This metadata object should be passed to the transactional
 *          producer's rd_kafka_send_offsets_to_transaction() API.
 *
 * @remark The returned pointer must be freed by the application using
 *         rd_kafka_consumer_group_metadata_destroy().
 *
 * @sa rd_kafka_send_offsets_to_transaction()
 *)
function rd_kafka_consumer_group_metadata(rk: Prd_kafka_t): Prd_kafka_consumer_group_metadata_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_group_metadata';

(**
 * @brief Create a new consumer group metadata object.
 *        This is typically only used for writing tests.
 *
 * @remark The returned pointer must be freed by the application using
 *         rd_kafka_consumer_group_metadata_destroy().
 *)
function rd_kafka_consumer_group_metadata_new(const group_id: PUTF8Char): Prd_kafka_consumer_group_metadata_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_group_metadata_new';

(**
 * @brief Frees the consumer group metadata object as returned by
 *        rd_kafka_consumer_group_metadata().
 *)
procedure rd_kafka_consumer_group_metadata_destroy(p1: Prd_kafka_consumer_group_metadata_t); cdecl;
  external Lib name _PU + 'rd_kafka_consumer_group_metadata_destroy';

(**
 * @brief Serialize the consumer group metadata to a binary format.
 *        This is mainly for client binding use and not for application use.
 *
 * @remark The serialized metadata format is private and is not compatible
 *         across different versions or even builds of librdkafka.
 *         It should only be used in the same process runtime and must only
 *         be passed to rd_kafka_consumer_group_metadata_read().
 *
 * @param cgmd Metadata to be serialized.
 * @param bufferp On success this pointer will be updated to point to na
 *                allocated buffer containing the serialized metadata.
 *                The buffer must be freed with rd_kafka_mem_free().
 * @param sizep The pointed to size will be updated with the size of
 *              the serialized buffer.
 *
 * @returns NULL on success or an error object on failure.
 *
 * @sa rd_kafka_consumer_group_metadata_read()
 *)
function rd_kafka_consumer_group_metadata_write(const cgmd: Prd_kafka_consumer_group_metadata_t; bufferp: PPointer; sizep: PNativeUInt): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_group_metadata_write';

(**
 * @brief Reads serialized consumer group metadata and returns a
 *        consumer group metadata object.
 *        This is mainly for client binding use and not for application use.
 *
 * @remark The serialized metadata format is private and is not compatible
 *         across different versions or even builds of librdkafka.
 *         It should only be used in the same process runtime and must only
 *         be passed to rd_kafka_consumer_group_metadata_read().
 *
 * @param cgmdp On success this pointer will be updated to point to a new
 *              consumer group metadata object which must be freed with
 *              rd_kafka_consumer_group_metadata_destroy().
 * @param buffer Pointer to the serialized data.
 * @param size Size of the serialized data.
 *
 * @returns NULL on success or an error object on failure.
 *
 * @sa rd_kafka_consumer_group_metadata_write()
 *)
function rd_kafka_consumer_group_metadata_read(cgmdp: PPrd_kafka_consumer_group_metadata_t; const buffer: Pointer; size: NativeUInt): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_consumer_group_metadata_read';

(**
 * @brief Produce and send a single message to broker.
 *
 * \p rkt is the target topic which must have been previously created with
 * `rd_kafka_topic_new()`.
 *
 * `rd_kafka_produce()` is an asynch non-blocking API.
 * See `rd_kafka_conf_set_dr_msg_cb` on how to setup a callback to be called
 * once the delivery status (success or failure) is known. The delivery report
 * is trigged by the application calling `rd_kafka_poll()` (at regular
 * intervals) or `rd_kafka_flush()` (at termination).
 *
 * Since producing is asynchronous, you should call `rd_kafka_flush()` before
 * you destroy the producer. Otherwise, any outstanding messages will be
 * silently discarded.
 *
 * When temporary errors occur, librdkafka automatically retries to produce the
 * messages. Retries are triggered after retry.backoff.ms and when the
 * leader broker for the given partition is available. Otherwise, librdkafka
 * falls back to polling the topic metadata to monitor when a new leader is
 * elected (see the topic.metadata.refresh.fast.interval.ms and
 * topic.metadata.refresh.interval.ms configurations) and then performs a
 * retry. A delivery error will occur if the message could not be produced
 * within message.timeout.ms.
 *
 * See the "Message reliability" chapter in INTRODUCTION.md for more
 * information.
 *
 * \p partition is the target partition, either:
 *   - RD_KAFKA_PARTITION_UA (unassigned) for
 *     automatic partitioning using the topic's partitioner function, or
 *   - a fixed partition (0..N)
 *
 * \p msgflags is zero or more of the following flags OR:ed together:
 *    RD_KAFKA_MSG_F_BLOCK - block \p produce*() call if
 *                           \p queue.buffering.max.messages or
 *                           \p queue.buffering.max.kbytes are exceeded.
 *                           Messages are considered in-queue from the point they
 *                           are accepted by produce() until their corresponding
 *                           delivery report callback/event returns.
 *                           It is thus a requirement to call
 *                           rd_kafka_poll() (or equiv.) from a separate
 *                           thread when F_BLOCK is used.
 *                           See WARNING on \c RD_KAFKA_MSG_F_BLOCK above.
 *
 *    RD_KAFKA_MSG_F_FREE - rdkafka will free(3) \p payload when it is done
 *                          with it.
 *    RD_KAFKA_MSG_F_COPY - the \p payload data will be copied and the
 *                          \p payload pointer will not be used by rdkafka
 *                          after the call returns.
 *    RD_KAFKA_MSG_F_PARTITION - produce_batch() will honour per-message
 *                               partition, either set manually or by the
 *                               configured partitioner.
 *
 *    .._F_FREE and .._F_COPY are mutually exclusive. If neither of these are
 *    set, the caller must ensure that the memory backing \p payload remains
 *    valid and is not modified or reused until the delivery callback is
 *    invoked. Other buffers passed to `rd_kafka_produce()` don't have this
 *    restriction on reuse, i.e. the memory backing the key or the topic name
 *    may be reused as soon as `rd_kafka_produce()` returns.
 *
 *    If the function returns -1 and RD_KAFKA_MSG_F_FREE was specified, then
 *    the memory associated with the payload is still the caller's
 *    responsibility.
 *
 * \p payload is the message payload of size \p len bytes.
 *
 * \p key is an optional message key of size \p keylen bytes, if non-NULL it
 * will be passed to the topic partitioner as well as be sent with the
 * message to the broker and passed on to the consumer.
 *
 * \p msg_opaque is an optional application-provided per-message opaque
 * pointer that will provided in the message's delivery report callback
 * (\c dr_msg_cb or \c dr_cb) and the \c rd_kafka_message_t \c _private field.
 *
 * @remark on_send() and on_acknowledgement() interceptors may be called
 *         from this function. on_acknowledgement() will only be called if the
 *         message fails partitioning.
 *
 * @remark If the producer is transactional (\c transactional.id is configured)
 *         producing is only allowed during an on-going transaction, namely
 *         after rd_kafka_begin_transaction() has been called.
 *
 * @returns 0 on success or -1 on error in which case errno is set accordingly:
 *  - ENOBUFS  - maximum number of outstanding messages has been reached:
 *               "queue.buffering.max.messages"
 *               (RD_KAFKA_RESP_ERR__QUEUE_FULL)
 *  - EMSGSIZE - message is larger than configured max size:
 *               "messages.max.bytes".
 *               (RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE)
 *  - ESRCH    - requested \p partition is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
 *  - ENOENT   - topic is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
 *  - ECANCELED - fatal error has been raised on producer, see
 *                rd_kafka_fatal_error(),
 *               (RD_KAFKA_RESP_ERR__FATAL).
 *  - ENOEXEC  - transactional state forbids producing
 *               (RD_KAFKA_RESP_ERR__STATE)
 *
 * @sa Use rd_kafka_errno2err() to convert `errno` to rdkafka error code.
 *)
function rd_kafka_produce(rkt: Prd_kafka_topic_t; partition: Int32; msgflags: Integer; payload: Pointer; len: NativeUInt; const key: Pointer; keylen: NativeUInt; msg_opaque: Pointer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_produce';

(**
 * @brief Produce and send a single message to broker.
 *
 * The message is defined by a va-arg list using \c rd_kafka_vtype_t
 * tag tuples which must be terminated with a single \c RD_KAFKA_V_END.
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, else an error code as
 *          described in rd_kafka_produce().
 *          \c RD_KAFKA_RESP_ERR__CONFLICT is returned if _V_HEADER and
 *          _V_HEADERS are mixed.
 *
 * @sa rd_kafka_produce, rd_kafka_produceva, RD_KAFKA_V_END
 *)
function rd_kafka_producev(rk: Prd_kafka_t): rd_kafka_resp_err_t varargs; cdecl;
  external Lib name _PU + 'rd_kafka_producev';

(**
 * @brief Produce and send a single message to broker.
 *
 * The message is defined by an array of \c rd_kafka_vu_t of
 * count \p cnt.
 *
 * @returns an error object on failure or NULL on success.
 *          See rd_kafka_producev() for specific error codes.
 *
 * @sa rd_kafka_produce, rd_kafka_producev, RD_KAFKA_V_END
 *)
function rd_kafka_produceva(rk: Prd_kafka_t; const vus: Prd_kafka_vu_t; cnt: NativeUInt): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_produceva';

(**
 * @brief Produce multiple messages.
 *
 * If partition is RD_KAFKA_PARTITION_UA the configured partitioner will
 * be run for each message (slower), otherwise the messages will be enqueued
 * to the specified partition directly (faster).
 *
 * The messages are provided in the array \p rkmessages of count \p message_cnt
 * elements.
 * The \p partition and \p msgflags are used for all provided messages.
 *
 * Honoured \p rkmessages[] fields are:
 *  - payload,len    Message payload and length
 *  - key,key_len    Optional message key
 *  - _private       Message opaque pointer (msg_opaque)
 *  - err            Will be set according to success or failure, see
 *                   rd_kafka_produce() for possible error codes.
 *                   Application only needs to check for errors if
 *                   return value != \p message_cnt.
 *
 * @remark If \c RD_KAFKA_MSG_F_PARTITION is set in \p msgflags, the
 *         \c .partition field of the \p rkmessages is used instead of
 *         \p partition.
 *
 * @returns the number of messages succesfully enqueued for producing.
 *
 * @remark This interface does NOT support setting message headers on
 *         the provided \p rkmessages.
 *)
function rd_kafka_produce_batch(rkt: Prd_kafka_topic_t; partition: Int32; msgflags: Integer; rkmessages: Prd_kafka_message_t; message_cnt: Integer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_produce_batch';

(**
 * @brief Wait until all outstanding produce requests, et.al, are completed.
 *        This should typically be done prior to destroying a producer instance
 *        to make sure all queued and in-flight produce requests are completed
 *        before terminating.
 *
 * @remark This function will call rd_kafka_poll() and thus trigger callbacks.
 *
 * @remark If RD_KAFKA_EVENT_DR has been enabled
 *         (through rd_kafka_conf_set_events()) this function will not call
 *         rd_kafka_poll() but instead wait for the librdkafka-handled
 *         message count to reach zero. This requires the application to
 *         serve the event queue in a separate thread.
 *         In this mode only messages are counted, not other types of
 *         queued events.
 *
 * @returns RD_KAFKA_RESP_ERR__TIMED_OUT if \p timeout_ms was reached before all
 *          outstanding requests were completed, else RD_KAFKA_RESP_ERR_NO_ERROR
 *
 * @sa rd_kafka_outq_len()
 *)
function rd_kafka_flush(rk: Prd_kafka_t; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_flush';

(**
 * @brief Purge messages currently handled by the producer instance.
 *
 * @param rk          Client instance.
 * @param purge_flags Tells which messages to purge and how.
 *
 * The application will need to call rd_kafka_poll() or rd_kafka_flush()
 * afterwards to serve the delivery report callbacks of the purged messages.
 *
 * Messages purged from internal queues fail with the delivery report
 * error code set to RD_KAFKA_RESP_ERR__PURGE_QUEUE, while purged messages that
 * are in-flight to or from the broker will fail with the error code set to
 * RD_KAFKA_RESP_ERR__PURGE_INFLIGHT.
 *
 * @warning Purging messages that are in-flight to or from the broker
 *          will ignore any sub-sequent acknowledgement for these messages
 *          received from the broker, effectively making it impossible
 *          for the application to know if the messages were successfully
 *          produced or not. This may result in duplicate messages if the
 *          application retries these messages at a later time.
 *
 * @remark This call may block for a short time while background thread
 *         queues are purged.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if the \p purge flags are invalid
 *          or unknown,
 *          RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if called on a non-producer
 *          client instance.
 *)
function rd_kafka_purge(rk: Prd_kafka_t; purge_flags: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_purge';

(**
 * @brief Request Metadata from broker.
 *
 * Parameters:
 *  - \p all_topics  if non-zero: request info about all topics in cluster,
 *                   if zero: only request info about locally known topics.
 *  - \p only_rkt    only request info about this topic
 *  - \p metadatap   pointer to hold metadata result.
 *                   The \p *metadatap pointer must be released
 *                   with rd_kafka_metadata_destroy().
 *  - \p timeout_ms  maximum response time before failing.
 *
 * @remark Consumer: If \p all_topics is non-zero the Metadata response
 *         information may trigger a re-join if any subscribed topics
 *         have changed partition count or existence state.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success (in which case *metadatap)
 *          will be set, else RD_KAFKA_RESP_ERR__TIMED_OUT on timeout or
 *          other error code on error.
 *)
function rd_kafka_metadata_(rk: Prd_kafka_t; all_topics: Integer; only_rkt: Prd_kafka_topic_t; metadatap: PPrd_kafka_metadata; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_metadata';

(**
 * @brief Release metadata memory.
 *)
procedure rd_kafka_metadata_destroy(const metadata: Prd_kafka_metadata); cdecl;
  external Lib name _PU + 'rd_kafka_metadata_destroy';

(**
 * @brief List and describe client groups in cluster.
 *
 * \p group is an optional group name to describe, otherwise (\p NULL) all
 * groups are returned.
 *
 * \p timeout_ms is the (approximate) maximum time to wait for response
 * from brokers and must be a positive value.
 *
 * @returns \c RD_KAFKA_RESP_ERR__NO_ERROR on success and \p grplistp is
 *           updated to point to a newly allocated list of groups.
 *           \c RD_KAFKA_RESP_ERR__PARTIAL if not all brokers responded
 *           in time but at least one group is returned in  \p grplistlp.
 *           \c RD_KAFKA_RESP_ERR__TIMED_OUT if no groups were returned in the
 *           given timeframe but not all brokers have yet responded, or
 *           if the list of brokers in the cluster could not be obtained within
 *           the given timeframe.
 *           \c RD_KAFKA_RESP_ERR__TRANSPORT if no brokers were found.
 *           Other error codes may also be returned from the request layer.
 *
 *           The \p grplistp remains untouched if any error code is returned,
 *           with the exception of RD_KAFKA_RESP_ERR__PARTIAL which behaves
 *           as RD_KAFKA_RESP_ERR__NO_ERROR (success) but with an incomplete
 *           group list.
 *
 * @sa Use rd_kafka_group_list_destroy() to release list memory.
 *)
function rd_kafka_list_groups(rk: Prd_kafka_t; const group: PUTF8Char; grplistp: PPrd_kafka_group_list; timeout_ms: Integer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_list_groups';

(**
 * @brief Release list memory
 *)
procedure rd_kafka_group_list_destroy(const grplist: Prd_kafka_group_list); cdecl;
  external Lib name _PU + 'rd_kafka_group_list_destroy';

(**
 * @brief Adds one or more brokers to the kafka handle's list of initial
 *        bootstrap brokers.
 *
 * Additional brokers will be discovered automatically as soon as rdkafka
 * connects to a broker by querying the broker metadata.
 *
 * If a broker name resolves to multiple addresses (and possibly
 * address families) all will be used for connection attempts in
 * round-robin fashion.
 *
 * \p brokerlist is a ,-separated list of brokers in the format:
 *   \c \<broker1\>,\<broker2\>,..
 * Where each broker is in either the host or URL based format:
 *   \c \<host\>[:\<port\>]
 *   \c \<proto\>://\<host\>[:port]
 * \c \<proto\> is either \c PLAINTEXT, \c SSL, \c SASL, \c SASL_PLAINTEXT
 * The two formats can be mixed but ultimately the value of the
 * `security.protocol` config property decides what brokers are allowed.
 *
 * Example:
 *    brokerlist = "broker1:10000,broker2"
 *    brokerlist = "SSL://broker3:9000,ssl://broker2"
 *
 * @returns the number of brokers successfully added.
 *
 * @remark Brokers may also be defined with the \c metadata.broker.list or
 *         \c bootstrap.servers configuration property (preferred method).
 *)
function rd_kafka_brokers_add(rk: Prd_kafka_t; const brokerlist: PUTF8Char): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_brokers_add';

type
  rd_kafka_set_logger_func = procedure(const rk: Prd_kafka_t; level: Integer; const fac: PUTF8Char; const buf: PUTF8Char); cdecl;

(**
 * @brief Set logger function.
 *
 * The default is to print to stderr, but a syslog logger is also available,
 * see rd_kafka_log_(print|syslog) for the builtin alternatives.
 * Alternatively the application may provide its own logger callback.
 * Or pass 'func' as NULL to disable logging.
 *
 * @deprecated Use rd_kafka_conf_set_log_cb()
 *
 * @remark \p rk may be passed as NULL in the callback.
 *)
procedure rd_kafka_set_logger(rk: Prd_kafka_t; func: rd_kafka_set_logger_func); cdecl;
  external Lib name _PU + 'rd_kafka_set_logger';

(**
 * @brief Specifies the maximum logging level emitted by
 *        internal kafka logging and debugging.
 *
 * @deprecated Set the \c "log_level" configuration property instead.
 *
 * @remark If the \p \"debug\" configuration property is set the log level is
 *         automatically adjusted to \c LOG_DEBUG (7).
 *)
procedure rd_kafka_set_log_level(rk: Prd_kafka_t; level: Integer); cdecl;
  external Lib name _PU + 'rd_kafka_set_log_level';

(**
 * @brief Builtin (default) log sink: print to stderr
 *)
procedure rd_kafka_log_print(const rk: Prd_kafka_t; level: Integer; const fac: PUTF8Char; const buf: PUTF8Char); cdecl;
  external Lib name _PU + 'rd_kafka_log_print';

(**
 * @brief Builtin log sink: print to syslog.
 * @remark This logger is only available if librdkafka was built
 *         with syslog support.
 *)
procedure rd_kafka_log_syslog(const rk: Prd_kafka_t; level: Integer; const fac: PUTF8Char; const buf: PUTF8Char); cdecl;
  external Lib name _PU + 'rd_kafka_log_syslog';

(**
 * @brief Returns the current out queue length.
 *
 * The out queue length is the sum of:
 *  - number of messages waiting to be sent to, or acknowledged by,
 *    the broker.
 *  - number of delivery reports (e.g., dr_msg_cb) waiting to be served
 *    by rd_kafka_poll() or rd_kafka_flush().
 *  - number of callbacks (e.g., error_cb, stats_cb, etc) waiting to be
 *    served by rd_kafka_poll(), rd_kafka_consumer_poll() or rd_kafka_flush().
 *  - number of events waiting to be served by background_event_cb() in
 *    the background queue (see rd_kafka_conf_set_background_event_cb).
 *
 * An application should wait for the return value of this function to reach
 * zero before terminating to make sure outstanding messages,
 * requests (such as offset commits), callbacks and events are fully processed.
 * See rd_kafka_flush().
 *
 * @returns number of messages and events waiting in queues.
 *
 * @sa rd_kafka_flush()
 *)
function rd_kafka_outq_len(rk: Prd_kafka_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_outq_len';

(**
 * @brief Dumps rdkafka's internal state for handle \p rk to stream \p fp
 *
 * This is only useful for debugging rdkafka, showing state and statistics
 * for brokers, topics, partitions, etc.
 *)
procedure rd_kafka_dump(fp: PPointer; rk: Prd_kafka_t); cdecl;
  external Lib name _PU + 'rd_kafka_dump';

(**
 * @brief Retrieve the current number of threads in use by librdkafka.
 *
 * Used by regression tests.
 *)
function rd_kafka_thread_cnt(): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_thread_cnt';

(**
 * @brief Wait for all rd_kafka_t objects to be destroyed.
 *
 * Returns 0 if all kafka objects are now destroyed, or -1 if the
 * timeout was reached.
 *
 * @remark This function is deprecated.
 *)
function rd_kafka_wait_destroyed(timeout_ms: Integer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_wait_destroyed';

(**
 * @brief Run librdkafka's built-in unit-tests.
 *
 * @returns the number of failures, or 0 if all tests passed.
 *)
function rd_kafka_unittest(): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_unittest';

(**
 * @brief Redirect the main (rd_kafka_poll()) queue to the KafkaConsumer's
 *        queue (rd_kafka_consumer_poll()).
 *
 * @warning It is not permitted to call rd_kafka_poll() after directing the
 *          main queue with rd_kafka_poll_set_consumer().
 *)
function rd_kafka_poll_set_consumer(rk: Prd_kafka_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_poll_set_consumer';

(**
 * @returns the event type for the given event.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         RD_KAFKA_EVENT_NONE is returned.
 *)
function rd_kafka_event_type(const rkev: Prd_kafka_event_t): rd_kafka_event_type_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_type';

(**
 * @returns the event type's name for the given event.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         the name for RD_KAFKA_EVENT_NONE is returned.
 *)
function rd_kafka_event_name(const rkev: Prd_kafka_event_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_event_name';

(**
 * @brief Destroy an event.
 *
 * @remark Any references to this event, such as extracted messages,
 *         will not be usable after this call.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         no action is performed.
 *)
procedure rd_kafka_event_destroy(rkev: Prd_kafka_event_t); cdecl;
  external Lib name _PU + 'rd_kafka_event_destroy';

(**
 * @returns the next message from an event.
 *
 * Call repeatedly until it returns NULL.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 *
 * @remark The returned message(s) MUST NOT be
 *         freed with rd_kafka_message_destroy().
 *
 * @remark on_consume() interceptor may be called
 *         from this function prior to passing message to application.
 *)
function rd_kafka_event_message_next(rkev: Prd_kafka_event_t): Prd_kafka_message_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_message_next';

(**
 * @brief Extacts \p size message(s) from the event into the
 *        pre-allocated array \p rkmessages.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 *
 * @returns the number of messages extracted.
 *
 * @remark on_consume() interceptor may be called
 *         from this function prior to passing message to application.
 *)
function rd_kafka_event_message_array(rkev: Prd_kafka_event_t; rkmessages: PPrd_kafka_message_t; size: NativeUInt): NativeUInt; cdecl;
  external Lib name _PU + 'rd_kafka_event_message_array';

(**
 * @returns the number of remaining messages in the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 *)
function rd_kafka_event_message_count(rkev: Prd_kafka_event_t): NativeUInt; cdecl;
  external Lib name _PU + 'rd_kafka_event_message_count';

(**
 * @returns the associated configuration string for the event, or NULL
 *          if the configuration property is not set or if
 *          not applicable for the given event type.
 *
 * The returned memory is read-only and its lifetime is the same as the
 * event object.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH: value of sasl.oauthbearer.config
 *)
function rd_kafka_event_config_string(rkev: Prd_kafka_event_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_event_config_string';

(**
 * @returns the error code for the event.
 *
 * Use rd_kafka_event_error_is_fatal() to detect if this is a fatal error.
 *
 * Event types:
 *  - all
 *)
function rd_kafka_event_error(rkev: Prd_kafka_event_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_error';

(**
 * @returns the error string (if any).
 *          An application should check that rd_kafka_event_error() returns
 *          non-zero before calling this function.
 *
 * Event types:
 *  - all
 *)
function rd_kafka_event_error_string(rkev: Prd_kafka_event_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_event_error_string';

(**
 * @returns 1 if the error is a fatal error, else 0.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_ERROR_
 *
 * @sa rd_kafka_fatal_error()
 *)
function rd_kafka_event_error_is_fatal(rkev: Prd_kafka_event_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_event_error_is_fatal';

(**
 * @returns the event opaque (if any) as passed to rd_kafka_commit() (et.al) or
 *          rd_kafka_AdminOptions_set_opaque(), depending on event type.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_OFFSET_COMMIT
 *  - RD_KAFKA_EVENT_CREATETOPICS_RESULT
 *  - RD_KAFKA_EVENT_DELETETOPICS_RESULT
 *  - RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 *  - RD_KAFKA_EVENT_ALTERCONFIGS_RESULT
 *  - RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT
 *)
function rd_kafka_event_opaque(rkev: Prd_kafka_event_t): Pointer; cdecl;
  external Lib name _PU + 'rd_kafka_event_opaque';

(**
 * @brief Extract log message from the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_LOG
 *
 * @returns 0 on success or -1 if unsupported event type.
 *)
function rd_kafka_event_log_(rkev: Prd_kafka_event_t; fac: PPUTF8Char; str: PPUTF8Char; level: PInteger): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_event_log';

(**
 * @brief Extract log debug context from event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_LOG
 *
 *  @param rkev the event to extract data from.
 *  @param dst destination string for comma separated list.
 *  @param dstsize size of provided dst buffer.
 *  @returns 0 on success or -1 if unsupported event type.
 *)
function rd_kafka_event_debug_contexts(rkev: Prd_kafka_event_t; dst: PUTF8Char; dstsize: NativeUInt): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_event_debug_contexts';

(**
 * @brief Extract stats from the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_STATS
 *
 * @returns stats json string.
 *
 * @remark the returned string will be freed automatically along with the event object
 *
 *)
function rd_kafka_event_stats_(rkev: Prd_kafka_event_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_event_stats';

(**
 * @returns the topic partition list from the event.
 *
 * @remark The list MUST NOT be freed with rd_kafka_topic_partition_list_destroy()
 *
 * Event types:
 *  - RD_KAFKA_EVENT_REBALANCE
 *  - RD_KAFKA_EVENT_OFFSET_COMMIT
 *)
function rd_kafka_event_topic_partition_list(rkev: Prd_kafka_event_t): Prd_kafka_topic_partition_list_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_topic_partition_list';

(**
 * @returns a newly allocated topic_partition container, if applicable for the event type,
 *          else NULL.
 *
 * @remark The returned pointer MUST be freed with rd_kafka_topic_partition_destroy().
 *
 * Event types:
 *   RD_KAFKA_EVENT_ERROR_  (for partition level errors)
 *)
function rd_kafka_event_topic_partition(rkev: Prd_kafka_event_t): Prd_kafka_topic_partition_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_topic_partition';

(**
 * @brief Get CreateTopics result.
 *
 * @returns the result of a CreateTopics request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_CREATETOPICS_RESULT
 *)
function rd_kafka_event_CreateTopics_result_(rkev: Prd_kafka_event_t): Prd_kafka_CreateTopics_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_CreateTopics_result';

(**
 * @brief Get DeleteTopics result.
 *
 * @returns the result of a DeleteTopics request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DELETETOPICS_RESULT
 *)
function rd_kafka_event_DeleteTopics_result_(rkev: Prd_kafka_event_t): Prd_kafka_DeleteTopics_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_DeleteTopics_result';

(**
 * @brief Get CreatePartitions result.
 *
 * @returns the result of a CreatePartitions request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 *)
function rd_kafka_event_CreatePartitions_result_(rkev: Prd_kafka_event_t): Prd_kafka_CreatePartitions_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_CreatePartitions_result';

(**
 * @brief Get AlterConfigs result.
 *
 * @returns the result of a AlterConfigs request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_ALTERCONFIGS_RESULT
 *)
function rd_kafka_event_AlterConfigs_result_(rkev: Prd_kafka_event_t): Prd_kafka_AlterConfigs_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_AlterConfigs_result';

(**
 * @brief Get DescribeConfigs result.
 *
 * @returns the result of a DescribeConfigs request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT
 *)
function rd_kafka_event_DescribeConfigs_result_(rkev: Prd_kafka_event_t): Prd_kafka_DescribeConfigs_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_event_DescribeConfigs_result';

(**
 * @brief Poll a queue for an event for max \p timeout_ms.
 *
 * @returns an event, or NULL.
 *
 * @remark Use rd_kafka_event_destroy() to free the event.
 *
 * @sa rd_kafka_conf_set_background_event_cb()
 *)
function rd_kafka_queue_poll(rkqu: Prd_kafka_queue_t; timeout_ms: Integer): Prd_kafka_event_t; cdecl;
  external Lib name _PU + 'rd_kafka_queue_poll';

(**
 * @brief Poll a queue for events served through callbacks for max \p timeout_ms.
 *
 * @returns the number of events served.
 *
 * @remark This API must only be used for queues with callbacks registered
 *         for all expected event types. E.g., not a message queue.
 *
 * @remark Also see rd_kafka_conf_set_background_event_cb() for triggering
 *         event callbacks from a librdkafka-managed background thread.
 *
 * @sa rd_kafka_conf_set_background_event_cb()
 *)
function rd_kafka_queue_poll_callback(rkqu: Prd_kafka_queue_t; timeout_ms: Integer): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_queue_poll_callback';

(**
 * @brief Append an on_conf_set() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_set Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_conf_interceptor_add_on_conf_set(conf: Prd_kafka_conf_t; const ic_name: PUTF8Char; on_conf_set: Prd_kafka_interceptor_f_on_conf_set_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_interceptor_add_on_conf_set';

(**
 * @brief Append an on_conf_dup() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_dup Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_conf_interceptor_add_on_conf_dup(conf: Prd_kafka_conf_t; const ic_name: PUTF8Char; on_conf_dup: Prd_kafka_interceptor_f_on_conf_dup_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_interceptor_add_on_conf_dup';

(**
 * @brief Append an on_conf_destroy() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_destroy Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 *
 * @remark Multiple on_conf_destroy() interceptors are allowed to be added
 *         to the same configuration object.
 *)
function rd_kafka_conf_interceptor_add_on_conf_destroy(conf: Prd_kafka_conf_t; const ic_name: PUTF8Char; on_conf_destroy: Prd_kafka_interceptor_f_on_conf_destroy_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_interceptor_add_on_conf_destroy';

(**
 * @brief Append an on_new() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_new Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @remark Since the on_new() interceptor is added to the configuration object
 *         it may be copied by rd_kafka_conf_dup().
 *         An interceptor implementation must thus be able to handle
 *         the same interceptor,ic_opaque tuple to be used by multiple
 *         client instances.
 *
 * @remark An interceptor plugin should check the return value to make sure it
 *         has not already been added.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_conf_interceptor_add_on_new(conf: Prd_kafka_conf_t; const ic_name: PUTF8Char; on_new: Prd_kafka_interceptor_f_on_new_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_conf_interceptor_add_on_new';

(**
 * @brief Append an on_destroy() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_destroy Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_destroy(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_destroy: Prd_kafka_interceptor_f_on_destroy_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_destroy';

(**
 * @brief Append an on_send() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_send Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_send(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_send: Prd_kafka_interceptor_f_on_send_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_send';

(**
 * @brief Append an on_acknowledgement() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_acknowledgement Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_acknowledgement(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_acknowledgement: Prd_kafka_interceptor_f_on_acknowledgement_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_acknowledgement';

(**
 * @brief Append an on_consume() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_consume Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_consume(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_consume: Prd_kafka_interceptor_f_on_consume_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_consume';

(**
 * @brief Append an on_commit() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_commit() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_commit(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_commit: Prd_kafka_interceptor_f_on_commit_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_commit';

(**
 * @brief Append an on_request_sent() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_request_sent() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_request_sent(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_request_sent: Prd_kafka_interceptor_f_on_request_sent_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_request_sent';

(**
 * @brief Append an on_thread_start() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_thread_start() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_thread_start(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_thread_start: Prd_kafka_interceptor_f_on_thread_start_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_thread_start';

(**
 * @brief Append an on_thread_exit() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_thread_exit() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 *)
function rd_kafka_interceptor_add_on_thread_exit(rk: Prd_kafka_t; const ic_name: PUTF8Char; on_thread_exit: Prd_kafka_interceptor_f_on_thread_exit_t; ic_opaque: Pointer): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_interceptor_add_on_thread_exit';

(**
 * @returns the error code for the given topic result.
 *)
function rd_kafka_topic_result_error(const topicres: Prd_kafka_topic_result_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_topic_result_error';

(**
 * @returns the human readable error string for the given topic result,
 *          or NULL if there was no error.
 *
 * @remark lifetime of the returned string is the same as the \p topicres.
 *)
function rd_kafka_topic_result_error_string(const topicres: Prd_kafka_topic_result_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_topic_result_error_string';

(**
 * @returns the name of the topic for the given topic result.
 * @remark lifetime of the returned string is the same as the \p topicres.
 *
 *)
function rd_kafka_topic_result_name(const topicres: Prd_kafka_topic_result_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_topic_result_name';

(**
 * @brief Create a new AdminOptions object.
 *
 *        The options object is not modified by the Admin API request APIs,
 *        (e.g. CreateTopics) and may be reused for multiple calls.
 *
 * @param rk Client instance.
 * @param for_api Specifies what Admin API this AdminOptions object will be used
 *                for, which will enforce what AdminOptions_set_..() calls may
 *                be used based on the API, causing unsupported set..() calls
 *                to fail.
 *                Specifying RD_KAFKA_ADMIN_OP_ANY disables the enforcement
 *                allowing any option to be set, even if the option
 *                is not used in a future call to an Admin API method.
 *
 * @returns a new AdminOptions object (which must be freed with
 *          rd_kafka_AdminOptions_destroy()), or NULL if \p for_api was set to
 *          an unknown API op type.
 *)
function rd_kafka_AdminOptions_new(rk: Prd_kafka_t; for_api: rd_kafka_admin_op_t): Prd_kafka_AdminOptions_t; cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_new';

(**
 * @brief Destroy a AdminOptions object.
 *)
procedure rd_kafka_AdminOptions_destroy(options: Prd_kafka_AdminOptions_t); cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_destroy';

(**
 * @brief Sets the overall request timeout, including broker lookup,
 *        request transmission, operation time on broker, and response.
 *
 * @param options Admin options.
 * @param timeout_ms Timeout in milliseconds, use -1 for indefinite timeout.
 *                   Defaults to `socket.timeout.ms`.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if timeout was out of range in which
 *          case an error string will be written \p errstr.
 *
 * @remark This option is valid for all Admin API requests.
 *)
function rd_kafka_AdminOptions_set_request_timeout(options: Prd_kafka_AdminOptions_t; timeout_ms: Integer; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_set_request_timeout';

(**
 * @brief Sets the broker's operation timeout, such as the timeout for
 *        CreateTopics to complete the creation of topics on the controller
 *        before returning a result to the application.
 *
 * CreateTopics: values <= 0 will return immediately after triggering topic
 * creation, while > 0 will wait this long for topic creation to propagate
 * in cluster. Default: 0.
 *
 * DeleteTopics: same semantics as CreateTopics.
 * CreatePartitions: same semantics as CreateTopics.
 *
 * @param options Admin options.
 * @param timeout_ms Timeout in milliseconds.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if timeout was out of range in which
 *          case an error string will be written \p errstr.
 *
 * @remark This option is valid for CreateTopics, DeleteTopics and
 *         CreatePartitions.
 *)
function rd_kafka_AdminOptions_set_operation_timeout(options: Prd_kafka_AdminOptions_t; timeout_ms: Integer; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_set_operation_timeout';

(**
 * @brief Tell broker to only validate the request, without performing
 *        the requested operation (create topics, etc).
 *
 * @param options Admin options.
 * @param true_or_false Defaults to false.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an
 *          error code on failure in which case an error string will
 *          be written \p errstr.
 *
 * @remark This option is valid for CreateTopics,
 *         CreatePartitions, AlterConfigs.
 *)
function rd_kafka_AdminOptions_set_validate_only(options: Prd_kafka_AdminOptions_t; true_or_false: Integer; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_set_validate_only';

(**
 * @brief Override what broker the Admin request will be sent to.
 *
 * By default, Admin requests are sent to the controller broker, with
 * the following exceptions:
 *   - AlterConfigs with a BROKER resource are sent to the broker id set
 *     as the resource name.
 *   - DescribeConfigs with a BROKER resource are sent to the broker id set
 *     as the resource name.
 *
 * @param options Admin Options.
 * @param broker_id The broker to send the request to.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an
 *          error code on failure in which case an error string will
 *          be written \p errstr.
 *
 * @remark This API should typically not be used, but serves as a workaround
 *         if new resource types are to the broker that the client
 *         does not know where to send.
 *)
function rd_kafka_AdminOptions_set_broker(options: Prd_kafka_AdminOptions_t; broker_id: Int32; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_set_broker';

(**
 * @brief Set application opaque value that can be extracted from the
 *        result event using rd_kafka_event_opaque()
 *)
procedure rd_kafka_AdminOptions_set_opaque(options: Prd_kafka_AdminOptions_t; ev_opaque: Pointer); cdecl;
  external Lib name _PU + 'rd_kafka_AdminOptions_set_opaque';

(**
 * @brief Create a new NewTopic object. This object is later passed to
 *        rd_kafka_CreateTopics().
 *
 * @param topic Topic name to create.
 * @param num_partitions Number of partitions in topic.
 * @param replication_factor Default replication factor for the topic's
 *                           partitions, or -1 if set_replica_assignment()
 *                           will be used.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 *
 * @returns a new allocated NewTopic object, or NULL if the input parameters
 *          are invalid.
 *          Use rd_kafka_NewTopic_destroy() to free object when done.
 *)
function rd_kafka_NewTopic_new(const topic: PUTF8Char; num_partitions: Integer; replication_factor: Integer; errstr: PUTF8Char; errstr_size: NativeUInt): Prd_kafka_NewTopic_t; cdecl;
  external Lib name _PU + 'rd_kafka_NewTopic_new';

(**
 * @brief Destroy and free a NewTopic object previously created with
 *        rd_kafka_NewTopic_new()
 *)
procedure rd_kafka_NewTopic_destroy(new_topic: Prd_kafka_NewTopic_t); cdecl;
  external Lib name _PU + 'rd_kafka_NewTopic_destroy';

(**
 * @brief Helper function to destroy all NewTopic objects in the \p new_topics
 *        array (of \p new_topic_cnt elements).
 *        The array itself is not freed.
 *)
procedure rd_kafka_NewTopic_destroy_array(new_topics: PPrd_kafka_NewTopic_t; new_topic_cnt: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_NewTopic_destroy_array';

(**
 * @brief Set the replica (broker) assignment for \p partition to the
 *        replica set in \p broker_ids (of \p broker_id_cnt elements).
 *
 * @remark When this method is used, rd_kafka_NewTopic_new() must have
 *         been called with a \c replication_factor of -1.
 *
 * @remark An application must either set the replica assignment for
 *         all new partitions, or none.
 *
 * @remark If called, this function must be called consecutively for each
 *         partition, starting at 0.
 *
 * @remark Use rd_kafka_metadata() to retrieve the list of brokers
 *         in the cluster.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 *)
function rd_kafka_NewTopic_set_replica_assignment(new_topic: Prd_kafka_NewTopic_t; partition: Int32; broker_ids: PInt32; broker_id_cnt: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_NewTopic_set_replica_assignment';

(**
 * @brief Set (broker-side) topic configuration name/value pair.
 *
 * @remark The name and value are not validated by the client, the validation
 *         takes place on the broker.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 * @sa http://kafka.apache.org/documentation.html#topicconfigs
 *)
function rd_kafka_NewTopic_set_config(new_topic: Prd_kafka_NewTopic_t; const name: PUTF8Char; const value: PUTF8Char): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_NewTopic_set_config';

(**
 * @brief Create topics in cluster as specified by the \p new_topics
 *        array of size \p new_topic_cnt elements.
 *
 * @param rk Client instance.
 * @param new_topics Array of new topics to create.
 * @param new_topic_cnt Number of elements in \p new_topics array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * Supported admin options:
 *  - rd_kafka_AdminOptions_set_validate_only() - default false
 *  - rd_kafka_AdminOptions_set_operation_timeout() - default 0
 *  - rd_kafka_AdminOptions_set_timeout() - default socket.timeout.ms
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_CREATETOPICS_RESULT
 *)
procedure rd_kafka_CreateTopics(rk: Prd_kafka_t; new_topics: PPrd_kafka_NewTopic_t; new_topic_cnt: NativeUInt; const options: Prd_kafka_AdminOptions_t; rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_CreateTopics';

(**
 * @brief Get an array of topic results from a CreateTopics result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result to get topics from.
 * @param cntp Updated to the number of elements in the array.
 *)
function rd_kafka_CreateTopics_result_topics(const result: Prd_kafka_CreateTopics_result_t; cntp: PNativeUInt): PPrd_kafka_topic_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_CreateTopics_result_topics';

(**
 * @brief Create a new DeleteTopic object. This object is later passed to
 *        rd_kafka_DeleteTopics().
 *
 * @param topic Topic name to delete.
 *
 * @returns a new allocated DeleteTopic object.
 *          Use rd_kafka_DeleteTopic_destroy() to free object when done.
 *)
function rd_kafka_DeleteTopic_new(const topic: PUTF8Char): Prd_kafka_DeleteTopic_t; cdecl;
  external Lib name _PU + 'rd_kafka_DeleteTopic_new';

(**
 * @brief Destroy and free a DeleteTopic object previously created with
 *        rd_kafka_DeleteTopic_new()
 *)
procedure rd_kafka_DeleteTopic_destroy(del_topic: Prd_kafka_DeleteTopic_t); cdecl;
  external Lib name _PU + 'rd_kafka_DeleteTopic_destroy';

(**
 * @brief Helper function to destroy all DeleteTopic objects in
 *        the \p del_topics array (of \p del_topic_cnt elements).
 *        The array itself is not freed.
 *)
procedure rd_kafka_DeleteTopic_destroy_array(del_topics: PPrd_kafka_DeleteTopic_t; del_topic_cnt: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_DeleteTopic_destroy_array';

(**
 * @brief Delete topics from cluster as specified by the \p topics
 *        array of size \p topic_cnt elements.
 *
 * @param rk Client instance.
 * @param del_topics Array of topics to delete.
 * @param del_topic_cnt Number of elements in \p topics array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_DELETETOPICS_RESULT
 *)
procedure rd_kafka_DeleteTopics(rk: Prd_kafka_t; del_topics: PPrd_kafka_DeleteTopic_t; del_topic_cnt: NativeUInt; const options: Prd_kafka_AdminOptions_t; rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_DeleteTopics';

(**
 * @brief Get an array of topic results from a DeleteTopics result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result to get topic results from.
 * @param cntp is updated to the number of elements in the array.
 *)
function rd_kafka_DeleteTopics_result_topics(const result: Prd_kafka_DeleteTopics_result_t; cntp: PNativeUInt): PPrd_kafka_topic_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_DeleteTopics_result_topics';

(**
 * @brief Create a new NewPartitions. This object is later passed to
 *        rd_kafka_CreatePartitions() to increas the number of partitions
 *        to \p new_total_cnt for an existing topic.
 *
 * @param topic Topic name to create more partitions for.
 * @param new_total_cnt Increase the topic's partition count to this value.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns a new allocated NewPartitions object, or NULL if the
 *          input parameters are invalid.
 *          Use rd_kafka_NewPartitions_destroy() to free object when done.
 *)
function rd_kafka_NewPartitions_new(const topic: PUTF8Char; new_total_cnt: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt): Prd_kafka_NewPartitions_t; cdecl;
  external Lib name _PU + 'rd_kafka_NewPartitions_new';

(**
 * @brief Destroy and free a NewPartitions object previously created with
 *        rd_kafka_NewPartitions_new()
 *)
procedure rd_kafka_NewPartitions_destroy(new_parts: Prd_kafka_NewPartitions_t); cdecl;
  external Lib name _PU + 'rd_kafka_NewPartitions_destroy';

(**
 * @brief Helper function to destroy all NewPartitions objects in the
 *        \p new_parts array (of \p new_parts_cnt elements).
 *        The array itself is not freed.
 *)
procedure rd_kafka_NewPartitions_destroy_array(new_parts: PPrd_kafka_NewPartitions_t; new_parts_cnt: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_NewPartitions_destroy_array';

(**
 * @brief Set the replica (broker id) assignment for \p new_partition_idx to the
 *        replica set in \p broker_ids (of \p broker_id_cnt elements).
 *
 * @remark An application must either set the replica assignment for
 *         all new partitions, or none.
 *
 * @remark If called, this function must be called consecutively for each
 *         new partition being created,
 *         where \p new_partition_idx 0 is the first new partition,
 *         1 is the second, and so on.
 *
 * @remark \p broker_id_cnt should match the topic's replication factor.
 *
 * @remark Use rd_kafka_metadata() to retrieve the list of brokers
 *         in the cluster.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 *)
function rd_kafka_NewPartitions_set_replica_assignment(new_parts: Prd_kafka_NewPartitions_t; new_partition_idx: Int32; broker_ids: PInt32; broker_id_cnt: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_NewPartitions_set_replica_assignment';

(**
 * @brief Create additional partitions for the given topics, as specified
 *        by the \p new_parts array of size \p new_parts_cnt elements.
 *
 * @param rk Client instance.
 * @param new_parts Array of topics for which new partitions are to be created.
 * @param new_parts_cnt Number of elements in \p new_parts array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * Supported admin options:
 *  - rd_kafka_AdminOptions_set_validate_only() - default false
 *  - rd_kafka_AdminOptions_set_operation_timeout() - default 0
 *  - rd_kafka_AdminOptions_set_timeout() - default socket.timeout.ms
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 *)
procedure rd_kafka_CreatePartitions(rk: Prd_kafka_t; new_parts: PPrd_kafka_NewPartitions_t; new_parts_cnt: NativeUInt; const options: Prd_kafka_AdminOptions_t; rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_CreatePartitions';

(**
 * @brief Get an array of topic results from a CreatePartitions result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result o get topic results from.
 * @param cntp is updated to the number of elements in the array.
 *)
function rd_kafka_CreatePartitions_result_topics(const result: Prd_kafka_CreatePartitions_result_t; cntp: PNativeUInt): PPrd_kafka_topic_result_t; cdecl;
  external Lib name _PU + 'rd_kafka_CreatePartitions_result_topics';

(**
 * @returns a string representation of the \p confsource.
 *)
function rd_kafka_ConfigSource_name(confsource: rd_kafka_ConfigSource_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigSource_name';

(**
 * @returns the configuration property name
 *)
function rd_kafka_ConfigEntry_name(const entry: Prd_kafka_ConfigEntry_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_name';

(**
 * @returns the configuration value, may be NULL for sensitive or unset
 *          properties.
 *)
function rd_kafka_ConfigEntry_value(const entry: Prd_kafka_ConfigEntry_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_value';

(**
 * @returns the config source.
 *)
function rd_kafka_ConfigEntry_source(const entry: Prd_kafka_ConfigEntry_t): rd_kafka_ConfigSource_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_source';

(**
 * @returns 1 if the config property is read-only on the broker, else 0.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 *)
function rd_kafka_ConfigEntry_is_read_only(const entry: Prd_kafka_ConfigEntry_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_is_read_only';

(**
 * @returns 1 if the config property is set to its default value on the broker,
 *          else 0.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 *)
function rd_kafka_ConfigEntry_is_default(const entry: Prd_kafka_ConfigEntry_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_is_default';

(**
 * @returns 1 if the config property contains sensitive information (such as
 *          security configuration), else 0.
 * @remark An application should take care not to include the value of
 *         sensitive configuration entries in its output.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 *)
function rd_kafka_ConfigEntry_is_sensitive(const entry: Prd_kafka_ConfigEntry_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_is_sensitive';

(**
 * @returns 1 if this entry is a synonym, else 0.
 *)
function rd_kafka_ConfigEntry_is_synonym(const entry: Prd_kafka_ConfigEntry_t): Integer; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_is_synonym';

(**
 * @returns the synonym config entry array.
 *
 * @param entry Entry to get synonyms for.
 * @param cntp is updated to the number of elements in the array.
 *
 * @remark The lifetime of the returned entry is the same as \p conf .
 * @remark Shall only be used on a DescribeConfigs result,
 *         otherwise returns NULL.
 *)
function rd_kafka_ConfigEntry_synonyms(const entry: Prd_kafka_ConfigEntry_t; cntp: PNativeUInt): PPrd_kafka_ConfigEntry_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigEntry_synonyms';

(**
 * @returns a string representation of the \p restype
 *)
function rd_kafka_ResourceType_name(restype: rd_kafka_ResourceType_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ResourceType_name';

(**
 * @brief Create new ConfigResource object.
 *
 * @param restype The resource type (e.g., RD_KAFKA_RESOURCE_TOPIC)
 * @param resname The resource name (e.g., the topic name)
 *
 * @returns a newly allocated object
 *)
function rd_kafka_ConfigResource_new(restype: rd_kafka_ResourceType_t; const resname: PUTF8Char): Prd_kafka_ConfigResource_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_new';

(**
 * @brief Destroy and free a ConfigResource object previously created with
 *        rd_kafka_ConfigResource_new()
 *)
procedure rd_kafka_ConfigResource_destroy(config: Prd_kafka_ConfigResource_t); cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_destroy';

(**
 * @brief Helper function to destroy all ConfigResource objects in
 *        the \p configs array (of \p config_cnt elements).
 *        The array itself is not freed.
 *)
procedure rd_kafka_ConfigResource_destroy_array(config: PPrd_kafka_ConfigResource_t; config_cnt: NativeUInt); cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_destroy_array';

(**
 * @brief Set configuration name value pair.
 *
 * @param config ConfigResource to set config property on.
 * @param name Configuration name, depends on resource type.
 * @param value Configuration value, depends on resource type and \p name.
 *              Set to \c NULL to revert configuration value to default.
 *
 * This will overwrite the current value.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if config was added to resource,
 *          or RD_KAFKA_RESP_ERR__INVALID_ARG on invalid input.
 *)
function rd_kafka_ConfigResource_set_config(config: Prd_kafka_ConfigResource_t; const name: PUTF8Char; const value: PUTF8Char): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_set_config';

(**
 * @brief Get an array of config entries from a ConfigResource object.
 *
 * The returned object life-times are the same as the \p config object.
 *
 * @param config ConfigResource to get configs from.
 * @param cntp is updated to the number of elements in the array.
 *)
function rd_kafka_ConfigResource_configs(const config: Prd_kafka_ConfigResource_t; cntp: PNativeUInt): PPrd_kafka_ConfigEntry_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_configs';

(**
 * @returns the ResourceType for \p config
 *)
function rd_kafka_ConfigResource_type(const config: Prd_kafka_ConfigResource_t): rd_kafka_ResourceType_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_type';

(**
 * @returns the name for \p config
 *)
function rd_kafka_ConfigResource_name(const config: Prd_kafka_ConfigResource_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_name';

(**
 * @returns the error for this resource from an AlterConfigs request
 *)
function rd_kafka_ConfigResource_error(const config: Prd_kafka_ConfigResource_t): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_error';

(**
 * @returns the error string for this resource from an AlterConfigs
 *          request, or NULL if no error.
 *)
function rd_kafka_ConfigResource_error_string(const config: Prd_kafka_ConfigResource_t): PUTF8Char; cdecl;
  external Lib name _PU + 'rd_kafka_ConfigResource_error_string';

(**
 * @brief Update the configuration for the specified resources.
 *        Updates are not transactional so they may succeed for a subset
 *        of the provided resources while the others fail.
 *        The configuration for a particular resource is updated atomically,
 *        replacing values using the provided ConfigEntrys and reverting
 *        unspecified ConfigEntrys to their default values.
 *
 * @remark Requires broker version >=0.11.0.0
 *
 * @warning AlterConfigs will replace all existing configuration for
 *          the provided resources with the new configuration given,
 *          reverting all other configuration to their default values.
 *
 * @remark Multiple resources and resource types may be set, but at most one
 *         resource of type \c RD_KAFKA_RESOURCE_BROKER is allowed per call
 *         since these resource requests must be sent to the broker specified
 *         in the resource.
 *
 *)
procedure rd_kafka_AlterConfigs(rk: Prd_kafka_t; configs: PPrd_kafka_ConfigResource_t; config_cnt: NativeUInt; const options: Prd_kafka_AdminOptions_t; rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_AlterConfigs';

(**
 * @brief Get an array of resource results from a AlterConfigs result.
 *
 * Use \c rd_kafka_ConfigResource_error() and
 * \c rd_kafka_ConfigResource_error_string() to extract per-resource error
 * results on the returned array elements.
 *
 * The returned object life-times are the same as the \p result object.
 *
 * @param result Result object to get resource results from.
 * @param cntp is updated to the number of elements in the array.
 *
 * @returns an array of ConfigResource elements, or NULL if not available.
 *)
function rd_kafka_AlterConfigs_result_resources(const result: Prd_kafka_AlterConfigs_result_t; cntp: PNativeUInt): PPrd_kafka_ConfigResource_t; cdecl;
  external Lib name _PU + 'rd_kafka_AlterConfigs_result_resources';

(**
 * @brief Get configuration for the specified resources in \p configs.
 *
 * The returned configuration includes default values and the
 * rd_kafka_ConfigEntry_is_default() or rd_kafka_ConfigEntry_source()
 * methods may be used to distinguish them from user supplied values.
 *
 * The value of config entries where rd_kafka_ConfigEntry_is_sensitive()
 * is true will always be NULL to avoid disclosing sensitive
 * information, such as security settings.
 *
 * Configuration entries where rd_kafka_ConfigEntry_is_read_only()
 * is true can't be updated (with rd_kafka_AlterConfigs()).
 *
 * Synonym configuration entries are returned if the broker supports
 * it (broker version >= 1.1.0). See rd_kafka_ConfigEntry_synonyms().
 *
 * @remark Requires broker version >=0.11.0.0
 *
 * @remark Multiple resources and resource types may be requested, but at most
 *         one resource of type \c RD_KAFKA_RESOURCE_BROKER is allowed per call
 *         since these resource requests must be sent to the broker specified
 *         in the resource.
 *)
procedure rd_kafka_DescribeConfigs(rk: Prd_kafka_t; configs: PPrd_kafka_ConfigResource_t; config_cnt: NativeUInt; const options: Prd_kafka_AdminOptions_t; rkqu: Prd_kafka_queue_t); cdecl;
  external Lib name _PU + 'rd_kafka_DescribeConfigs';

(**
 * @brief Get an array of resource results from a DescribeConfigs result.
 *
 * The returned \p resources life-time is the same as the \p result object.
 *
 * @param result Result object to get resource results from.
 * @param cntp is updated to the number of elements in the array.
 *)
function rd_kafka_DescribeConfigs_result_resources(const result: Prd_kafka_DescribeConfigs_result_t; cntp: PNativeUInt): PPrd_kafka_ConfigResource_t; cdecl;
  external Lib name _PU + 'rd_kafka_DescribeConfigs_result_resources';

(**
 * @brief Set SASL/OAUTHBEARER token and metadata
 *
 * @param rk Client instance.
 * @param token_value the mandatory token value to set, often (but not
 *  necessarily) a JWS compact serialization as per
 *  https://tools.ietf.org/html/rfc7515#section-3.1.
 * @param md_lifetime_ms when the token expires, in terms of the number of
 *  milliseconds since the epoch.
 * @param md_principal_name the mandatory Kafka principal name associated
 *  with the token.
 * @param extensions optional SASL extensions key-value array with
 *  \p extensions_size elements (number of keys * 2), where [i] is the key and
 *  [i+1] is the key's value, to be communicated to the broker
 *  as additional key-value pairs during the initial client response as per
 *  https://tools.ietf.org/html/rfc7628#section-3.1. The key-value pairs are
 *  copied.
 * @param extension_size the number of SASL extension keys plus values,
 *  which must be a non-negative multiple of 2.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * The SASL/OAUTHBEARER token refresh callback or event handler should invoke
 * this method upon success. The extension keys must not include the reserved
 * key "`auth`", and all extension keys and values must conform to the required
 * format as per https://tools.ietf.org/html/rfc7628#section-3.1:
 *
 *     key            = 1*(ALPHA)
 *     value          = *(VCHAR / SP / HTAB / CR / LF )
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, otherwise \p errstr set
 *              and:<br>
 *          \c RD_KAFKA_RESP_ERR__INVALID_ARG if any of the arguments are
 *              invalid;<br>
 *          \c RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
 *              supported by this build;<br>
 *          \c RD_KAFKA_RESP_ERR__STATE if SASL/OAUTHBEARER is supported but is
 *              not configured as the client's authentication mechanism.<br>
 *
 * @sa rd_kafka_oauthbearer_set_token_failure
 * @sa rd_kafka_conf_set_oauthbearer_token_refresh_cb
 *)
function rd_kafka_oauthbearer_set_token(rk: Prd_kafka_t; const token_value: PUTF8Char; md_lifetime_ms: Int64; const md_principal_name: PUTF8Char; extensions: PPUTF8Char; extension_size: NativeUInt; errstr: PUTF8Char; errstr_size: NativeUInt): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_oauthbearer_set_token';

(**
 * @brief SASL/OAUTHBEARER token refresh failure indicator.
 *
 * @param rk Client instance.
 * @param errstr mandatory human readable error reason for failing to acquire
 *  a token.
 *
 * The SASL/OAUTHBEARER token refresh callback or event handler should invoke
 * this method upon failure.
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, otherwise:<br>
 *          \c RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
 *              supported by this build;<br>
 *          \c RD_KAFKA_RESP_ERR__STATE if SASL/OAUTHBEARER is supported but is
 *              not configured as the client's authentication mechanism,<br>
 *          \c RD_KAFKA_RESP_ERR__INVALID_ARG if no error string is supplied.
 *
 * @sa rd_kafka_oauthbearer_set_token
 * @sa rd_kafka_conf_set_oauthbearer_token_refresh_cb
 *)
function rd_kafka_oauthbearer_set_token_failure(rk: Prd_kafka_t; const errstr: PUTF8Char): rd_kafka_resp_err_t; cdecl;
  external Lib name _PU + 'rd_kafka_oauthbearer_set_token_failure';

(**
 * @brief Initialize transactions for the producer instance.
 *
 * This function ensures any transactions initiated by previous instances
 * of the producer with the same \c transactional.id are completed.
 * If the previous instance failed with a transaction in progress the
 * previous transaction will be aborted.
 * This function needs to be called before any other transactional or
 * produce functions are called when the \c transactional.id is configured.
 *
 * If the last transaction had begun completion (following transaction commit)
 * but not yet finished, this function will await the previous transaction's
 * completion.
 *
 * When any previous transactions have been fenced this function
 * will acquire the internal producer id and epoch, used in all future
 * transactional messages issued by this producer instance.
 *
 * Upon successful return from this function the application has to perform at
 * least one of the following operations within \c transaction.timeout.ms to
 * avoid timing out the transaction on the broker:
 *   * rd_kafka_produce() (et.al)
 *   * rd_kafka_send_offsets_to_transaction()
 *   * rd_kafka_commit_transaction()
 *   * rd_kafka_abort_transaction()
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call init_transactions() again.
 *
 * @remark This function may block up to \p timeout_ms milliseconds.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether a fatal
 *          error has been raised by calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction coordinator
 *          could be not be contacted within \p timeout_ms (retriable),
 *          RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE if the transaction
 *          coordinator is not available (retriable),
 *          RD_KAFKA_RESP_ERR_CONCURRENT_TRANSACTIONS if a previous transaction
 *          would not complete within \p timeout_ms (retriable),
 *          RD_KAFKA_RESP_ERR__STATE if transactions have already been started
 *          or upon fatal error,
 *          RD_KAFKA_RESP_ERR__UNSUPPORTED_FEATURE if the broker(s) do not
 *          support transactions (<Apache Kafka 0.11), this also raises a
 *          fatal error,
 *          RD_KAFKA_RESP_ERR_INVALID_TRANSACTION_TIMEOUT if the configured
 *          \c transaction.timeout.ms is outside the broker-configured range,
 *          this also raises a fatal error,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          or \p timeout_ms is out of range.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 *)
function rd_kafka_init_transactions(rk: Prd_kafka_t; timeout_ms: Integer): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_init_transactions';

(**
 * @brief Begin a new transaction.
 *
 * rd_kafka_init_transactions() must have been called successfully (once)
 * before this function is called.
 *
 * Any messages produced, offsets sent (rd_kafka_send_offsets_to_transaction()),
 * etc, after the successful return of this function will be part of
 * the transaction and committed or aborted atomatically.
 *
 * Finish the transaction by calling rd_kafka_commit_transaction() or
 * abort the transaction by calling rd_kafka_abort_transaction().
 *
 * @param rk Producer instance.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether a fatal error has been raised by
 *          calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if a transaction is already in progress
 *          or upon fatal error,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark With the transactional producer, rd_kafka_produce(),
 *         rd_kafka_producev(), et.al, are only allowed during an on-going
 *         transaction, as started with this function.
 *         Any produce call outside an on-going transaction, or for a failed
 *         transaction, will fail.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 *)
function rd_kafka_begin_transaction(rk: Prd_kafka_t): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_begin_transaction';

(**
 * @brief Sends a list of topic partition offsets to the consumer group
 *        coordinator for \p cgmetadata, and marks the offsets as part
 *        part of the current transaction.
 *        These offsets will be considered committed only if the transaction is
 *        committed successfully.
 *
 *        The offsets should be the next message your application will consume,
 *        i.e., the last processed message's offset + 1 for each partition.
 *        Either track the offsets manually during processing or use
 *        rd_kafka_position() (on the consumer) to get the current offsets for
 *        the partitions assigned to the consumer.
 *
 *        Use this method at the end of a consume-transform-produce loop prior
 *        to committing the transaction with rd_kafka_commit_transaction().
 *
 * @param rk Producer instance.
 * @param offsets List of offsets to commit to the consumer group upon
 *                successful commit of the transaction. Offsets should be
 *                the next message to consume, e.g., last processed message + 1.
 * @param cgmetadata The current consumer group metadata as returned by
 *                   rd_kafka_consumer_group_metadata() on the consumer
 *                   instance the provided offsets were consumed from.
 * @param timeout_ms Maximum time allowed to register the offsets on the broker.
 *
 * @remark This function must be called on the transactional producer instance,
 *         not the consumer.
 *
 * @remark The consumer must disable auto commits
 *         (set \c enable.auto.commit to false on the consumer).
 *
 * @remark Logical and invalid offsets (such as RD_KAFKA_OFFSET_INVALID) in
 *         \p offsets will be ignored, if there are no valid offsets in
 *         \p offsets the function will return RD_KAFKA_RESP_ERR_NO_ERROR
 *         and no action will be taken.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether an abortable
 *          or fatal error has been raised by calling
 *          rd_kafka_error_txn_requires_abort() or rd_kafka_error_is_fatal()
 *          respectively.
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR_GROUP_AUTHORIZATION_FAILED if the producer is
 *          not authorized to write the consumer offsets to the group
 *          coordinator,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          or if the \p consumer_group_id or \p offsets are empty,
 *          RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS if a previous
 *          rd_kafka_send_offsets_to_transaction() call is still in progress.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 *)
function rd_kafka_send_offsets_to_transaction(rk: Prd_kafka_t; const offsets: Prd_kafka_topic_partition_list_t; const cgmetadata: Prd_kafka_consumer_group_metadata_t; timeout_ms: Integer): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_send_offsets_to_transaction';

(**
 * @brief Commit the current transaction (as started with
 *        rd_kafka_begin_transaction()).
 *
 *        Any outstanding messages will be flushed (delivered) before actually
 *        committing the transaction.
 *
 *        If any of the outstanding messages fail permanently the current
 *        transaction will enter the abortable error state and this
 *        function will return an abortable error, in this case the application
 *        must call rd_kafka_abort_transaction() before attempting a new
 *        transaction with rd_kafka_begin_transaction().
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call this function again.
 *
 * @remark This function will block until all outstanding messages are
 *         delivered and the transaction commit request has been successfully
 *         handled by the transaction coordinator, or until \p timeout_ms
 *         expires, which ever comes first. On timeout the application may
 *         call the function again.
 *
 * @remark Will automatically call rd_kafka_flush() to ensure all queued
 *         messages are delivered before attempting to commit the
 *         transaction.
 *         If the application has enabled RD_KAFKA_EVENT_DR it must
 *         serve the event queue in a separate thread since rd_kafka_flush()
 *         will not serve delivery reports in this mode.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether an abortable
 *          or fatal error has been raised by calling
 *          rd_kafka_error_txn_requires_abort() or rd_kafka_error_is_fatal()
 *          respectively.
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction could not be
 *          complete commmitted within \p timeout_ms, this is a retriable
 *          error as the commit continues in the background,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 *)
function rd_kafka_commit_transaction(rk: Prd_kafka_t; timeout_ms: Integer): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_commit_transaction';

(**
 * @brief Aborts the ongoing transaction.
 *
 *        This function should also be used to recover from non-fatal abortable
 *        transaction errors.
 *
 *        Any outstanding messages will be purged and fail with
 *        RD_KAFKA_RESP_ERR__PURGE_INFLIGHT or RD_KAFKA_RESP_ERR__PURGE_QUEUE.
 *        See rd_kafka_purge() for details.
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call this function again.
 *
 * @remark This function will block until all outstanding messages are purged
 *         and the transaction abort request has been successfully
 *         handled by the transaction coordinator, or until \p timeout_ms
 *         expires, which ever comes first. On timeout the application may
 *         call the function again.
 *         If the application has enabled RD_KAFKA_EVENT_DR it must
 *         serve the event queue in a separate thread since rd_kafka_flush()
 *         will not serve delivery reports in this mode.

 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether a fatal error
 *          has been raised by calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction could not be
 *          complete commmitted within \p timeout_ms, this is a retriable
 *          error as the commit continues in the background,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 *)
function rd_kafka_abort_transaction(rk: Prd_kafka_t; timeout_ms: Integer): Prd_kafka_error_t; cdecl;
  external Lib name _PU + 'rd_kafka_abort_transaction';

implementation

end.
